{"Date": 2023, "Claim": "Since reaching the peak of NFT transaction activities in late 2021,\nthe NFT market has leveled off. However, studies on predictors of\nNFTpricesafterlate2021arescarce,especiallythoseinvolvingethi-\n: The comparison of the performance of price pre- calconcerns,e.g.,genderandskintones.Examiningthepredictabil-\ndictors (with and without sentiment score). ity of factors such as social media sentiment and ethics can help\nbetter understand the mechanics of NFT valuation and assess the\nethical issues currently in the NFT marketplace. Therefore, in this\nTo better illustrate the change in regression results before and\nstudy,wetakethelargestNFTcollection,CryptoPunks,asanexam-\nafter adding the sentiment score in the model, we put the coeffi-\nple,aimingtoanswerthequestionsrelatedtothestatusquooftweet\ncients of independent variables in 7. In the lollipop graph, each\nsentiments,skintones,andgendersofCryptoPunks,aswellastheir\nlollipop represents one independent variable, and the correspond-\ning significance levels, which range from 0 to 3, are shown in the 9Additionally,thecoefficientsofsentimentscoresandNon-humanCryptoPunkson\ncenter of the lollipop. The larger the number, the more significant pricesshowsignificantchanges.Wealsoconductregressionanalysisontheoverall\ndatafrom2017to2022,andtheresultsareclosertothosefrombefore2021.Weattribute\nthe variable, while 0 indicates no statistical significance.", "Source": "On the Mechanics of NFT Valuation: AI Ethics and Social Media", "File": "On the Mechanics of NFT Valuation AI Ethics and Social Media.pdf"}
{"Date": 2022, "Claim": "to say that we should not care about the principles that AI\nsystems endorse or violate, we just might need to provide\na different reasoning, or different incentives from gaining Though notions of trust and trustworthiness have gained\ntrust. Structures that institutionalize \u201cdistrust\u201d like binding significant attention in AI research especially after the\nstandards can also provide strong incentives to act in certain publication of Europe\u2019s High-Level Expert Group\u2019s Ethics\nways. Related to this point, the call for more trust needs to guidelines for trustworthy AI, ethics guidelines referring to\nbe monitored closely; in some cases, it might stand in for trust with regard to AI diverge substantively in no less than\nan avoidance of strict hard law regulations. The dominant four main areas: (1) why trust is important or of value, (2)\nperspective in the corpus of documents is that building trust who or what the envisioned trustors and trustees are, (3)\nis a \u201cfundamental requirement for ethical governance\u201d . what trustworthiness is and entails, and (4) how it should\nIn many cases, however, it might be the more ethical deci- be implemented technically. Further clarification on all four\nsion to call for a robust legal framework, not for more trust. points is needed. Philosophy can help to conceptualize trust\nOn a more conceptual note, accepting that people are and trustworthiness.", "Source": "Trust and trustworthiness in AI ethics", "File": "Trust and trustworthiness in AI ethics.pdf"}
{"Date": 2023, "Claim": "By asserting that GSK v. Teva was a narrow, fact-specific case that\nshould not be viewed as upsetting the balance struck by the Hatch\u2013Wax-\nman Act, the Federal Circuit underestimates their holding.229 Under\n\u00a7 271(b), the word actively loses meaning when a brand-name company\ncan offer evidence that fails to demonstrate that culpable conduct by the\ngeneric was the sole cause of such infringement, thus, weakening the\nspecific intent and causation elements. Generics will now be forced to\nheavily dissect and narrowly tailor all aspects of its label, promotional\nmaterials, and external communications.230 By allowing GSK to prevail\n224. Id.\n225. Id. at 3.\n226. Id. at 21.\n227. Id. at 23.\n228. Tina W. McKeon & April A. Isaacson, GSK v. Teva: The Skinny on Induced Infringe-\nment and Label Carve-Outs, KILPATRICK TOWNSEND (Jan. 13, 2021), https://kilpatricktown-\nsend.com/en/Blog/MEMO/2021/1/GSK-v-Teva\u2014-The-Skinny-On-Induced-\nInfringement-And-Label-Carve-Outs [https://perma.cc/4F5L-79DH] (quoting Brief of\nAmicus Curiae Former Congressman Henry A. Waxman in Support of Petition for Re-\nhearing En Banc [Corrected] at 1\u20132, GlaxoSmithKline LLC, Nos. 18-1976, 18-2023 (Fed.\nCir. Dec. 30, 2020)).\n229. GlaxoSmithKline LLC v. Teva Pharms. USA, Inc., 7 F.4th 1320, 1326 (Fed. Cir.\n2021).\n230. GSK v. Teva: Federal Circuit Opinion After Rehearing Confirms Induced\nInfringement Liability Despite Skinny Label, COOLEY, https://www.cooley.com/news/ins\n222 TEXAS A&M J. OF PROP. L. [Vol.", "Source": "Hacking or Hatching the Skinny Label: How the Federal Circuit\u2019s Decision in GSK v. Teva Threatens Generics and Induced Infringement", "File": "Hacking or Hatching the Skinny Label How the Federal Circuits Decision in GSK v. Teva Threatens Generics and Induced Infringement.pdf"}
{"Date": 2022, "Claim": "trust relationship between the trustor and the trustee. ZTA In this work, we have examined existing trustworthy AI ap-\nPEP interacts with a policy administrator  responsible for proaches and identified the trustworthy properties highlighted\nestablishing and/or stop trust relationship between parties. To in the literature. After analysis of the properties, we identified\ntake the appropriate decision, the policy administrator can gaps in the different properties as well as some overlaps. Based\nreason based on a decision engine (i.e., the policy engine). on these gaps, we propose a trust (resp. zero trust) model to\nThe policy engine manages the decision to authorize access ensure a trustworthy AI. Results show that Transparency is\nto a resource (e.g., data, predictions) for a given subject the most cited principle among the studied countries. The 12\n(e.g., models, users). To do so, the policy engine uses TAI ideal trustworthy properties identified are Transparency, Pri-\npolicy based on ethical trustworthy AI principles, guidelines, vacy, Fairness, Security, Safety, Responsibility, Accountability,\nstandards, and checklists in order to verify the trustworthiness Explainability, Well-being, Human Rights, Inclusiveness, and\nof the entity. ZTA PDP takes an immediate decision based on Sustainability. The proposed trust and zero-trust models are\ninstructions received from the policy administrator in the ZTA based on EthicsOps to ensure end-to-end application and\nPEP component.", "Source": "Never trust, always verify : a roadmap for Trustworthy AI?", "File": "Never trust always verify  a roadmap for Trustworthy AI.pdf"}
{"Date": 2020, "Claim": "I have tried to show that it is good that AI researchers have identified the\nimportance of ensuring that AI will remain beneficial and that therefore AI\nshould align with human values. However, since the term \u2018value\u2019 only functions\nas a placeholder and in reality one could limit oneself to making AI fulfil human\npreferences, the problem may arise that AI will not necessarily act morally, but\npotentially also satisfy immoral preferences. Thus, in order to secure ethical AI\none should compel it to follow moral principles or obtain moral values.\nIn the course of the argument the question emerged whether there are universal\nvalues or whether AI should reckon with cultural differences. I hope I have\nshown that value pluralism can accommodate both moral disagreements caused\nby diversity of values and the objectivity of ethics, accepting that there are\nno uniquely correct answers to moral questions. Moral ambiguity does not\nnecessarily imply relativism. Accepting plurality does not mean that anything\ngoes. We can identify wrong answers as long as we share our expectations of\nmorality. Indeed, the aim of morality is to regulate our life in society, enabling\nus to live together peacefully and allowing human beings to satisfy their various\nneeds: physiological, psychological, social, cultural and intellectual.", "Source": "Challenges of Aligning Artificial Intelligence with Human Values", "File": "Challenges of Aligning Artificial Intelligence with Human Values.pdf"}
{"Date": 2018, "Claim": "As discussed above, the singularity is a hypothesized, futuristic event\nthat pertains to the invention of machines that are of greater-than-human-\nlevel intelligence. This scenario may be considered a natural consequence of\ndeveloping AI systems. Once these things are fully realized, an intelligence\nexplosion would follow soon after. Note that such runaway would seemingly\nput into question humankind\u2019s standard concepts about reality, life, and so\non, and this would also include our general ethical notions.\n56 Wallach and Allen, Moral Machines, 25-33.\n\u00a9 2018 Robert James M. Boyles\nhttps://www.kritike.org/journal/issue_22/boyles_june2018.pdf\nISSN 1908-7330\nR. BOYLES 197\nAt present, there is no definitive way on how to make artificially\nintelligent systems value what humans do. The idea of machines\nexterminating the entire human race, therefore, is not, strictly speaking, a tall\ntale. As highlighted by both Vinge and Chalmers, there are plausible threats.\nTo prevent these dangers, those working in artificial intelligence research,\nincluding philosophers, should take the problem of creating artificial moral\nagents more seriously. Philosophers, for instance, should further examine the\nphilosophical tenability of the top-down, bottom-up, and hybrid options,\namong others, of modeling such agents. For one, as noted earlier, the actual\nnature of AMAs is still an open question.", "Source": "A Case for Machine Ethics in Modeling Human-Level Intelligent Agents", "File": "A Case for Machine Ethics in Modeling Human-Level Intelligent Agents.pdf"}
{"Date": 2023, "Claim": "This article argues that the current discourse on AI and ethics, despite its breadth and richness, has structural and\nfundamental limitations. The discourse provides detailed insights into many of the ethical issues and concerns\nthat AI technologies can raise, and it includes numerous mechanisms that can be employed to address these. One\nway of assessing the impact of this discourse is to use the concept of responsibility. The current focus of much of\nthe ethics of AI discussion is on identifying specific responsibility relationships based on specific issues, subjects\nor outcomes. While such individual responsibilities are of crucial importance, they find their limitations in the\nfact that AI is not so much a clear and well-described technology but can be better described as an ecosystem of\nsocio-technical systems. Based on this conceptualisation of AI the article asked what a responsible ecosystem of\nintelligent systems would look like and suggested some characteristic that it would have to display.\nThis argument is important for several audiences. It enriches the theoretical landscape of the discussion of\nethics of AI and should thus prove to be of interest to scholars who participate in this discourse. Another audi-\nence is made up of scientist and technical experts who work on developing and implementing intelligent systems.\nMembers of this community are generally aware of the ethical and social challenges that intelligent systems can\nraise.", "Source": "Embedding responsibility in intelligent systems: from AI ethics to responsible AI ecosystems", "File": "Embedding responsibility in intelligent systems from AI ethics to responsible AI ecosystems.pdf"}
{"Date": 2022, "Claim": "AI systems are used today to make life-altering decisions about employment, bail, parole, and\nlending, and the scope of decisions delegated by AI systems seems likely to expand in the future.\nThe pervasiveness of AI across many fields is something that will not slowdown anytime soon and\norganizations will want to keep up with such applications. However, they must be cognisant of the\nrisks that come with AI and have guidelines around how they approach applications of AI to avoid\nsuch risks. By establishing a framework for AI Governance, organizations will be able to harness AI\nfor their use cases while at the same time avoiding risks and having plans in place for risk mitigation,\nwhich is paramount.\nSocial Impact As we discuss in this paper, governance and certain control over AI applications\nin organizations should be mandatory. AI Governance aims to enable and facilitate connections\nbetween various aspects of trustworthy and socially responsible machine learning systems, and\ntherefore it accounts for security, robustness, privacy, fairness, ethics, and transparency. We believe\nthe implementation of these ideas should have a positive impact in the society.\nAcknowledgements We thank the Trustworthy and Socially Responsible Machine Learning\n(TSRML) Workshop at NeurIPS 2022. This work was supported by H2O.ai.", "Source": "A Brief Overview of AI Governance for Responsible Machine Learning Systems", "File": "A Brief Overview of AI Governance for Responsible Machine Learning Systems.pdf"}
{"Date": 2023, "Claim": "of a solution for any given task. Initially coined by Laughlin and\nAdamopoulos , demonstrability requires four conditions: (1) There are many reasons to develop AI systems that can explain\nGroup consensus on a verbal (e.g., vocabulary, syntax) or mathe- themselves, but it has become apparent that different uses require\nmatical system (e.g., primitives, axioms, permissible operations like different types of explanations. It is therefore important to ensure a\nalgebra);(2) Sufficientinformationforasolutionwithinthesystem; given type of explanation is well-aligned with its use. In this paper,\n(3) Group members who cannot solve the problem must have suffi- wefocusedspecificallyontheutilityofAIexplanationsforinstance-\ncient knowledge to recognize and accept a proposed solution; (4) level human-AI decision making. We proposed that explanations\nThe correct team member must have sufficient ability, motivation, can engender complementary performance in decision making to\nand time to demonstrate the correct solution to other members. the extent the explanation allows a human decision maker to verify\nDemonstrability is therefore contingent on the information sys- correctness of the AI\u2019s recommendation. Unfortunately, most kinds\ntem and the extent to which the task type affords recognition of of explanations\u2014notably those which elucidate the AI\u2019s reasoning\ncorrect solutions.", "Source": "In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making", "File": "In Search of Verifiability Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making.pdf"}
{"Date": 2022, "Claim": "In this work, we presented a thorough analysis that shows that KD can be\nused to boost the model robustness, surpassing state-of-the-art performance.\nWe proposed a new robust KD algorithm, AKD, and gave guidelines to use\nearly stopping, label mixing, ensembling, and small-epsilon adversarial training\nto boost the student model performance. Finally, we analyzed how KD affects\nthe training trajectories of different samples, and found that it calibrates the\nmodel and improves its performance in difficult-to-learn regions.\nIn the future, we think our robustness results could be improved improved\neven further by using an ensemble of teachers, small-\u03b5 adversarial training and\nfurther tuning the label mixing parameter. Moreover, it would be interesting to\nhave results on larger architectures and more datasets.\nOn the benefits of knowledge distillation for adversarial robustness 15", "Source": "On the benefits of knowledge distillation for adversarial robustness", "File": "On the benefits of knowledge distillation for adversarial robustness.pdf"}
{"Date": 2018, "Claim": "The alignment of artificially intelligent agents with human goals and values is a fundamental challenge\nin AI research. It is also the fundamental challenge of organizing human economic interaction in\nan economy built on specialization and the division of labor\u2013in which humans are tasked with\ntaking actions that generate costs and benefits for other humans. By recognizing and elaborating the\nparallels between the challenge of incomplete contracting in the human principal-agent setting and\nthe challenge of misspecification in robot reward functions, this paper provides AI researchers with\na different framework for the alignment problem. That framework urges researchers to see reward\nmisspecification as fundamental and not merely the result of poor engineering. Doing so, as we show,\ngenerates insights both for the analysis of current, weakly strategic, AI systems and potential, strongly\nstrategic, systems. Our most important claim is that aligning robots with humans will inevitably\nrequire building the technical tools to allow AI to do what human agents do naturally: import into\ntheir assessment of rewards the costs associated with taking actions tagged as wrongful by human\ncommunities. These are the lessons learned by economists and legal scholars over the past several\ndecades in the context of incomplete contracting. They are lessons available also to AI researchers.", "Source": "Incomplete Contracting and AI Alignment", "File": "Incomplete Contracting and AI Alignment.pdf"}
{"Date": 2022, "Claim": "et al., 2021; Zheng et al., 2020) and segmentation (Wang\net al., 2020; Liu et al., 2021; Zheng et al., 2020). The suc-\nIn this paper, we verified self-attention as a contributor of\ncess of vision transformers for vision tasks triggers broad\nthe improved robustness in vision transformers. Our study\ndebates and studies on the advantages of self-attention ver-\nshows that self-attention promotes naturally formed clusters\nsus convolutions (Raghu et al., 2021; Tang et al., 2021).\nin tokens, which exhibits interesting relation to the exten-\nCompared to convolutions, an important advantage is the ro-\nsive early studies in vision grouping prior to deep learning.\nbustness against observable corruptions. Several works (Bai\nWe also established an explanatory framework from the\net al., 2021; Xie et al., 2021; Paul & Chen, 2022; Naseer\nperspective of information bottleneck to explain these prop-\net al., 2021) have empirically shown that the robustness of\nerties of self-attention. To push the boundary of robust\nViTs against corruption consistently outperforms ConvNets\nrepresentation learning with self-attention, we introduced\nby significant margins. However, how the key component\na family of fully-attentional network (FAN) architectures,\n(i.e. self-attention) contributes to the robustness is under-\nwhere self-attention is leveraged in both token mixing and\nexplored. In contrast, our work conducts empirical studies\nchannel processing.", "Source": "Understanding The Robustness in Vision Transformers", "File": "Understanding The Robustness in Vision Transformers.pdf"}
{"Date": 2024, "Claim": "of the EU (65). He was one of only four ethicists alongside 48\nnon-ethicists from politics, universities, civil society, and mainly\nindustry (62). He reflected on the resulting framework as follows: In conclusion, we present the position that the translational\n\u201cAs a member of the expert group, I am disappointed with the gaps in healthcare AI significantly result from a lack of an\nresult that has now been presented. The guidelines are lukewarm, operational definition of (trust)worthiness. This leads to (a)\nshort-sighted, and deliberately vague. They ignore long-term unintentional misunderstandings about the term and (b) creates\nrisks, gloss over difficult problems (\u201cexplainability\u201d) with opportunity for intentional misuse in the form of ethics washing.\nrhetoric, violate elementary principles of rationality and pretend To prevent these risks and foster genuine trust in AI for\nto know things that nobody really knows.\u201d (62) Given how much healthcare, it is crucial to establish an operational definition of\nimpact this framework already had on the field of \u201cTrustworthy trust(worthiness) which includes guidance on how to tangibly\nAI\u201d, these insights are more than concerning. According to produce, measure and evaluate trustworthiness of AI. This\nMetzingers report, there is a considerable danger that the whole definitory work must be performed carefully since there are\nconcept of \u201cTrustworthy AI\u201d was deliberately kept vague and many possible conceptualizations of trust(worthiness).", "Source": "The unmet promise of trustworthy AI in healthcare: why we fail at clinical translation", "File": "The unmet promise of trustworthy AI in healthcare why we fail at clinical translation.pdf"}
{"Date": 2024, "Claim": "In this article, we have focused on the need to develop a methodology to interpret\nand apply AI ethics principle to specific practices, in this case those of the defence\ndomain. To be effective, such a methodology has to produce applicable guidelines\nleading to fair solutions, which are consistent with the spirit of the AI ethics princi-\nples as well as with the foundational values of democratic societies. The methodol-\nogy also has to be replicable and scrutinisable, so that it can be refined and adapted\nwhen its applications show flaws or limitations. With it, we aim to close the gap\nhighlighted in Sect. 1 and provide the necessary middle step for the interpretation of\nAI ethics principles and their application into practices.\nQuestions concerning the operationalisation of the methodology are outside the\nscope of this article. However, we shall submit that the proposed methodology has\ngreat potential to be practical and agile, because following it an EB can identify ethi-\ncal risks and requirements focusing on types of AI systems and for purposes of use.\nIn this way, it is feasible that an EB will not have to consider and specify ethical\nrequirements for every new AI system to be procured, developed, designed or used\nby a defence organisation.", "Source": "From AI Ethics Principles to Practices: A Teleological Methodology to Apply AI Ethics Principles in The Defence Domain", "File": "From AI Ethics Principles to Practices A Teleological Methodology to Apply AI Ethics Principles in The Defence Domain.pdf"}
{"Date": 2020, "Claim": "As more firms embrace field experiments to optimize their marketing activities, the focus on how to\nderive more value from field experiments is likely to intensify. One way to derive additional value is to\nsegment customers and evaluate how to target different customers with different marketing treatments.\nFortunately the literature on customer segmentation and targeting methods is vast, and so firms have a\nbroad range of methods to choose from. However, the literature offers little guidance as to which of these\nmethods are most effective in practice.\nIn this paper, we have evaluated seven widely used segmentation methods using a series of two large-\nscale field experiments. The first field experiment is used to generate a common pool of training data for\neach of the seven methods. We then validate the optimized policies provided by each method in a second\nfield experiment. Our detailed comparison of the methods reveals an important general finding. Model-\ndriven methods perform better than distance-driven and classification methods when the data is ideal.\nHowever, they also deteriorate faster in the presence of covariate shift, concept shift and information loss\nthrough aggregation. Intuitively, the model-driven regression methods make the best use of the available\ninformation, but the performance of these methods is more sensitive to deterioration in the quality of the\ninformation.", "Source": "Targeting Prospective Customers: Robustness of Machine-Learning Methods to Typical Data Challenges", "File": "Targeting Prospective Customers Robustness of Machine-Learning Methods to Typical Data Challenges.pdf"}
{"Date": 2024, "Claim": "Funding\nThis paper presented a literature review on the use of artificial\nintelligence in the context of lower-limb robot-aided rehabilitation.\nThisstudyidentifieswhichalgorithmsaremainlyusedtoaddressthe The author(s) declare financial support was received for the\ndifferentaspectssuchasrobotcontrol,walkingpatternclassification, research, authorship, and/or publication of this article. This work\nmotion interaction detection, and motion planning of the robotic was supported by the Italian Ministry of Research, under the\nsystem. It is worth noting that the devices currently used in clinical complementaryactionstotheNRRP\u201cFit4MedRob\u2014FitforMedical\nsettings do not integrate AI algorithms into their functioning even Robotics\u201d Grant (PNC0000007), CUP: B53C22006990001.\nifthescientificliteraturereviewedherehasdemonstratedhowthese\nmethodologies may improve robot-assisted walking.\nConflict of interest\nOntheoneside,thisreviewalsoshowedthatsomeAIalgorithms\nare suitable for solving specific problems, e.g., RL is only used for\nexoskeletal device control tasks, while other approaches are flexible The authors declare that the research was conducted in the\nto different application domains. On the other side, the take-home absence of any commercial or financial relationships that could be\nmessages emphasize the need for the scientific community working construed as a potential conflict of interest.", "Source": "AI-based methodologies for exoskeleton-assisted rehabilitation of the lower limb: a review", "File": "AI-based methodologies for exoskeleton-assisted rehabilitation of the lower limb a review.pdf"}
{"Date": 2022, "Claim": "In this work we analyzed the adversarial robustness of MoEs showing their advantage over dense\nmodels, with more experts leading to better robustness, both theoretically and empirically. We showed\nhow the properties of the data and its routing plays and important role in learning robust MoEs.\nWhile there is some evidence that routing learned by MoEs in practice display some clustering of\nthe features in some layers of the model (see  in Riquelme et al. ), it is currently not\nexplicitly encouraged during training. Hence developing smarter routing strategies that take data\ngeometry into account can be an interesting direction of future work. Currently our analysis is limited\nto linear models, extending this to general models and deriving the dependency of optimal routing on\nthem is another promising research direction.\nWe have also shown that, for inputs that weigh two experts similarly, if the two expert values are\nvery different, then the MoEs can suffer from higher Lipschitz constant. However, for models trained\nin practice, we saw their predictions to be relatively stable, despite significant changes in choice of\nexperts, highlighting potential redundancy of learned experts. However too much redundancy, with\nall experts learning similar functions is a waste of capacity and can affect model performance. Hence\nit is an interesting research problem to balance robustness and accuracy of MoEs by controlling the\nredundancy of experts.", "Source": "On the Adversarial Robustness of Mixture of Experts", "File": "On the Adversarial Robustness of Mixture of Experts.pdf"}
{"Date": 2022, "Claim": "it is required to address the ethics requirements of a project\nwill be similar to our approach. The nature of ALTAI as a\nThis paper presents the findings from a real-life test of the list of questions will tempt many projects to take a similar\nALTAI self-assessment. We engaged with a large number approach to the one we undertook, i.e. to ask the individual\nof scholars who can be described as working in the field of experts involved in AI development to go through these\nneuroinformatics to determine the extent to which the appli- questions. This means that our approach can be expected to\ncation of an assessment of AI trustworthiness (ALTAI) to be typical and therefore our insights are likely to be relevant\nresearch activities allow for the identification and mitigation more broadly.\nof social, ethical and technical benefits or problems of AI.\nBefore providing the answer to this research question, it is\nworth highlighting the limitations of our approach.\n1 3\nAI and Ethics (2023) 3:745\u2013767 757\n5.2 The value of assessing trustworthiness in AI the technical system in question.", "Source": "Assessing the ethical and social concerns of artificial intelligence in neuroinformatics research: an empirical test of the European Union Assessment List for Trustworthy AI (ALTAI)", "File": "Assessing the ethical and social concerns of artificial intelligence in neuroinformatics research an empirical test of the European Union Assessment List for Trustworthy AI ALTAI.pdf"}
{"Date": 2019, "Claim": "We presented a method to extract priors from a set of known tasks in the domain. The prior is learned in the\nform of a Q-function, and is based on inferred rewards corresponding to consistently undesirable actions\nacross these tasks. The effectiveness of the prior in enabling safe learning behaviors was demonstrated in\ndiscrete as well as continuous environments, and its performance was compared to various baselines. This\nwas further supported by our theoretical analysis, which suggests that the use of these priors helps reduce\nthe probability of taking unsafe exploratory actions. In addition to leading to safer learning behaviors for\narbitrary tasks in the domain, the priors were shown to be transferable to some extent, and capable of\nadapting to changes in the environment.", "Source": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning", "File": "Learning Transferable Domain Priors for Safe Exploration in Reinforcement Learning.pdf"}
{"Date": 2024, "Claim": "frontiers, the vision of transparent AI decision making moves\ncloser to realization .", "Source": "Transparency in AI Decision Making: A Survey of Explainable AI Methods and Applications", "File": "Transparency in AI Decision Making A Survey of Explainable AI Methods and Applications.pdf"}
{"Date": 2022, "Claim": "Effectiveness of Random vs. Scheduled Ordering. We\nevaluated the effectiveness of random versus scheduled or- In this paper, we proposed GAT, a generic approach for\ndering by conducting pairwise t-tests. Ten experiments were enhancing the robustness of deep learning models to compos-\nconducted with different initializations, and the experimental ite semantic perturbations, with the ultimate goal of prepar-\nresults on CIFAR10/Full-attack demonstrated that the robust ing classifiers for the real world. Our approach is based\naccuracy of the scheduled ordering was significantly lower on a unique design of attack order scheduling for multiple\nthan that of the random ordering (p-value < .001 for all perturbation types and the optimization of each attack com-\nmodels). ponent. This further enables GAT to achieve state-of-the-art\nInadequacies of Current Adversarial Robustness Assess- robustness against a wide range of adversarial attacks, in-\nments. Existing methods for evaluating adversarial robust- cluding those in \u2113 p norms and semantic spaces. Evaluated\nness, which only considers perturbations in \u2113 -ball, may on CIFAR-10 and ImageNet datasets, our results demon-\np\nbe incomplete and biased. To investigate this issue, we strate that GAT achieves the highest robust accuracy on most\ncompared the rankings of the top ten models on the Robust- composite attacks by a large margin, providing new insights\nBench dataset (CIFAR-10, \u2113 ) .", "Source": "Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations", "File": "Towards Compositional Adversarial Robustness Generalizing Adversarial Training to Composite Semantic Perturbations.pdf"}
{"Date": 2023, "Claim": "sibilities in AI technology must take centre stage going\nforwards. Although this paper has focused specifically on\nOur study not only brings awareness to the potential gen- gender bias, this is just one of many other areas of discrimi-\nder biases caused by the current AI technologies, but also nation evident in society which has the potential to be fil-\nhighlights the potential of mitigation of gender biases from tered or reflected through the prism of algorithms. As the\nthe AI technologies. As shown by UN and EU attempts to concept of intersectionality makes clear, the mitigation of\npropose new policies and principles to regulate the poten- one type of bias does not solve the issue of inequality\u2014a\ntial effects of AI on gender equality, policy makers are yet holistic effort to mitigate interacting biases is the only way\nto reach a consensus on how to balance AI\u2019s potential for to effectively improve the fairness of outcomes for all.\nempowering women with the possible detrimental effect it\nAcknowledgements The authors would also like to acknowledge the\ncould also have.\nadvice and suggestions given by Karen Lu, Graduate Assistant, Com-\nIt is clear that greater collaboration is necessary across\nmunications & Multimedia Laboratory, National Taiwan University,\ndifferent sectors affected by these issues. Tannenbaum et al. Taipei, Taiwan (R.O.C).", "Source": "Gender bias\u00a0perpetuation and mitigation in AI technologies: challenges and opportunities", "File": "Gender biasperpetuation and mitigation in AI technologies challenges and opportunities.pdf"}
{"Date": 2022, "Claim": "In this paper, we picture the landscape of real-world AI ethics incidents with an exploratory content analysis. Intelligent\nservice robots, language/vision models, and autonomous driving are the three application areas where AI failures occur\nmost frequently. AI ethics issues span all dimensions of people\u2019s life, including racial/gender discrimination, physical\nsafety, privacy leakage, etc. By closely inspecting AI ethics issues associated with the top four application areas, we\nprovide an example for AI practitioners to mitigate AI ethics issues in their work. We also relate AI ethics incidents to\nAI ethics guidelines, and provide a perspective for guideline makers to formulate more operable guidelines by analyzing\nreal-world incidents corresponding to the rules.", "Source": "AI Ethics Issues in Real World: Evidence from AI Incident Database", "File": "AI Ethics Issues in Real World Evidence from AI Incident Database.pdf"}
{"Date": 2021, "Claim": "\u201ccontain\u201d of values related to them (e.g., the value of friend-\nship contains such values as intimacy, fellow-feeling, com-\nmitment, openness of oneself to the friend), such that Bren- The time for this research is come. Baskerville\u2019s et al. (2020)\ntano\u2019s \u201claws,\u201d listed earlier, can be used to generate actions work speaks directly to the singularity, a time, where com-\nof AI agents that are preferable to other actions possible in a puters will have surpassed human abilities; they will be\ngiven context in which choices must be made among possi- beyond merely calculating mathematical proofs but will\nble courses of action, in such wise that the chosen action will actually possess human traits. They state that the time is\nbe aligned with the values and aims of persons operating rapidly approaching\u2014if not already here\u2014where engineer-\nin that context. Clearly, this alignment, and also the values ing will lose pride of place and be replaced by concerns\non which it operates and the human contexts in which it is more pertinent to the growth of the digital world first. The\napplied, must be part of an ongoing project, one guided by discussions of the emergence of digital reality in compari-\nthe principle outlined in the paper. The way is clear though son to physical reality pose new challenge for researchers to\nthe achievement is not yet given, for the complex and uncer- understand the impact of AI on human values (e.g.", "Source": "Aligning artificial intelligence with human values: reflections from a phenomenological perspective", "File": "Aligning artificial intelligence with human values reflections from a phenomenological perspective.pdf"}
{"Date": 2024, "Claim": "technological subjectivity. On this view, reflexivity, imagi-\nnation and embodied openness will find no purchase unless\nCoeckelbergh and Gunkel (2023) state that the \u201cperfor- grounded in the corpus of human expression.\nmances and materiality of text [\u2026] create their own meaning A practical programme of engagement might include an\nand value\u201d independently of who or what their performer is. embodiment with gradually widened modalities of agency\nHowever, it is my contention that assessing the productive and perception under human monitoring processes, simul-\nvalue of text is not enough when we are faced with powerful taneous with an ongoing dialogue as the AI becomes more\nagents that can pursue their own goals and prerogatives\u2014or complex and capable of realising the desiderata above.16 Its\nthose supplied by third parties\u2014with impunity and invis- ability to imagine new selves, values and strategies needs to\nibility. We need an understanding of how AI subjects can be tuned in conversation with us, and its forms of counter-\nbecome ethical agents that are also responsive to context conduct need to be circumscribed. This language deliber-\nand situation. Foucault\u2019s self-conducting subject, a subjec- ately echoes that of the Panopticon, because discipline, as\ntivity always-already embedded in a continuous political and Foucault so carefully described, is a key formative process.\nsocial contestation, offers an attractive possibility to emu- Hence the need for embodied self-care.", "Source": "ChatGPT: towards AI subjectivity", "File": "ChatGPT towards AI subjectivity.pdf"}
{"Date": 2024, "Claim": "privileged by AI industry with the multiple ways in which\nelements such as water come to matter for local communi-\nIn this article, I showed that no \u2018ethical\u2019 and \u2018sustainable\u2019 ties. While in this article, I have privileged an ontological\nAI would be possible as long the communities participat- approach, political economy perspectives could also shed\ning within AI value chain, and their ways of relating to the light on the essential role of elements such as water in what\nelements, are excluded from the design and development of AI companies do and how they obtain their profits.\nso-called \u2018intelligent\u2019 systems. Drawing on interviews and\nAcknowledgements This article would not have been possible without\nprotest material, I analysed how MOSACAT in Cerrillos and\nthe support provided by the Centre of Governance and Human Rights\nthe Council of Atacameno Peoples in the Atacama Desert\n(CGHR) at the University of Cambridge. I am also thankful to Rob\nresisting a Google data centre project and lithium extraction Sharp, Nick Couldry and the reviewer of this article for their insightful\nrespectively are mobilising water in order to mobilise their comments. I appreciate the engagement and feedback provided by peo-\nple at the Department of Media and Communications, London School\ncommunities against extraction.", "Source": "An elemental ethics for artificial intelligence: water as resistance within AI\u2019s value chain", "File": "An elemental ethics for artificial intelligence water as resistance within AIs value chain.pdf"}
{"Date": 2023, "Claim": "AI is an exciting technology with enormous risks and benefits. Until recently, the technology has\nbeen developed without sufficient consideration of its social implications. Given the pervasive\nnature of AI and consequential decisions it makes, governance policies are needed. In this chapter\nwe have outlined a framework that emphasizes the interdependencies among three stakeholders\n\u2013 governments, corporations, and citizens. Further, we offer competence, integrity and\nbenevolence dimensions of trust as lenses to examine the interrelationships among the\nstakeholder groups. We concluded by offering AI ethics as a mechanism to build trust among\nstakeholders and identify virtue ethics. Along with rule-based ethics, virtue ethics is a promising\napproach to nurture the right sensibilities required for AI governance.\nA Multilevel Framework for AI Governance 12", "Source": "A multilevel framework for AI governance", "File": "A multilevel framework for AI governance.pdf"}
{"Date": 2024, "Claim": "their societal engagement . To help alleviate those harmful\nimpacts, practitioners should proactively draw on existing meth- This study introduces the concept of user-driven value alignment\nin the context of AI companion applications. By analyzing 77 user\nods such as combining workplace strategies, clinical practices, and\ncomplaintpostsandconductingsemi-structuredinterviewswith20\ntechnological interventions to create comprehensive well-being\nexperienced users, we identified a wide range of perceived discrimi-\nsupport systems .\nnation statements in AI companion applications, users\u2019 conceptual-\n6.3.3 When the design implication is not to re-align ? There are izations of the reasons behind these discriminatory statements, and\nquestionsaboutwhetherweshouldchoosetore-aligncertainharm- seven user alignment strategies. We discuss design opportunities,\nful AI systems at all. User-driven value alignment can be considered challenges and raise open questions for how to better support and\na type of \u201crepair\u201d work  that users perform in their daily inter- incorporate user-driven value alignment in the design of future AI\nactions with AI companions to reduce biases. However, researchers systems,whereindividualusersandtheircommunitieshavegreater\nhave also pointed out that in some cases, \u201crefusal\u201d\u2014rather than agency.\n\u201crepair\u201d\u2014should be considered a more appropriate response to miti-\ngate the potential harm that systems can inflict on users .", "Source": "User-Driven Value Alignment: Understanding Users' Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions", "File": "User-Driven Value Alignment Understanding Users Perceptions and Strategies for Addressing Biased and Discriminatory Statements in AI Companions.pdf"}
{"Date": 2023, "Claim": "Medicine is a multidisciplinary endeavour. Generalist biomedical AI systems that effectively assimilate and\nencode multimodal medical data at scale and rapidly adapt to new clinical contexts are likely to be the\nfoundation of next generation learning health systems and make healthcare more accessible, efficient, equitable\nand humane. While further development and rigorous validation is needed, we believe Med-PaLM M represents\nan important step towards the development of such generalist biomedical AI.", "Source": "Towards Generalist Biomedical AI", "File": "Towards Generalist Biomedical AI.pdf"}
{"Date": 2024, "Claim": "In conclusion, the rise of AI in the Metaverse presents References\nexciting opportunities for human interaction and creativity,\nbut it also poses significant ethical challenges that demand 1. Adams, D., Bah, A., Barwulor, C., Musaby, N., Pitkin, K., & Red-\nthoughtful solutions. Through a study of real-world case miles, E. M. (2018). Ethics emerging: the story of privacy and\nsecurity perceptions in virtual reality. In Fourteenth Symposium\nstudies, we have explored the complexities of AI implemen-\non Usable Privacy and Security (SOUPS 2018) (pp. 427\u2013442).\ntation in virtual environments.\n2. Age Rating & Scene Reporting. (n.d.). Decentraland Documenta-\nKey ethical concerns revolve around responsible content tion. Retrieved February 14, 2024, from https://d ocs.d ecent ralan d.\nmoderation, safeguarding user data privacy, and promoting org/p layer/g enera l/i n-w orld-f eatur es/a ge-r ating-s cene-r eport ing/\n3. Ahmet, E.F.E.: The impact of artificial intelligence on social prob-\ninclusivity. Striking a balance between freedom of expres-\nlems and solutions: an analysis on the context of digital divide and\nsion and user safety is crucial in AI-driven content curation,\nexploitation. Yeni Medya 2022(13), 247\u2013264 (2022)\nand human-AI hybrid moderation approaches show promise 4. Allowed Experiences Controls. (n.d.). Roblox Help Center.\nin achieving this balance. Additionally, transparent data han- Retrieved Retrieved February 14, 2024, from https://e n.h elp.\nroblox.", "Source": "Ethical implications of AI in the Metaverse", "File": "Ethical implications of AI in the Metaverse.pdf"}
{"Date": 2024, "Claim": "require high levels of compute in many cases, making HMT\nresearch inaccessible to many researchers .\nThis review has shown that the SHC approach to AI\nThe spread of autoregressive large language models\nsystems suffers from a series of limitations accentuated by\nacross consumer applications and the establishment\nthe development and adoption of AI systems that rely, in\nof foundation models across domains also create an\nwhole or in part, on foundation models. This is because the\nopportunity to contribute to closing the testing gap, to the\nautonomy that AI confers to artificial agents is directly at\nextent that researchers can observe and test the performance\nodds with the assumptions of supervisory human control.\nof human\u2013AI configurations in new contexts and under\nHaving reviewed the literature on HMT, we argue that HMT\ndifferent levels of risk and benefit. Research focused on\noffers a better framework to develop an alternative approach\ntranslating the general gain in AI systems' capabilities\nto human control of foundation model-based AI systems that\nto improve human operators' ability to interact with and\nfocuses on bi-directional interactions and can be generalised\nleverage AI will be critical to unlocking the potential of\nto different areas of AI application.\nHMT-based approaches to human control.", "Source": "Human control of AI systems: from supervision to teaming", "File": "Human control of AI systems from supervision to teaming.pdf"}
{"Date": 2023, "Claim": "This research explored applying core principles of trustworthy AI to enhance resilience of intrusion detection systems\nagainst data poisoning attacks within healthcare IoMT environments. Augmenting conventional anomaly detector models\nwith capabilities for explainability, bias visibility, adversarial sample detection, and transparency yielded a profoundly more\nrobust system design.Evaluating the integrated solution against staged poisoning campaigns demonstrated significant\nsecurity advantages over legacy detectors vulnerable to compromise\u2014without degrading baseline predictive performance\non normal data. Output explainability built justified trust by allowing operators to disambiguate trusted vs untrusted\nbehaviors. SafeML classifiers reliably spotted even highly evasive sample manipulation attempts. Continuous bias tracking\n37 Aljanabi, Babylonian Journal of Internet of Things Vol.2023, 31\u201337\nenabled preemptively identifying and mitigating unfair performance gaps before they widened into exposures. And full\nlifecycle accountability supported external auditing to catch blindspots emerging during off-hours operation. Together these\ntrustworthy AI techniques eliminated blind trust dilemmas through context and cohesive interactions between humans and\nAI subcomponents. The redesigned system upheld reliability even when faced with overwhelming data corruption attempts.", "Source": "Safeguarding Connected Health: Leveraging Trustworthy AI Techniques to Harden Intrusion Detection Systems Against Data Poisoning Threats in IoMT Environments", "File": "Safeguarding Connected Health Leveraging Trustworthy AI Techniques to Harden Intrusion Detection Systems Against Data Poisoning Threats in IoMT Environments.pdf"}
{"Date": 2023, "Claim": "Open Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\nThe widespread availability of powerful smart mobile\ntion, distribution and reproduction in any medium or format, as long\ndevices presents opportunities for global accessibility of\nas you give appropriate credit to the original author(s) and the source,\nAI-powered medical applications. These applications can provide a link to the Creative Commons licence, and indicate if changes\nplay an essential role in empowering patients better manage were made. The images or other third party material in this article are\nincluded in the article's Creative Commons licence, unless indicated\ntheir conditions whilst enabling clinicians to make person-\notherwise in a credit line to the material. If material is not included in\nalized and effective clinical decisions. Therefore, improve\nthe article's Creative Commons licence and your intended use is not\noutcomes for patients. However, the rise of AI in healthcare permitted by statutory regulation or exceeds the permitted use, you will\ncan lead to disproportionate effects on marginalized com- need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://c reati vecom mons.o rg/l icens es/b y/4.0 /.\nmunities, possibly exacerbating health disparities. AI ethics\nhas garnered significant attention to ensure the ethical devel-\nopment, deployment, and usage of AI systems.", "Source": "Operationalising AI ethics through the agile software development lifecycle: a case study of AI-enabled mobile health applications", "File": "Operationalising AI ethics through the agile software development lifecycle a case study of AI-enabled mobile health applications.pdf"}
{"Date": 2023, "Claim": "In this paper, I have argued that LLM-providing companies are colonialist and behave as metropoles not only through\nmechanisms covered in prior research such as extractivism, automation, sociological essentialism, surveillance and\ncontainment, but also through a coloniality of knowledge built upon ethical essentialism that arises in the process\nof alignment. This specific coloniality in alignment is perpetuated through both the practices and the underlying\ntechnologies for alignment that the metropoles have developed and deployed. They deliver their models in a closed way\nthrough APIs and institute the values and guardrails that they want, not what user communities may want. In these\nvalues that they institute, they do not admit, in practices or in technologies, anything other than Western philosophy.\nBy doing so, they approach alignment with moral absolutism that only considers universal value systems and derogates\nnon-universal value systems. Moreover, they only permit values coming from explicit instruction-based knowledge\nManuscriptsubmittedtoACM\nDecolonial AI Alignment: Openness, Vi\u015bes.a-Dharma, and Including Excluded Knowledges 15\n. SystemdiagramofproposeddecolonialAIalignmentarchitecture.\nsystems. This criticism leads me to propose a decolonial alignment approach that dismantles each of the three identified\naspects of the coloniality of knowledge.", "Source": "Decolonial AI Alignment: Vi\u015besadharma, Argument, and Artistic Expression", "File": "Decolonial AI Alignment Vi\u015besadharma Argument and Artistic Expression.pdf"}
{"Date": 2024, "Claim": "The AP, as outlined at the beginning of this paper, poses a significant and urgent\nchallenge to the rapid advancements of AI technology for many reasons. For\ninstance, governments and authorities need to quickly adapt to a new technologi-\ncal landscape, and the AP makes this more challenging. We have argued that Witt-\ngensteinian ideas on rule-following and our alignment model offer a framework\nthat helps thinking about possible tools. The three dimensions of human alignment\n(individual, social, and cultural) should all be checked when developing alignment\n12 In fact, the notion of emergent bias, consisting in the application of algorithms in new contexts for\nwhich they were not devised (e.g., Friedman and Nissenbaum, 1996; Mann and Matzner, 2019), depends\non this ad hoc assessment to be conceptually sound: there cannot be a bias in new contexts without a nor-\nmative reference, and thus, the ad hoc adjustment is adapted as the normative reference (despite high lev-\nels of misalignment between humans themselves concerning such an assessment!). The exception to this\nis when AIs are trained under situations different from the real world situations where they are employed,\ndespite these real world situations being known and anticipated by humans who are aligned in the appli-\ncation of a rule in these situations.\n1 3\n80 Page 22 of 25 J. A. P\u00e9rez-Escobar, D. Sarikaya\nstrategies.", "Source": "Philosophical Investigations into AI Alignment: A Wittgensteinian Framework", "File": "Philosophical Investigations into AI Alignment A Wittgensteinian Framework.pdf"}
{"Date": 2023, "Claim": "We introduce Boundless DAS, a novel and effective method for scaling causal analysis of LLMs to\nbillions of parameters. Using Boundless DAS, we find that Alpaca, off-the-shelf, solves a simple\nnumerical reasoning problem in a human-interpretable way. Additionally, we address one of the main\nconcerns around interpretability tools developed for LLMs \u2013 whether found alignments generalize\nacross different settings. We rigorously study this by evaluating found alignments under several\nchanges to inputs and instructions. Our findings indicate robust and interpretable algorithmic structure.\nOur framework is generic for any LLMs and is released to the public. We hope that this marks a step\nforward in terms of understanding the internal causal mechanisms behind the massive LLMs that are\nat the center of so much work in AI.", "Source": "Interpretability at Scale: Identifying Causal Mechanisms in Alpaca", "File": "Interpretability at Scale Identifying Causal Mechanisms in Alpaca.pdf"}
{"Date": 2018, "Claim": "HR RCAN TOFlow TDAN\n. A failure case of the TDAN. The very deep SISR net- In this paper, we propose a one-stage temporal alignment\nwork: RCAN trained on the DIV2K can accurately recover the\nnetwork: TDAN for video super-resolution. Unlike previ-\nstructures of the shown image region in the city video frame, but\nous optical flow-based methods, which splits the temporal\nTOFlow and TDAN failed.\nalignment problem into two sub-problems: motion estima-\ntion and motion compensation, the TDAN implicitly cap-\nresults) than the compared state-of-the-art VSR networks. tures motion cues via a deformable sampling module at the\nThe comparison results show that our TDAN can robustly feature level and directly predicts aligned LR video frames\nhandle even unknown degradation, which further demon- from sampled features without image-wise wrapping oper-\nstrates the superiority of the proposed framework. ations. In addition, the TDAN is capable of exploring im-\nage contextual information. With the advanced one-stage\n5. Limitation and Failure Exploration temporal alignment design and the strong exploration capa-\nbility, the proposed TDAN-based VSR framework outper-\nAs discussed in Sec. 4.2, the resolution of HR video forms the compared flow-based state-of-the-art VSR net-\nframes in Vimeo Super-Resolution dataset is only 448 \u00d7 works. In the future, we would like to adopt the proposed\n256.", "Source": "TDAN: Temporally-Deformable Alignment Network for Video Super-Resolution", "File": "TDAN Temporally-Deformable Alignment Network for Video Super-Resolution.pdf"}
{"Date": 2021, "Claim": "nal self-assessment, with only the IEEE standards requiring\nany kind of external verification, and the two examples of\nThis work provides an analysis of the AI guidelines and public registers providing explicit transparency. In addition\nframeworks that have practical tools to operationalise ethi- to missing large stakeholder groups, the current set of AI\ncal concerns. By reviewing best practices from historical Guidelines and tools do not fully utilize the full range of\nframeworks created to assess the effects of technology on the techniques available, including: participation process, base-\nenvironment , information privacy , data protection line study, life-cycle assessment, change measurement or", "Source": "Putting AI ethics to work: are the tools fit for purpose?", "File": "Putting AI ethics to work are the tools fit for purpose.pdf"}
{"Date": 2022, "Claim": "At present, training certified adversarially robust deep learning models requires specialized tech-\nniques explicitly designed for the purpose of performing provably robust classification (Cohen et al.,\n2019). While this has proven effective, these models are extremely difficult to train to high accuracy,\nand degrade clean accuracy significantly.\nWe suggest an alternative approach is possible. By exclusively making use of off-the-shelf mod-\nels designed to be state-of-the-art at classification and image denoising, we can leverage the vast\nresources dedicated to training highly capable models for the new purpose of robust classification.\nPublished as a conference paper at ICLR 2023", "Source": "(Certified!!) Adversarial Robustness for Free!", "File": "Certified Adversarial Robustness for Free.pdf"}
{"Date": 2022, "Claim": "Taken together, we believe the research clusters described in this\nbrief give a good initial view into research progress on topics\nrelated to AI safety. It appears that robustness research forms the\nclearest and most cohesive category, with a large fraction of the\nwork in that category focused on adversarial examples.\nInterpretability and reward learning appear to be somewhat fuzzier\ncategories, though the clusters we identified do contain a\nsignificant amount of research in those areas.\nBased on the research clusters we identified, it appears that work\nin all these areas is growing at a significant pace worldwide. The\nUnited States appears to lead in every area, with China showing\nsubstantial growth in robustness research and the EU producing a\nlarge amount of interpretability work. Notably, despite the\nsignificant growth of the AI safety\u2013related clusters we describe\nhere, they still represent only a tiny fraction of total worldwide\nresearch on AI in general. We identified eight clusters containing a\nlittle over 15,000 papers; if we remove the \u201csafety\u201d component and\nconsider all research clusters that contain more than 50 percent AI\npapers, we instead find nearly 2,000 clusters containing over 1.9\nmillion papers. These numbers imply that safety research may\nmake up less than 1 percent of AI research overall.\nThe clusters we identified show some promising progress on\nsafety-related problems and may also point to some gaps.", "Source": "Exploring Clusters of Research in Three Areas of AI Safety", "File": "Exploring Clusters of Research in Three Areas of AI Safety.pdf"}
{"Date": 2023, "Claim": "ful content into the model\u2019s output. For instance, in a cap-\ntioning task, a small perturbation could lead to entirely dif-\nOur investigation into the adversarial robustness of the\nferent and inaccurate captions, potentially causing misun-\nOpenFlamingo model showed that it is highly susceptible\nderstanding or misinformation. Similarly, in visual ques-\nto perturbations on its visual inputs. Even slight perturba-\ntion answering tasks, manipulated images could lead to in-\ntions that are hardly visible for humans can fool the model\ncorrect or misleading responses, affecting decision-making\ninto poor performance on captioning and VQA tasks. More\nbased on the model\u2019s predictions.\nalarmingly, the targeted attacks presented in this paper al-\nThese outcomes are particularly concerning given the low an attacker to control the model\u2019s outputs, crafting a\nwide range of applications multi-modal models could be desired response that may be deceiving or harmful.\nemployed in, from aiding visually impaired individuals in The potential for targeted adversarial manipulation has\nunderstanding their surroundings to generating news arti- serious implications for end users. As model outputs are\ncles based on visual input. For news article generation, the often trusted implicitly, this vulnerability could thus lead to\nmodel can automate the process of interpreting images and the spread of misinformation or manipulation of user be-\ngenerating relevant text. However, the introduction of ad- havior.", "Source": "On the Adversarial Robustness of Multi-Modal Foundation Models", "File": "On the Adversarial Robustness of Multi-Modal Foundation Models.pdf"}
{"Date": 2023, "Claim": "In the absence of regulation, continued rapid development of highly capable foundation models may present\nsevere risks to public safety and global security. This paper has outlined possible regulatory approaches to\nreduce the likelihood and severity of these risks while also enabling beneficial AI innovation.\nGovernments and regulators will likely need to consider a broad range of approaches to regulating frontier AI.\nSelf-regulation and certification for compliance with safety standards for frontier AI could be an important\nstep. However, government intervention will be needed to ensure sufficient compliance with standards.\nAdditional approaches include mandates and enforcement by a supervisory authority, and licensing the\ndeployment and potentially the development of frontier AI models.\nClear and concrete safety standards will likely be the main substantive requirements of any regulatory\napproach. AI developers and AI safety researchers should, with the help of government actors, invest heavily\nto establish and converge on risk assessments, model evaluations, and oversight frameworks with the greatest\npotential to mitigate the risks of frontier AI, and foundation models overall. These standards should be\nreviewed and updated regularly.\nAs global leaders in AI development and AI safety, jurisdictions such as the United States or United Kingdom\ncould be natural leaders in implementing the regulatory approaches described in this paper.", "Source": "Frontier AI Regulation: Managing Emerging Risks to Public Safety", "File": "Frontier AI Regulation Managing Emerging Risks to Public Safety.pdf"}
{"Date": 2022, "Claim": "This work revisits the task of robust SSL for learning robust representation with-\nout labels. Discard the practice of seeking an optimal strategy to combine SSL\nand AT, we propose a novel two-stage framework termed DeACL. Our DeACL\nenables independent configuration for SSL and AT for achieving SOTA robust-\nness by using significantly smaller training resources. Extensive results confirm\nthe effectiveness and efficiency of our DeACL over existing single-stage frame-\nworks by a significant margin. Our findings also have non-trivial implications for\npushing (a) towards a more explainable solution for robust SSL and (b) towards\na unified perspective of understanding on semi/self-supervised AT regarding how\nto effectively exploit unlabeled samples for robust representation learning.\nAcknowledgments: This work was partly supported by the National Research\nFoundation of Korea (NRF) grant funded by the Korea government(MSIT) (No.\n2022R1A2C201270611)\nDeACL: Decoupled Adversarial Contrastive Learning 15", "Source": "Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness", "File": "Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness.pdf"}
{"Date": 2022, "Claim": "Conflict of interest The corresponding author states that there is no\nconflict of interest.\nThis paper has shown that it is important to evaluate how the\ndata collection process influences the final performance of the Open Access This article is licensed under a Creative Commons Attri-\nDL-system and the data ethics associated to it. We present an bution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long\nintroduction to the data collection process challenges present in\nas you give appropriate credit to the original author(s) and the source,\nthe radiology area that can hinder the amount and quality of the\nprovide a link to the Creative Commons licence, and indicate if changes\ndata available for DL-systems development. Following, some were made. The images or other third party material in this article are\ndata ethics points that are deemed of interest and of relevance included in the article's Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in\nin the context of data collection are presented. In particular, we\nthe article's Creative Commons licence and your intended use is not\nfocus on data bias and shifts, ownership, recollection, privacy,\npermitted by statutory regulation or exceeds the permitted use, you will\nsharing, synthetic data and model re-training.", "Source": "Deep learning in radiology: ethics of data and on the value of algorithm transparency, interpretability and explainability", "File": "Deep learning in radiology ethics of data and on the value of algorithm transparency interpretability and explainability.pdf"}
{"Date": 2022, "Claim": "In evaluating specific use-cases, we developed Z-Inspection\u00ae, a participatory pro-\ncess for the assessment of the trustworthiness of AI systems. The process begins by\ndescribing the tensions between ethical values using an open vocabulary and gradu-\nally narrows the options down to finally agree on the closed vocabulary description\nof an ethics framework, in our case the EU Ethics Guidelines for Trustworthy AI (AI\nHLEG, 2019). Our process allows for the inclusion of various experts from different\nbackgrounds and provides a structured way for them to find an agreement on ethical\nissues with AI systems while also including their viewpoints.\nWhile gaps in AI regulation remain, the Z-Inspection\u00ae process can provide\nimportant validation and ethical considerations in accordance with soft ethics guide-\nlines that go beyond hard legal requirements. With increasing regulation efforts, the\nZ-Inspection\u00ae process will necessarily evolve alongside the regulatory environ-\nments, and once regulation is in place, the lessons learned from assessments with\nZ-Inspection\u00ae can assist AI systems developers and users in navigating the legal\nand ethical requirements. Overall, broad and interdisciplinary subject matter exper-\ntise will be critical to making a valuable assessment of trustworthiness, something\nZ-Inspection\u00ae is able to efficiently provide, either in self-assessment or in supple-\nment to third-party assessments of bodies whose remit will be more focused.", "Source": "Lessons Learned from Assessing Trustworthy AI in Practice", "File": "Lessons Learned from Assessing Trustworthy AI in Practice.pdf"}
{"Date": 2022, "Claim": "Artificial intelligence continues to be developed and we, the biological minds,\nwill become increasingly surrounded by the digital minds. The major task for\nhumans is to stay in control in this unprecedented situation in the history\nof civilisation because we continue to carry the responsibility for what is\ngoing to happen. First of all, this means paying constant attention to the\nvalue alignment problem, especially its normative aspect. The technological\nside is evolving rapidly and we better keep up with our considerations on\nthe normative aspect of the problem. In this article we have analysed the\nproblem from different perspectives, basing the approach on some \u201cofficial\u201d\nunderstandings of the content of the main concepts used. The article\nproposes the novel idea that dealing with the value alignment problem\nrequires taking a fresh look at the concept of rationality and connecting\nthe new understanding of rationality that is based on the ideas of Nicholas\nMaxwell with developing digital minds. We suggest to proceed from here\nand ask whether the missing something argument would help us, although\nwe have great difficulty in specifying what exactly we are missing here. Is it\nTalTech Journal of European Studies\n94 Tallinn University of Technology (ISSN 2674-4619), Vol. 12, No.", "Source": "Artificial Intelligence, Value Alignment and Rationality", "File": "Artificial Intelligence Value Alignment and Rationality.pdf"}
{"Date": 2019, "Claim": "The paper exposed the film Bicentennial Man (1999) by picturing the realities of Andrew and his\npersonality. This is done first by utilizing a philosophical reflection with three points on the film\nconcerning the determinacy of Andrew as a person, the question of freedom, and then of death.\nNext, it tackled the case in the perspective of machine ethics. Machine ethics serves as a\nframework in which cases of the blurring distinctions of man and machine and the machines for the\nfuture can be captured. Machine ethics must stand as the principle that serves as law and limitation\nto any scientific advancement showing dangerous potentials. A machine in the growing progress of\nscience needs an ethical component that must serve as laws to limit the possible adverse effects of\nits production and eventual global use in the future.", "Source": "Reflecting on the Personality of Artificiality: Reading Asimov\u2019s Film Bicentennial Man through Machine Ethics", "File": "Reflecting on the Personality of Artificiality Reading Asimovs Film Bicentennial Man through Machine Ethics.pdf"}
{"Date": 2022, "Claim": "ability,muchtechnicalworkonalgorithmicfairnesshasattempted\ntoaddressunfairnessharmsbydevelopingtrainingalgorithmsthat Inthispaperwerevisited Nissenbaum\u2019s \u201cfourbarriers\u201dtoaccount-\nare robust to biased input data. The field of algorithmic fairness ability,withattentiontothecontemporarymomentinwhichdata-\ntherefore serves as anexamplethatchallenges thenarrative ofthe drivenalgorithmicsystems havebecomeubiquitousinconsequen-\ninvulnerability of the barriers. The technical community and its tialdecision-making contexts.Wehavedrawnonconceptualfram-\ninterlocutorshave demanded more from MLmodelers concerning ing from Nissenbaum\u2019s use of the concept of blameworthiness and\nthe treatment of unfair discrimination. The community has set ex- howitcanbealignedwith,ratherthancastinoppositionto,Bovens\u2019s\npectations concerning the necessity of interventions to root out workonaccountabilityasarelationalpropertyofsocialstructures[20,\nandcorrectforunfairness,therebyweakeningthebarriersofscape- 21]. We have demonstrated how data-driven algorithmic systems\ngoating or being attributed to \u201cbugs\u201d. This example could, and we heighten the barriers to accountability with regard to determin-\nbelieve should,encouragesimilar treatment ofotherissues likero- ing the conditions of blame, and have looked ahead to how one\nbustness and its relationship to privacy violations, or adversarial might endeavor to weaken the barriers. In particular, we have put\nML and its relationship to manipulation.", "Source": "Accountability in an Algorithmic Society: Relationality, Responsibility, and Robustness in Machine Learning", "File": "Accountability in an Algorithmic Society Relationality Responsibility and Robustness in Machine Learning.pdf"}
{"Date": 2022, "Claim": "tion . However, a recent study  found that simpler\nIn our study, we investigated the role of deep learning architectures such as CBR-LargeT provide comparable per-\nmodel complexity in adversarial robustness for medical formance to large ImageNet architectures on unperturbed\nimages and demonstrated that standard trained medical medical images. To this end, we evaluate the role of model\n(See figure on next page.)\n Saliency maps of standard trained CBR-LargeT (a, c, e) and Resnet50 (b, d, & f) for Chest X-ray, Dermoscopy & OCT datasets. Unperturbed\nimages with predicted labels and corresponding saliency maps were visualized on row 1 (no attack) for (a\u2013f), respectively. Imperceptibly perturbed\nimages with predicted labels and corresponding saliency maps were visualized on row 2 (PGD attack) for (a\u2013f), respectively. Saliency maps of\nCBR-LargeT are more concentrated on the regions of interest whereas Resnet50 includes attention regions that are more sporadic on areas that do\nnot contribute to the classification of the disease\nRodriguez et al. BMC Medical Informatics and Decision Making (2022) 22:160 Page 8 of 14\n (See legend on previous page.)\nR odriguez et al. BMC Medical Informatics and Decision Making (2022) 22:160 Page 9 of 14\ncomplexity using a family of four Resnet architectures and Fast gradient sign method\na five-layer Convolutional Neural Network (CNN).", "Source": "On the role of deep learning model complexity in adversarial robustness for medical images", "File": "On the role of deep learning model complexity in adversarial robustness for medical images.pdf"}
{"Date": 2021, "Claim": "In this paper, we have presented a novel approach ACCENT to address the adversarial robustness\nproblem of DNN models for code comment generation tasks, and demonstrated that the current\nmainstream code comment generation architectures are of poor robustness. Simply replacing identi-\nfiers which results in functionality-persevering and syntactically correct code snippets can degrade\nthe performance of these representative models greatly. Experiment results show that our method\ncan generate more effective adversarial examples on two public datasets across five mainstream\ncode comment generation architectures. In addition, we demonstrated that the adversarial examples\ngenerated by our method had better transferability. To improve robustness, we have also proposed\na novel training method. Our experimental results showed that this training method can achieve\nbetter performance in the code comment generation setting compared to the data augmentation\nmethod which has widely been used to improve robustness.\nIn the future, we plan to extend the existing framework and include more sophisticated, structure-\nrewriting based adversarial example generation techniques. More generally, we plan to explore the\nrobustness issues of machine learning models for other software engineering tasks.\nACMTrans. Softw. Eng. Methodol.,Vol. 0,No. 0,Article0. Publicationdate: .\nAdversarialRobustnessofDeepCodeCommentGeneration 0:27", "Source": "Adversarial Robustness of Deep Code Comment Generation", "File": "Adversarial Robustness of Deep Code Comment Generation.pdf"}
{"Date": 2007, "Claim": "We have constructed a Lagrangian relaxation of the multiple sequence alignment ILP\nformulation that allowed us to obtain strong bounds by solving a generalization of\nthe pairwise alignment problem. By utilizing these bounds in a branch-and-bound\nmanner we achieved running times that outperform all other exact or almost exact\nmethods. We plan to integrate our implementation into the software project SEQAN\ncurrently developed by the Free University of Berlin.\nBesides optimizing our implementation for speed an important issue in our future\nwork will be to extend the scheme to Volume and to Bundle algorithms. A more\nsophisticated Lagrangian heuristic for computing lower bounds in the bb nodes will\nbe necessary to be able to solve instances of larger size (compare instance 3pmg in\n).\nOpenAccess ThisarticleisdistributedunderthetermsoftheCreativeCommons AttributionNoncom-\nmercial License which permits any noncommercial use, distribution, and reproduction in any medium,\nprovided the original author(s) andsource are credited.", "Source": "A Lagrangian relaxation approach for the multiple sequence alignment problem", "File": "A Lagrangian relaxation approach for the multiple sequence alignment problem.pdf"}
{"Date": 2018, "Claim": "ROC curves and CMC curves of these two models. Exper-\nimental results show that the fine-tuning FCN model per- We have proposed a novel approach called Deep Spatial\nforms better than the pre-trained model, which indicates feature Reconstruction (DSR) to address partial person\nthat fine-tuning with DSR can learn more discriminative re-identification. To get rid of the fixed input size, the\nspatial deep features. Pre-trained model with softmax loss proposed spatial feature reconstruction method provides a\ntraining can only represent the probability of each class that feasibility scheme where each channel in the probe spatial\na person image belongs to. For the fine-tuning model, DSR feature map is linearly reconstructed by those channels\ncan effectively reduce the intra-variations between a pair of of a gallery spatial image map, it also avoids the trivial\nperson images of the same individual. alignment-free matching. Furthermore, we embed DSR\ninto FCN to learn more discriminative features, such that\n4.7. Evaluation on Holistic Person Image\nthe reconstruction error for a person image pair from the\nTo verify the effectiveness of DSR on holistic person re- same person is minimized and that of image pair from\nidentification, we carry out additional holistic person re-id different persons is maximized. Experimental results on\nexperiments on Market1501 dataset .", "Source": "Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-free Approach", "File": "Deep Spatial Feature Reconstruction for Partial Person Re-identification Alignment-free Approach.pdf"}
{"Date": 2023, "Claim": "AI has the potential to address challenges facing our world, stretching\nfrom environmental, public health, energy, efficiency, industrial productivity\nand to many other fields, but it is increasingly used to cement and maintain\ndivisions, which exacerbate existing forms of inequality.\nAI is a heterogeneous set of very powerful and very useful techniques for\ndiscovering complex connections, but that does not make it an intelligent\nsubject. The appearance of intelligence does not make a model intelligent,\nhowever surprising its results may be. The intelligence and confidence lie\nwith the specialists who design them, use them and are able to interpret their\nresults, like a doctor does when making a diagnosis.\nThe opacity of AI systems, rather than engendering trust, erodes the trust\nhumans have in machines, especially as machines move towards autonomous\ndecision-making (even more so if they apply self-learning). But ethics in AI\nshould not be limited to the development of transparent and fair systems.\nSustainable AI solutions must take into account other human dimensions\nsuch as the personal, social, legal, organisational and even institutional.\nThe proposition of Trustworthy AI is a distraction that can cover up the\nhidden work of underpaid employees, blur responsibilities and justify the\nmassive and unnecessary use of resources to create superfluous needs that\nfoster a society that privileges the benefit of the few.", "Source": "Trustworthy AI Alone Is Not Enough.", "File": "Trustworthy AI Alone Is Not Enough..pdf"}
{"Date": 2019, "Claim": "setting, because with PAR the information from the support\nset can be better exploited.\nWe propose a novel PANet for few-shot segmentation\nbased on metric learning. PANet is able to extract robust\n4.4. Test with weak annotations\nprototypes from the support set and performs segmentation\nWe further evaluate our model with scribble and bound- using non-parametric distance calculation. With the pro-\ning box annotations. During testing, the pixel-level annota- posed PAR, our model can further exploit the support infor-\ntions of the support set are replaced by scribbles or bound- mation to assist training. Without any decoder structure or\ning boxes which are generated from the dense segmentation post-processing step, our PANet outperforms previous work\nmasks automatically. Each bounding box is obtained from by a large margin.\none randomly chosen instance mask in each support image.\nAs  shows, our model works pretty well with very Acknowledgements Jiashi Feng was partially supported\nsparse annotations and is robust to the noise brought by the by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-\nbounding box. In 1-shot learning case, the model performs 133 and MOE Tier-II R-263-000-D17-112.\nReferences", "Source": "PANet: Few-Shot Image Semantic Segmentation With Prototype Alignment", "File": "PANet Few-Shot Image Semantic Segmentation With Prototype Alignment.pdf"}
{"Date": 2019, "Claim": "The realisation that there is a need to embed ethical considerations into the\ndesign of computational, specifically algorithmic, artefacts is not new. Samuel\n(1960), Wiener (1961) and Turing were vocal about this in the 1940s and 1960s\n(Turilli 2008). However, as the complexity of algorithmic systems and our reli-\nance on them increases (Cath et al. 2017), so too does the need to be critical\n(Floridi 2016a) AI governance (Cath 2018) and design solutions. It is possible to\ndesign things to be better (Floridi 2017), but this will require more coordinated\nand sophisticated approaches (Allen et al. 2000) to translating ethical principles\ninto design protocols (Turilli 2007).\nThis call for increased coordination is necessary. The research has shown that\nthere is an uneven distribution of effort across the \u2018Applied AI Ethics\u2019 typology.\nFurthermore, many of the tools included are relatively immature. This makes\nit difficult to assess the scope of their use (resulting in Arvan\u2019s 2018 \u2018moral-\nsemantic trilemma\u2019) and consequently hard to encourage their adoption by the\npractically-minded ML developers, especially when the competitive advantage of\nmore ethically-aligned AI is not yet clear. Taking the time to complete any of\nthe \u2018exercises\u2019 suggested by the methods reviewed, and investing in the develop-\nment of new tools or methods that \u2018complete the pipeline\u2019, add additional work\nand costs to the research and development process.", "Source": "From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices", "File": "From What to How An Initial Review of Publicly Available AI Ethics Tools Methods and Research to Translate Principles into Practices.pdf"}
{"Date": 2023, "Claim": "In this article, we first introduced the GSE problem and proved that it is more general than three\ncommon safe RL problems. We then proposed MASE to optimize a policy under safety constraints\nthat allow the agent to execute an emergency stop action at the sacrifice of a penalty based on the\n\u03b4-uncertainty qualifier. As a specific instance of MASE, we first presented GLM-MASE to theoretically\nguarantee the near-optimality and safety of the acquired policy under generalized linear CMDP\nassumptions. Finally, we provided a practical MASE and empirically evaluated its performance in\ncomparison with several baselines on the Safety Gym and grid-world.\nAcknowledgments and Disclosure of Funding\nWe would like to thank the anonymous reviewers for their helpful comments. This work is partially\nsupported by JST CREST JPMJCR201 and by JSPS KAKENHI Grant 21K14184.", "Source": "Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms", "File": "Safe Exploration in Reinforcement Learning A Generalized Formulation and Algorithms.pdf"}
{"Date": 2016, "Claim": "arXiv preprint arXiv:1511.07289 (2015).\nThe existence of adversarial examples limits the areas in", "Source": "Towards Evaluating the Robustness of Neural Networks", "File": "Towards Evaluating the Robustness of Neural Networks.pdf"}
{"Date": 2022, "Claim": "Our work begins the formal study of reward hacking in reinforcement learning. We formally define\nhackability and simplification of reward functions, and show conditions for the (non-)existence of\nnon-trivial examples of each. We find that unhackability is quite a strict condition, as the set of all\npolicies never contains non-trivial unhackable pairs of reward functions. Thus in practice, reward\nhacking must be prevented by limiting the set of possible policies, or controlling (e.g. limiting)\noptimization. Alternatively, we could pursue approaches not based on optimizing reward functions.", "Source": "Defining and Characterizing Reward Hacking", "File": "Defining and Characterizing Reward Hacking.pdf"}
{"Date": 2023, "Claim": "In this paper, we proposed a simple but effective alignment framework, Reward rAnked FineTuning (RAFT),\nfor aligning generative models to human preference using a reward function. Compared to the popular\nPPO algorithm, RAFT is easy to implement and tune with a simple parameter configuration, and typically\nconverges more robustly and faster than the DRL approach PPO because of the SFT-like training feature.\nAnother notable distinction between RAFT and the on-policy PPO is the decoupling of data generation and\nfine-tuning processes. This decoupling enables RAFT to be implemented 1) with less GPU memory source\nand 2) flexibly in terms of data sources and collection strategies.\nAnother potential advantage of RAFT is its interpretability. We can interpret RAFT as iteratively learning\nfrom the induced best-of-K policies. In our study, we have demonstrated that the performance of RAFT\nheavily depends on the quality of the data set derived from the best-of-K policy, which depends on the\nhyper-parameter choices. In a broader context, any strategies for improving inference, such as prompt\nengineering and advanced generation strategies, can also be integrated into the RAFT framework to further\nboost the performance of aligned models. Furthermore, the clear learning objective of RAFT enables us to\nmitigate the fundamental issue of reward hacking (Michaud et al., 2020; Tien et al., 2022), which is a common\nconcern in RLHF.", "Source": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment", "File": "RAFT Reward rAnked FineTuning for Generative Foundation Model Alignment.pdf"}
{"Date": 2024, "Claim": "We have argued for a new AI safety approach: shutdown-seeking AI. The approach\nis quite different from other goal engineering strategies in that it is not an attempt\nto design AGIs with aligned or human-promoting final goals. We\u2019ve called our\napproach one of \u2018beneficial goal misalignment,\u2019 since a beneficial shutdown-seeking\nAI will have a final goal that we do not share, and we will need to engineer its envi-\nronment so that it pursues a subgoal that is beneficial to us. This could, in some\ncircumstances, make a shutdown-seeking AGI less useful to us than we like. If it is\nable to develop a dangerous capability (e.g., to disobey our orders), it may be able\nto shut down before doing what we want. But this \u2018limitation\u2019 is a key benefit of the\napproach, since it can function as a \u2018trip-wire\u2019 to bring a dangerous AGI that has\nescaped our control into a safe state. We have also argued that the shutdown-seeking\napproach may present us with an easier version of the specification problem, avoid\ndangerous entrenchment behavior, and pose less of a problem of manipulation than\nits opponents have thought. While there are still difficulties to be resolved and fur-\nther details to work out, we believe that shutdown-seeking AI merits this further\ninvestigation.\nFunding No funding to report.\nShutdown-seeking AI 1579\nData availability No data to report.\nDeclarations\nConflict of interest No competing interests to report.", "Source": "Shutdown-seeking AI", "File": "Shutdown-seeking AI.pdf"}
{"Date": 2023, "Claim": "Insummary,wehopethatthispaperservesasareference\nFor years now, the ever-growing capabilities of AI- for researchers, practitioners and neophytes who are new to\npowered systems have stimulated debates about the impact, the world of AI, with interest in trustworthy AI from a holis-\nbenefits, implications and risks brought by AI systems to tic perspective. A well-rounded analysis of what trust means\nthe industry and society. The ground-breaking potential of in AI-based systems and its requirements as the one offered\nlarge generative AI models such as ChatGPT and GPT4 has in this manuscript is a key for the design and development\nreinvigorated this debate, since their near general-purpose of responsible AI systems throughout their life cycle. We\ncapabilities learned from multimodal data can support a should not regulate scientific progress, but rather products\nwide variety of intended and unintended purposes and and its usage. As we emphasize in this paper, regulation\ntasks, by generating content that is hardly distinguishable is the key for consensus, and for this purpose, trustworthy\nfrom that made by humans.", "Source": "Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation", "File": "Connecting the Dots in Trustworthy Artificial Intelligence From AI Principles Ethics and Key Requirements to Responsible AI Systems and Regulation.pdf"}
{"Date": 2022, "Claim": "for ACT[S] are omitted for being insufficiently robust.\nOur method overwhelmingly outperforms the previous In this paper, HM efficiently and flexibly creates adver-\nmethods in terms of the overall performance. Namely, our sarial examples for adversarial training; LGA specifies an\nmethod efficiently reaches the highest ERS with a very low \u201cintermediate\u201d destination hardness for balancing robustness\ndecrement in R@1 under a fixed training cost. HM[R,M] and performance on benign examples; ICS loss term further\nor HM[R,g ] can reach an even higher ERS, but are improves model robustness. The state-of-the-art defenses\nLGA\nexcluded from comparison due to significant drop in R@1. have been surpassed in terms of overall performance.\nReferences", "Source": "Enhancing Adversarial Robustness for Deep Metric Learning", "File": "Enhancing Adversarial Robustness for Deep Metric Learning.pdf"}
{"Date": 2024, "Claim": "Future works along these lines include the integration with\nWe presented a novel approach combining the Deep Re- a network simulator and a testbed for a more realistic perfor-\ninforcement Learning (DRL) and a Control Barrier Function mance evaluation. We also plan to address more challenging\n(CBF) to guarantee safe exploration and exploitation in the environments where, for instance, QoE needs to be optimized\ncontext of Software Defined-Wide Area Network (SD-WAN). and other constraints need to be handled.\nTackling a typical load balancing problem where latency needs\nto be optimized while meeting safety requirements in terms of\ncapacity constraints, our DRL-CBF approach is able to achieve ACKNOWLEDGMENT\nnear-optimal performance with safe exploration and exploita-\ntion. Furthermore, we show that on-policy optimization based This work was partially supported by the French Nation\non PPO achieves better performance than off-policy learning Research Agency (ANR) SAFE project under grant ANR-21-\nwith DDPG. We implemented all the algorithms on GPU to CE25-0005.\nREFERENCES Learning - Volume 32, ser. ICML\u201914. Beijing, China: JMLR.org, Jun.\n2014, pp. I\u2013387\u2013I\u2013395.", "Source": "Towards Safe Load Balancing based on Control Barrier Functions and Deep Reinforcement Learning", "File": "Towards Safe Load Balancing based on Control Barrier Functions and Deep Reinforcement Learning.pdf"}
{"Date": 2023, "Claim": "them to process input from diverse domains without exten-\nsive training. At the same time, their unpredictable outputs\nraise concerns. The risk drivers here identified for LLMs This risk assessment model offers two contributions. First, it\ncan be easily inferred from the AIA, e.g., from the new Arti- enhances AIA enforcement by facilitating the development\ncle 4a, which contains \u2018General principles applicable to all of more sustainable and effective risk management meas-\nAI systems\u2019. Of course, applying our proposed assessment ures for national regulators and AI providers, while pursuing\nmodel during the AIA implementation stage would neces- the AIA\u2019s objective of protecting the EU values. Second, it\nsitate enhanced legislative transparency in setting the drivers favors a granular regulation of GPAIs using scenario-based\nand interactional risk types. risk assessment to adapt to their versatile and uncertain\napplications.\n1 The issue generated a major debate, resulting in the proposal of Curmudgeon Corner Curmudgeon Corner is a short opinionated col-\numn on trends intechnology, arts, science and society, commenting\na series of amendments to the draft AIA: https:// www. consi lium.\non issues of concernto the research community and wider society.\neuropa.", "Source": "Taking AI risks seriously: a new assessment model for the AI Act", "File": "Taking AI risks seriously a new assessment model for the AI Act.pdf"}
{"Date": 2023, "Claim": "learning models for mental health. These forthcoming studies,\nenriched with an expansive array of human-interpretable features\nIn our work, we aimed to address the lack of systematic spanning the entire spectrum of language behavior, can draw\nassessments of explainability methods in the domain of mental from our work as a cornerstone. This will facilitate cross-data-set\nhealth detection. To the best of our knowledge, our work is comparisons of linguistic markers, spanning diverse mental health\nthe first to provide a comprehensive assessment across five conditions.Thesynthesisofinsightsgarneredfromsuchendeavors\nprominent mental health conditions (MHCs) using Natural will culminate in a methodical compendium of digital language\nLanguage Processing and Machine Learning. Our overall objective biomarkers. These markers, demonstrated to hold predictive\nwas to advance state-of-the-art research on explainable AI (XAI) efficacy across a spectrum of studies and contextual nuances, can\napproaches to automatic mental health detection from language empowermentalhealthpractitionersintheearlyidentificationand\nbehavior leveraging textual data from social media. continuous monitoring of mental health disorders.\nIn pursuit of this objective, we present a comprehensive\narray of XAI methodologies, leveraging their individual strengths\nwhile acknowledging their limitations.", "Source": "Toward explainable AI (XAI) for mental health detection based on language behavior", "File": "Toward explainable AI XAI for mental health detection based on language behavior.pdf"}
{"Date": 2024, "Claim": "Companies need to ensure data collected is accurate,\nfair, representative, and legally sourced. As the prin- We have seen that implementation of generative AI comes\nciple of autonomy demonstrates, there should be con- with considerable cyber security risk for businesses. When\nsiderations of how much users can have a say about rushing to implement generative AI and not fall behind oth-\nhow their data is used in the training of a model. Com- ers in industry, companies are also increasing the risk for\npanies should consider whether they will need to have cyber security breaches. While there is a great momentum\ntoward incorporating generative AI, there also needs to be a\n1 3\n802 AI and Ethics (2024) 4:791\u2013804\n2. IBM: The CEO\u2019s Guide to Generative AI: Supply chain. IBM:\nconsideration of the ethical responsibility toward the protec-\n(2023)\ntion of data and prevention against threats. 3. Carlini, N., Tram\u00e8r, F., Wallace, E., Jagielski, M., Herbert-Voss,\nA major risk with the rush to market of generative AI is A., Lee, K., Roberts, A., Brown, T.B., Song, D.X., Erlingsson,\n\u00da., Oprea, A., Raffel, C.: Extracting Training Data from Large\nits adoption by workers without guidance or understanding\nLanguage Models. In: USENIX Security Symposium. (2020)\nof how various generative AI tools are produced, managed\n4. McKinsey & Company: The Economic Potential of Genera-\nor of the risks they pose. This lack of understanding can tive AI: The next Productivity Frontier.", "Source": "AI hype as a cyber security risk: the moral responsibility of implementing generative AI in business", "File": "AI hype as a cyber security risk the moral responsibility of implementing generative AI in business.pdf"}
{"Date": 2023, "Claim": "input signal with text\u2019s.\nIn this study, we systematically evaluate the susceptibility\n4.5. Towards Real-World Application: Context-\nof LMMs to visual adversarial inputs across a diverse ar-\nAugmented Image Classification\nray of tasks and datasets. Our findings suggests LMMs are\nIn the previous section, we show that adding the correct highly vulnerable to visual adversarial attacks, even when\nobject context enhances LMMs\u2019 robustness against adver- such adversaries are crafted with respect to the visual model\nsarial images. In practice, the correct context is typically alone. On the other hand, we find that LMMs are \u201crobust\u201d\nunknown. However, in the case of closed-world image when the query and attack target does not match. Such\nclassification, where the list of object classes are fixed, we characteristics indicates that the traditional task-specific ad-\ncan decompose each question into multiple existence ques- versarial generation techniques are not universally effec-\ntions. Each question queries the presence of one object tive against current LMM, and points to the need for fur-\nclass, along with the context corresponding to that object. ther research into new adversarial attack strategies, particu-\nAfterwards, we choose the object with the highest confi- larly in the context of zero-shot inference. Finally, we find\ndence from the LLM\u2019s final projection head. We term our adding context about the querying object improves LMMs\u2019\napproach query decomposition. visual robustness.", "Source": "On the Robustness of Large Multimodal Models Against Image Adversarial Attacks", "File": "On the Robustness of Large Multimodal Models Against Image Adversarial Attacks.pdf"}
{"Date": 2023, "Claim": "This article addresses the issue of conceptualizing theory and practice in applied AI\nethics approaches. The guiding question was how to mediate between the what and\nthe how of AI ethics. Three exemplary applied AI ethics approaches served as refer-\nences to explore this question. Therefore, we hermeneutically analyzed the under-\nstanding of theory and practice as well as their entanglement in each approach. The\ndifferent notions of theory and practice revealed the distinctive emphases and the\nguiding element of each conception. Moreover, delineating the specific potentiali-\nties and shortcomings of the approaches rendered the conceptualization of theory\nand practice a multi-faceted endeavor. Against this background, we have proposed\nthree reflective dimensions: first, affects and emotions; second, justifications; and\nthird, governance aspects. We argue in terms of critical theory that these dimen-\nsions provide a meta-framework understood as a reflective tool to understand, map,\nand assess current applied AI ethics conceptualizations of theory and practice. This\nmeta-framework aims to address and overcome the blind spots of these theory\u2013prac-\ntice conceptualizations by reflecting on experiences of discrimination and margin-\nalization, power structures, and the embeddedness of digital practices in political\ndiscourses.\nAcknowledgements We are grateful to Alina Oeder, Clara Odendahl, and Eva Maria Hille for their help-\nful feedback on earlier versions of this paper.", "Source": "Reflections on Putting AI Ethics into Practice: How Three AI Ethics Approaches Conceptualize Theory and Practice", "File": "Reflections on Putting AI Ethics into Practice How Three AI Ethics Approaches Conceptualize Theory and Practice.pdf"}
{"Date": 2022, "Claim": "The AI ethics research community faces two intertwined challenges: In the first place, we have a tech industry heavily\ninfluencing the AI ethics research agenda. Secondly, the AI ethics research community is busy cleaning up after the tech\nindustry and anticipating AI ethical problems lurking on the horizon. We have turned to value-driven design methods to\npro-actively bring ethics to the design of technology. But by framing research questions relevant to a technical practice,\nwe have facilitated the technological solutionism behind the tech industry\u2019s business model. Our research efforts have,\nof course, been relevant in response to the problems brought about by the cocktail of an overpromising tech industry\nand the political and societal embracement of technology. However, it is about time we take steps to reshape the AI\nethics research agenda and dedicate ourselves to an emancipatory framework by increasingly voicing and focusing on\nthe power dynamics exacerbated by AI, and by siding up with those most negatively affected by AI. In doing so, we face\nthe challenge of teaming up with the weaker part while avoiding our interventions being rooted in academic ignorance\nnurtured by a paternalistic know-all attitude.\nIn sum, we need to settle whether our role is that of an integrator taking care of ethics by applying theoretical lenses\nor by engaging in design practices in which we provide locally anchored value-based design for and with specific stake-\nholders.", "Source": "The tech industry hijacking of the AI ethics research agenda and why we should reclaim it", "File": "The tech industry hijacking of the AI ethics research agenda and why we should reclaim it.pdf"}
{"Date": 2022, "Claim": "models predict neural responses in higher visual cortex.\nTo conclude, we encourage researchers to conceptually separate Proc Natl Acad Sci US A 111, 8619-8624 (2014).\nthe objectives of AI and neuroscience while interpreting the\nparameters and operations of current computational models. 2. Schrimpf, M. et al. The neural architecture of language:\nHowever, we also suggest that-although current AI models Integrative modeling converges on predictive processing.\noperate differently from primate brains in various ways-all Proc. Natl. Acad. Sci. U.S.A. 118, e2105646118 (2021).\nelse being equal, interpretability methods in AI that provide\nmore primate-brain-aligned model interpretations are likely to 3. Schrimpf, M. et al. Brain-Score: Which Artificial\nbe more promising. Neural Network for Object Recognition is most Brain\nLike?http:/ /biorxiv.org/lookup / doi/10.1101/ 407007\n(2018) doi:10.1101/ 407007.", "Source": "Interpretability of artificial neural network models in artificial intelligence versus neuroscience", "File": "Interpretability of artificial neural network models in artificial intelligence versus neuroscience.pdf"}
{"Date": 2020, "Claim": "periment is conducted to examine the robustness of the\ntrained networks on PGD attack. While the PGD attack In this paper, we proposed Learn2Perturb, an end-to-\nis more powerful in fooling the networks, results show end feature perturbation learning approach for improving\nthat the network designed and trained by the proposed adversarial robustness of deep neural networks. Learned\nLearn2Perturb framework still outperforms other state-of- perturbation injection modules are introduced to increase\nthe-art approaches. uncertainty during both training and inference to make it\nharder to craft successful adversarial attacks. A novel al-\n4.5. Expectation over Transformation (EOT)\nternating back-propagation approach is also introduced to\nAthalye et. al  showed that many of the defense algo- learn both network parameters and perturbation-injection\nrithmsthattakeadvantageofinjectingrandomizationtonet- module parameters in an alternating fashion. Experimental\nwork interior layers or applying random transformations on results on both different black-box and white-box attacks\ntheinputbeforefeedingit to thenetworkachieverobustness demonstrated the efficacy of the proposed Learn2Perturb\nthrough false stochastic gradients. They further stated that algorithm, which outperformed the state-of-the-art meth-\nthese methods obfuscate the gradients that attackers utilize ods in improving robustness against different adversarial\nto perform iterative attacking optimizations. As such, they attacks.", "Source": "Learn2Perturb: An End-to-End Feature Perturbation Learning to Improve Adversarial Robustness", "File": "Learn2Perturb An End-to-End Feature Perturbation Learning to Improve Adversarial Robustness.pdf"}
{"Date": 2022, "Claim": "4.1. Research implications\nThe chief research contribution of this paper is the integration of AI governance layers,\ncomponents, and literature into an integrated model of organizational AI governance. These\ndisparate elements have previously been discussed in various literature areas, as shown in the\nprevious section.\nOur integrated conception of AI governance is illustrated by the hourglass model ( ), which comprises three layers: 1) environmental requirements, 2) organizational practices\nand capabilities, and 3) operational governance of AI systems. Each layer points to different\nresearch fields. The environmental layer touches on technology and innovation policy (Stahl,\n2022), AI ethics (Ib\u00e1\u00f1ez & Olmeda, 2021), and regulatory studies (Kaminski & Malgieri, 2020;\nViljanen & Parviainen, 2022), for instance. The organizational layer involves corporate\ngovernance (Harjoto & Jo, 2011), IT governance (Tiwana & Kim, 2015; Weill, 2008), and\nstrategic management (Brendel et al., 2021), among others. Finally, the AI system layer\ninvolves fields such as software engineering (Dennehy & Conboy, 2018), critical algorithm\nstudies (Kitchin, 2017; Ziewitz, 2016), and data governance (Abraham et al., 2019; Brous et\nal., 2016; Janssen et al., 2020). Together, the layers form a multidisciplinary area of study that\nresearchers need to understand to enable them to unpack the challenges and phenomena\nsurrounding AI governance.", "Source": "Putting AI Ethics into Practice: The Hourglass Model of Organizational AI Governance", "File": "Putting AI Ethics into Practice The Hourglass Model of Organizational AI Governance.pdf"}
{"Date": 2022, "Claim": "time on the site, they\u2019ll click on less ads, they\u2019ll make less\nmoney\u2019 (Milmo 2021a).\nGoing to the press or accessing whistle-blowing outlets The main aim of this paper was to identify the values that\nis neither simple nor easy for AI practitioners. For instance, guide AI practitioners in their roles, how they view the AI\ntheir former organisations may have staunchly denied such ethics of their organisations, and what happens when there\naccusations, in a similar way as Mark Zuckerberg has with is a tension or conflict between the two. Through a review of\nHaugen\u2019s allegations (Milmo 2021b). In addition to this, the literature and an analysis of three workshops, we investi-\nthere is also the fear of a public smear campaign against gated how AI practitioners negotiate and mediate ethical val-\nthem by their former employer or that it becomes too dif- ues in their workplace and the challenges and resistance they\nficult to obtain work after such allegations, affecting their face when attempting to initiate change. We also explored\nlong-term career prospects. Individuals who quit their job several suggestions of steps that could be taken (both inter-\nand publicly whistle-blow about their former employer\u2019s nally and externally) when there is an ethical dilemma or\nunethical behaviour should feel they have the agency to do challenge.", "Source": "An AI ethics \u2018David and Goliath\u2019: value conflicts between large tech companies and their employees", "File": "An AI ethics David and Goliath value conflicts between large tech companies and their employees.pdf"}
{"Date": 2024, "Claim": "For example, trust in autonomous vehicles is dynamic (Luo et al.,\n2020) and easily swayed by mass media (Lee et al., 2022). Furthermore,\nmedia portrayals often lack objectivity, with companies overstating This article provides a comprehensive review and analysis of\nautonomy levels in promotions, whereas media primarily reports on factors influencing trust in AI and offers insights and suggestions\naccidents. Therefore, ensuring balanced and factual media on the development of trustworthy AI. The three-dimension\nrepresentations is essential to foster an environment where people can framework of trust is applicable for understanding trust in\ndevelop informed trust in autonomous vehicles. Moreover, interpersonal relationships, human-automation interactions, and\nimplementing sensible legislation and regulations, as well as clarifying human-AI systems. The framework can also help understand user\nresponsibility in accidents involving autonomous vehicles, is vital for needs and concerns, guide the refinement of AI system designs, and\npublic endorsement. aid in the making of policies and guidelines on trustworthy AI. All\nHigh-Level Expert Group on Artificial Intelligence (AI HLEG) of these shall lead to AI systems that are more trustworthy,\n(2019) delineated seven crucial requirements for trustworthy AI: increasing the likelihood for people to accept, adopt, and use\nhuman agency and oversight, technical robustness and safety, privacy them properly.", "Source": "Developing trustworthy artificial intelligence: insights from research on interpersonal, human-automation, and human-AI trust", "File": "Developing trustworthy artificial intelligence insights from research on interpersonal human-automation and human-AI trust.pdf"}
{"Date": 2024, "Claim": "like qualities such as the use of pronouns, cues, expressions\nof care and interest for the user, and even a semblance of\nreflection (e.g., the use of a quotation mark around the term With the widespread adaptation of GenAI tools across all\n\u201cme\u201d). We now observe that, in everyday language, people sectors of society, including higher education and pub-\noften reproduce these ideas referring to ChatGPT as \u201che\u201d lic administration, it is important to critically examine\nor \u201cshe.\u201d how these technologies are understood. This is especially\nTaken together, these images (1) encourage overlooking crucial to address and correct misleading assumptions\nthe ethical and legal challenges posed by trending GenAI (re)produced in images about them. The present study\ntechnologies, (2) overestimate their capabilities, and (3) examined the output of variations on the prompt \u201cCreate\npotentially lead to mistakenly perceive them as trustwor- an image of yourself\u201d in ChatGPT 4 and 4o to answer\nthy companions. While these issues need further empirical the question: What are dominant themes and features in\nvalidation, any intervention regarding the representation of images generated by ChatGPT when prompted to visualize\nGenAI would only be addressing the symptoms of a more itself? A semiotic analysis of the images and text result-\nstructural problem of how socio-technical imaginaries are ing from these prompts was conducted, revealing three\nshaped, how they gain prominence, and eventually find main themes:...", "Source": "\u201cYour friendly AI assistant\u201d: the anthropomorphic self-representations of ChatGPT and its implications for imagining AI", "File": "Your friendly AI assistant the anthropomorphic self-representations of ChatGPT and its implications for imagining AI.pdf"}
{"Date": 2022, "Claim": "We present CnC, a two-stage contrastive learning approach to learn representations robust to spurious\ncorrelations. We empirically observe, and theoretically analyze, the connection between alignment and worst-\ngroup versus average-group losses. CnC improves the quality of learned representations by making them more\nclass-dependent and less spurious-attribute-dependent, and achieves state-of-the-art or near-state-of-the-art\nworst-group accuracy across several benchmarks.", "Source": "Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations", "File": "Correct-N-Contrast A Contrastive Approach for Improving Robustness to Spurious Correlations.pdf"}
{"Date": 2023, "Claim": "5.3.2 Experiment 2: with uninformed prior. Comparing the aver- In this study, we developed and compared three robot interaction\nage workload (1) across the three interaction strategies showed strategies: the non-learner strategy, the non-adaptive-learner strat-\na significant difference (\ud835\udc39(2,46) = 10872, \ud835\udc5d < 0.001).  egy, and the adaptive-learner strategy, with and without an in-\nshows the participants\u2019 responses on each dimension. There were formedpriorfortheIRLlearningalgorithm.Wefocusedonevaluat-\nsignificant differences between the three strategies in performance ing their influence on various human trust in the robot, agreement,\n(\ud835\udc39(2,46) = 5.443, \ud835\udc5d = 0.008), effort (\ud835\udc39(2,46) = 4.252, \ud835\udc5d = 0.02), reliance, workload, and team performance.\nHRI\u201924,March11\u201314,2024,Boulder,CO Bhat,etal.\n6.1 Value Alignment with an Informed Prior scenarios involving conflicting objectives. For instance, a rehabili-\nOne critical insight from the research is the observed \u201cuniformity\u201d tation robot must balance a patient\u2019s pain tolerance with long-term\nacross all three interaction strategies when the IRL algorithm is health goals when assigning the appropriate level of exercise.\ninitiated with an informed prior. In Experiment 1, the informed Our study involved a relatively homogeneous participant group,\nprior was calculated by training on a previously collected dataset leading to the calculation of a fairly accurate informed prior.", "Source": "Evaluating the Impact of Personalized Value Alignment in Human-Robot Interaction: Insights into Trust and Team Performance Outcomes", "File": "Evaluating the Impact of Personalized Value Alignment in Human-Robot Interaction Insights into Trust and Team Performance Outcomes.pdf"}
{"Date": 2022, "Claim": "In this work, we propose and analysis the weakly misalignment problem in mul-\ntispectral aerial detection. Then we explore a TSRA module based multispec-\ntral detector named TSFADet to alleviate the weakly misalignment problems.\nSpecifically, we present a new alignment process, which predicts the deviations\nof position, size and angle to solve the misalignment caused by device factors.\nMeanwhile, the MS strategy is designed to address the problem caused by human\nfactors. Moreover, we adapt a Multi-task Jitter to further improve the robustness\nof TSRA module. Our detector can be trained with an end-to-end manner and\nachieves state-of-the-art accuracy on the DroneVehicle dataset. The proposed\nmethod can be generalized to other multispectral detection task and facilitate\npotential applications.", "Source": "Translation, Scale and Rotation: Cross-Modal Alignment Meets RGB-Infrared Vehicle Detection", "File": "Translation Scale and Rotation Cross-Modal Alignment Meets RGB-Infrared Vehicle Detection.pdf"}
{"Date": 2021, "Claim": "discourse ethics where the aim of AI ethics frameworks\nis to guide open discussions in which all sides of an argu-\nment are listened to and considered until a decision that Identifying the optimum mechanisms for implementing\nis acceptable to all can be reached (Morley et al. 2020a, the tenets of AI ethics will take time and is likely to be an\nb). It is the discussion, and the process followed to ensure ongoing task that will require regular reflection as both the\nthis discussion is held in an open, transparent, and \u2018fair\u2019 field of AI ethics and AI technology itself develop. In other\nway, that is important. Both Whittlestone et al. (2019) words, the implementation of AI ethics should be under-\nand Terzis (2020) discuss the complexities of encounter- pinned by a learning governance model where regular reflec-\ning tensions in AI ethics, in more detail. tion on impact is embedded in the research and decision-\nmaking cycle and overseen by those most affected by AI\nproducts and yet excluded from the development pipeline\u2014\n5 Limitations lay members of the public, especially those from tradition-\nally marginalised groups (Banner 2020). If those involved in\nAll research has limitations, and this is no exception.", "Source": "Operationalising AI ethics: barriers, enablers and next steps", "File": "Operationalising AI ethics barriers enablers and next steps.pdf"}
{"Date": 2024, "Claim": "how safe and trustworthy AI should be achieved. As discussed\nin Section II-A, trustworthiness, safety, and transparency are In this paper, we investigated the applications of XAI for\noverarching concepts that require the cross-disciplinary col- safe and trustworthy AD. We began the survey by defining\nrequirements for trustworthy AI in AD, noting that XAI is a", "Source": "Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review", "File": "Explainable AI for Safe and Trustworthy Autonomous Driving A Systematic Review.pdf"}
{"Date": 2022, "Claim": "ment (Tamkin et al., 2021; Kenton et al., 2021;\nEveritt et al., 2018). By continuously asking hu- In this work, we proposed SENSEI, a novel training\nman feedback during evaluation, Christiano et al. framework aimed at aligning LM generation with\n(2017) are able to train an RL agent that is aware human values. Given offline alignment datasets\nof human preferences. Irving et al. (2018) attempt with human demonstrations, SENSEI jointly learns\nto address AI safety and ethics problems by us- a reward distributor (Critic) and a conditional gen-\ning two RL agents to debate and have it judged erator (Actor). Compared to several baselines,\nby humans. Aiming to tackle larger scale align- SENSEI shows superior performance on three hu-\nment problems, researchers have tried to decom- man value alignment datasets and additional bene-\npose the problem into sub-problems (e.g., recur- fits for transfer learning of unseen human values.\nsively summarizing chapters of a book to align Future work could explore more fine-grained hu-\nwith human preference) (Wu et al., 2021), or de- man values and the value transfer ability of SENSEI\nploy a sequence of models while keeping humans on a larger scale. Another direction is to further\nin the loop (Leike et al., 2018). All these meth- study the integration of SENSEI with full-size foun-\nods can be seen as online alignment methods, as dation models, like GPT-3 (175B), or DeepMind\u2019s\nthey require human periodic human involvement. Gopher (Rae et al.", "Source": "Aligning Generative Language Models with Human Values", "File": "Aligning Generative Language Models with Human Values.pdf"}
{"Date": 2013, "Claim": "We described new strategies that can be composed to\nproduce a powerful local search strategy for the Tree\nAlignment Problem. The results showed that our meth-\nods improve on the best existing local search heuristics by\nmore than three orders of magnitude.\nVaro\u00b4nandWheelerBMCBioinformatics2013,14:66 Page11of12\nhttp://www.biomedcentral.com/1471-2105/14/66\nIn general, the Exhaustive\u2013TBR refinement strategy 4. OgdenTH,RosenbergMS:Alignmentandtopologicalaccuracyofthe\nshould always be used, while Union-pruning should only directoptimizationapproachviaPOYandTraditionalPhylogenetics\nviaClustalW+PAUP\u2217.SystBiol2007,56(2):182\u2013193.\nbe preferred if dense taxon sampling or short branch\n5. LehtonenS:PhylogenyEstimationandAlignmentviaPOYversus\nlengths are expected. Moreover, although the MST build Clustal+PAUP\u2217:AresponsetoOgdenandRosenberg(2007).Syst\nstrategy yields better results than the traditional Wagner Biol2008,57(4):653\u2013657.\n6. LiuK,NelesenS,RaghavanS,LinderCR,WarnowT:Barkingupthe\nbuild, the former should not be preferred in real analyses\nwrongtreelength:theimpactofgappenaltyonalignmentandtree\nsince it tends to produce less competitive trees after the accuracy.IEEETransComputBiolBioinf2008,6:7\u201320.\nrefinement step. 7. YueF,ShiJ,TangJ:Simultaneousphylogenyreconstructionand\nmultiplesequencealignment.BMCBioinf 2009,10(Suppl1):S11.\nIt is difficult to predict the performance of other high\n8. WheelerWC,AagesenL,ArangoCP,FaivovichJ,GrantT,D\u2019HaeseC,Janies\nlevel heuristics applied to the GTAP.", "Source": "Local search for the generalized tree alignment problem", "File": "Local search for the generalized tree alignment problem.pdf"}
{"Date": 2024, "Claim": "This work shows that the bidirectional LSTM algorithm has more accuracy for sentiment analysis.\nThe features used by random forest model for classification are not much accurate. The XAI technique such\nas LIME demonstrates the working of a black box model such as random forest classifier. The use of LIME\nincreases accuracy of sentiment analysis. Thus, the explainability helps in better understanding a model and it\ncan be used to improve the model performance which in turn increases the reliability and credibility of the AI\nsystems. In future the explainability techniques such as SHAP can be implemented over various machine\nlearning models.", "Source": "Use of explainable AI to interpret the results of NLP models for sentimental analysis", "File": "Use of explainable AI to interpret the results of NLP models for sentimental analysis.pdf"}
{"Date": 2021, "Claim": "Weight Consolidation (EWC) is an established technique for\nminimising the extent of the forgetting during model adap-\ntation (Kirkpatrick et al. 2017). Since it is desirable for the Although philosophical reflections on modern technologies\nadapted NMT system to produce outputs that achieve com- have been common ever since Martin Heidegger published\npetitive BLEU scores, EWC was applied during adaptation his Die Frage nach der Technik in 1954, recent advances in\nwhen the performance on the original validation set dropped. machine learning have created unprecedented practical and\n gives the results for two NMT systems adapted from theoretical scenarios that require careful ethical scrutiny.\nthe baseline NMT system using the Tiny dataset, the first Advocating a cautious form of technological utopianism,\nwithout EWC and the second with EWC. this article has argued that it is potentially beneficial to cre-\nWhile fine tuning on either Tiny dataset improves gen- ate language-based AI systems that are less biased than the\nder accuracy significantly, in both cases general translation communities which produced the data used to train them.\nperformance as measured by BLEU decreases. For Tiny, In some ways, systems of this kind can be compared with\nthe best-performing system on WinoMT, the BLEU score familiar devices such as speed cameras, which are material\nis 1.", "Source": "The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing", "File": "The practical ethics of bias reduction in machine translation why domain adaptation is better than data debiasing.pdf"}
{"Date": 2023, "Claim": "In this paper, we find that the vanilla fine-tuned (VFT) LLMs with chain-of-thought (COT) reasoning\nprocesssufferfromanassessmentmisalignmentproblem, i.e, theyfailtoaccessthequalityofdifferent\nCOTs of the learned questions, which hinders the reasoning ability of LLMs. To this end, we propose\nan alignment fine-tuning (AFT) paradigm. Our AFT consists of a novel constraint alignment loss that\ncan align the model assessment behaviors without harming the model performance. Furthermore, we\nalso delve deeply into recent widely used ranking losses for alignment and find that the constraint,\nwhich has been overlooked by these approaches, is also crucial for their performance. Extensive\nexperiments on four reasoning benchmarks demonstrate the effectiveness of AFT. In addition, AFT\nalso performs well in multi-task and out-of-distribution situations.\n8 LIMITATIONS\nOur paper has some limitations, which should be discussed in future works: 1) Due to the resource\nlimit, we do not scale the AFT to larger LLMs such as 65B and 70B LLama models. However, we\nbelieve that larger models still suffer from the assessment misalignment problem of VFT, and thus\nAFT can improve the performance of these larger models; 2) Our boundary constraint alignment loss\nincorporates a hyper-parameter \u03b2 that regulates the constraint strength, significantly impacting the\nmodel\u2019s performance. Finding the optimal hyper-parameter requires constructing a validation set\nand a certain search overhead.", "Source": "Making Large Language Models Better Reasoners with Alignment", "File": "Making Large Language Models Better Reasoners with Alignment.pdf"}
{"Date": 2020, "Claim": "The global pandemic of the severe acute respiratory syndrome Coronavirus 2 (SARS-CoV-2) has\nbecome the primary national security issue of many nations. Advancement of accurate prediction\nmodels for the outbreak is essential to provide insights into the spread and consequences of this\ninfectious disease. Due to the high level of uncertainty and lack of crucial data, standard\nepidemiological models have shown low accuracy for long-term prediction. This paper presents a\ncomparative analysis of ML and soft computing models to predict the COVID-19 outbreak. The\nresults of two ML models (MLP and ANFIS) reported a high generalization ability for long-term\nprediction. With respect to the results reported in this paper and due to the highly complex nature of\n33 of 39\nthe COVID-19 outbreak and differences from nation-to-nation, this study suggests ML as an effective\ntool to model the time series of outbreak. We should note that this paper provides an initial\nbenchmarking to demonstrate the potential of machine learning for future research.\nFor the advancement of higher performance models for long-term prediction, future research\nshould be devoted to comparative studies on various ML models for individual countries. Due to the\nfundamental differences between the outbreak in various countries, advancement of global models\nwith generalization ability would not be feasible. As observed and reported in many studies, it is\nunlikely that an individual outbreak will be replicated elsewhere .", "Source": "COVID-19 Outbreak Prediction with Machine Learning", "File": "COVID-19 Outbreak Prediction with Machine Learning.pdf"}
{"Date": 2023, "Claim": "We conduct an exhaustive analysis of adversarial robustness in camera-based 3D object detection models.\nOur findings reveal that a model\u2019s robustness does not necessarily align with its performance under normal\nconditions. Through our investigation, we successfully pinpoint several strategies that can enhance robust-\nness. We hope our findings will contribute valuable insights to the development of more robust camera-based\nobject detectors in the future.\nPublished in Transactions on Machine Learning Research (01/2024)\nAcknowledgement\nThis work is partially supported by a gift from Open Philanthropy and UCSC Office of Research Seed Fund-\ning for Early Stage Initiatives. This work is based upon the work supported by the National Center for\nTransportation Cybersecurity and Resiliency (TraCR) (a U.S. Department of Transportation National Uni-\nversity Transportation Center) headquartered at Clemson University, Clemson, South Carolina, USA. Any\nopinions, findings, conclusions, and recommendations expressed in this material are those of the author(s)\nand do not necessarily reflect the views of TraCR, and the U.S. Government assumes no liability for the\ncontents or use thereof.", "Source": "On the Adversarial Robustness of Camera-based 3D Object Detection", "File": "On the Adversarial Robustness of Camera-based 3D Object Detection.pdf"}
{"Date": 2023, "Claim": "tions (See Appendix M for the study and the details of the\nquestions) about their interpretability of AdvEX-RL policies. In this paper, we introduced an alternative view on safety\nA summary of the accuracy of the users\u2019 answers to each learning for RL through our task-agnostic safety framework\nquestion is shown in Figure.5. AdvEx-RL. We empirically showed that AdvEx-RL is effec-\nIn questions 1,2,5, we asked the users to identify optimal tive in ensuring safety even in uncertain conditions. Through\nactions the agent will take at a certain state with its task pol- a user study conducted on 41 non-technical end users, we also\nicy (Q1), task policy under attack (Q2), and task policy under demonstrated the transparency of AdvEx-RL by explaining\nattack but with AdvEx-RL\u2019s safety policy (Q3). The partici- its behavior using safety-CAPS. In future work, we plan to\npants demonstrated a good understanding of the environment, extend this framework to multi-agent settings along with an\nwith an accuracy rate above 80% in those questions. Notably, explainable safety method.\nProceedingsoftheThirty-SecondInternationalJointConferenceonArtificialIntelligence(IJCAI-23)\nAcknowledgments [Grigorescu et al., 2020] Sorin Grigorescu, Bogdan Trasnea,\nTiberiu Cocias, and Gigel Macesanu. A survey of deep\nThis material is based upon work supported by the National\nlearning techniques for autonomous driving. Journal of\nScience Foundation (NSF) under grant no. 2105007.\nField Robotics, 37(3):362\u2013386, 2020.", "Source": "Adversarial Behavior Exclusion for Safe Reinforcement Learning", "File": "Adversarial Behavior Exclusion for Safe Reinforcement Learning.pdf"}
{"Date": 2024, "Claim": "liberal rationality as well.\nAcknowledgements This work was supported by the Fritz Thyssen\nNational AI strategies articulate imaginaries of the integra-\nFoundation.\ntion of AI into society and envision the governing of AI\nresearch, development and applications accordingly. To inte- Funding Open Access funding enabled and organized by Projekt\ngrate these central aspects of national AI strategies under DEAL. Fritz Thyssen Foundation.\none coherent perspective, the paper presented an analysis of\nDeclarations\nGermany\u2019s strategy \u2018AI made in Germany\u2019 through the con-\nceptual lens of ordoliberal political rationality. By promot- Conflict of interest The author states there is no conflict of interest.\ning the guiding vision of a human-centric AI, the strategy\nnot only adheres to ethical and legal principles consistent Open Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\nwith Germany\u2019s liberal democratic constitutional system but\ntion, distribution and reproduction in any medium or format, as long\nalso addresses risks and promises in a way inherent to the\nas you give appropriate credit to the original author(s) and the source,\nordoliberal understanding of the conditionality of freedom. provide a link to the Creative Commons licence, and indicate if changes\nAgainst this background, the strategy cultivates the fear were made.", "Source": "Imagining and governing artificial intelligence: the ordoliberal way\u2014an analysis of the national strategy \u2018AI made in Germany\u2019", "File": "Imagining and governing artificial intelligence the ordoliberal wayan analysis of the national strategy AI made in Germany.pdf"}
{"Date": 2020, "Claim": "In this article we hope to have reaffirmed the ethical import of voice in literary\ntranslation, and drawn attention to the scant attention the translator\u2019s voice has\nreceived thus far in discussions of literary-adapted machine translation. We have\nproblematized the experimental settings used in recent studies of literary-adapted\nmachine translation, and made the case for a more ecologically valid approach.\nAgainst this background we have presented an initial study that attempts to\ninvestigate whether one translator\u2019s textual voice is manifest in his post-editing work\nand how it has been influenced by the use of machine translation. We also aimed to\ncapture the translator\u2019s contextual voice in a way that has not previously been\ndiscussed, to our knowledge, in the literature. The line-by-line analysis of textual\nvoice and comment-by-comment analysis of contextual voice that we propose is\nlabour-intensive yet sensitive enough to detect contrasting style-amplifying and style-\ndiminishing edits and non-edits. On the basis of our initial analysis of textual voice,\nwe tentatively conclude that the translator\u2019s voice is somewhat dampened in his post-\nediting work. We note, however, how he uses his contextual voice to justify his\ntranslation/post-editing choices by referring to source-language challenges, machine\ntranslation errors, narrative structures in German, and \u2013 crucially \u2013 elements of his\nown style.", "Source": "Machine translation, ethics and the literary translator\u2019s voice", "File": "Machine translation ethics and the literary translators voice.pdf"}
{"Date": 2020, "Claim": "A case study,\u201d in IEEE Symposium on Security and Privacy (S&P),\nThis paper describes the first step toward developing a pp. 197\u2013211, IEEE, 2014.", "Source": "Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors", "File": "Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors.pdf"}
{"Date": 2024, "Claim": "them to view the distant future as a simple extension of the\npresent. Second, they can be in professional near-sightedness\nThe paper aimed to present research in current and future of trending knowledge of AI, especially if they are they do\nAI-driven ICM process, surveying experts working in related not exclusively see themselves as responsible for the future\nfields and representing all world regions. Recalling the origi- directions. Both of these factors might be the reason behind\nnal assumptions, the experts have agreed on the first two: the (over-)optimism as well.\nThe over-optimistic scenarios express hopes and beliefs,\n\u2022 AI transforms ICM systems in several ways (Ellis and passing the responsibility on to future generations, technol-\nTucker 2020; Guzman and Lewis 2020; Chan-Olmsted ogy and nature-inspiration. Although we did not set out to\n2019; Gunkel 2019; Fletcher 2018; Waisbord 2018), and critique or question our study participants about their sense-\n\u2022 multifarious benefits are available from cost-effective making process in technology, techno-optimism holds risks\noperation to productive work (Mustak et al. 2023; Geor- and may fail to deal with worst-case scenarios (Del Rosso\ngieva et al. 2022; Preu et al. 2022; Wirtz 2020). 2014).\nSome themes do not have enough data to support them\nThe third assumption focused only on fake media, sys- or the data are too diverse (Braun and Clarke 2006).", "Source": "Modeling AI Trust for 2050: perspectives from media and info-communication experts", "File": "Modeling AI Trust for 2050 perspectives from media and info-communication experts.pdf"}
{"Date": 2024, "Claim": "understand, analyse and counteract the evolving landscape\nof AI-enabled cyber threats in a multidisciplinary approach.\nIt is predicated on the idea that a multifaceted approach is This paper explored the multifaceted dimensions of AI-\nessential to address the complex challenges posed by the driven cyberattacks, including their implications, strate-\nconvergence of AI technologies and cyberattacks [14, 27, gies, motivations, and societal impacts. The research study\n28, 93].  summarises the insights from the four main culminated in the development of the AICD Framework,\nresearch objectives of this paper and presents them as the which provides a holistic view of this evolving threat land-\nAICD Framework. scape. The analysis of offensive AI cyberattacks revealed\n AI Cybersecurity Dimensions Framework\n9 06 AI and Ethics (2025) 5:883\u2013910\ntheir intricate and dynamic nature, underscoring the need scholarly community, policymakers, and industry leaders\nfor adaptive, AI-infused defence mechanisms. Moreover, it will find value in the insights presented in this study and\nemphasises ethical considerations surrounding the design, galvanise collective action to secure our cyber future.\ndevelopment, and deployment of AI in cybersecurity. As\nFunding Open access funding provided by University of South Africa.\nAI-driven attacks grow in sophistication, defenders must\nstay a step ahead.", "Source": "Artificial intelligence (AI) cybersecurity dimensions: a comprehensive framework for understanding adversarial and offensive AI", "File": "Artificial intelligence AI cybersecurity dimensions a comprehensive framework for understanding adversarial and offensive AI.pdf"}
{"Date": 2023, "Claim": "E. Realistic Experiments in Driving Scenarios\nWe propose a new offline-to-online training scheme named\nOur method is applicable to and effective in realistic sce- Guided Online Distillation for safety-critical tasks. A large-\nnarios, which we demonstrate by experiments on MetaDrive. scale guide policy is first extracted from offline demonstra-\nThese experiments are fairly close to real-world scenes tions. It serves as a guide for online distillation, where a\nbecause we make MetaDrive replay vehicle trajectories from lightweight policy is distilled through interactions with the\nWOMD. The observations input to the ego agent, including task environment. This lightweight network can meet com-\nLidar cloud points, navigation information, and ego states, putation speed requirements in realistic settings, in contrast\nalso resemble the real-world setting. The goal of the ego to the bulk guide policy. The guided distillation saves the\nvehicle is to arrive at a specific target position defined in policy from being repeatedly exposed to hazards during\nWOMD. We randomly choose 10k scenarios from WOMD its exploration to find useful skills, which improves its\nfor training and 1k scenes for testing. training efficiency and final performance. Experiments in\nAs shown in Tab. I, our method surpasses baselines by both benchmarks and real-world driving experiments based\naround 15% in reward and maintains the cost below the on the WOMD show that the distilled policy by GOLD\nthreshold.", "Source": "Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration", "File": "Guided Online Distillation Promoting Safe Reinforcement Learning by Offline Demonstration.pdf"}
{"Date": 2023, "Claim": "To close, a final remark: I suggested that the norms governing AI systems should be\ndetermined by the overall social function of the practices they participate in. This seems\nto be in the spirit of Simion and Kelp\u2019s general framework, especially since they (briefly)\noutline an etiological account of the functions of social practices. However, I also called\n5 I am grateful to Helene Scott-Fordsmand for this point. See Scott-Fordsmand (2022) and Maung (2019).\n1 3\nAsian Journal of Philosophy (2023) 2:40 Page 9 of 10 40\nthis a first approximation. The reason for my hesitation is as follows: like Haslanger, I\nworry that many social practices, in their current formation, in part function to serve\nunjust ends\u2014e.g. to facilitate and reinforce the exploitation of marginalised populations\nand scarce natural resources for the benefit of the already well off. For those who share\nthis worry, calling AI systems that merely preserve the current functions of social\npractices \u2018trustworthy\u2019 will be deeply unsatisfactory (to put it mildly).\nIn the final analysis, then, I would want my account to build in more substantive\nmoral commitments. One promising approach might be to adopt a normative theory of\nsocial practices. For example, one might define social practices as patterns of learned\nbehaviour that allow agents to coordinate their actions in ways that benefit everyone or\nthat allow them to hold each other appropriately accountable.6 How best to work out\nthese details, I am not yet sure.", "Source": "Trustworthy AI: a plea for modest anthropocentrism", "File": "Trustworthy AI a plea for modest anthropocentrism.pdf"}
{"Date": 2022, "Claim": "Gaining insights into how we can enhance human feedback for tasks in which the AI models\u2019 capabilities\nexceed those of humans will be crucial for scaling AI alignment in the future and will go far beyond the scope\nof summarization tasks. In order to have a sound methodology for the latter, we suggest starting by selecting\nexperts, fine-tuning the criteria, the corresponding framework, and the training to such an extent that the\nexpert-expert agreement will reach a certain threshold. Only then can one be satisfied with using the experts\u2019\nlabels as a baseline. Then, a small subset of AI trainers should be invited to label a subset of summaries. If the\nexpert-trainer agreement is not satisfying, we suggest introducing further sandwiching techniques.\nAssuming that the texts are not especially long, we suggest starting with a model explaining potential\ntechnical terms to AI trainers. If this still does not enhance the expert-trainer agreement to the same level as\nthe expert-expert agreement, we would consider giving humans feedback from an assisting LLM that is able\nto critique summaries. Only when we are satisfied with the expert-trainer agreement for this small subgroup,\none should recruit and train more AI trainers with the same measures and let them take part in the whole\nexperiment. We propose monitoring whether one must exclude AI trainers (see quality checks) and use the\ndata to train the reward model and the summarization model in a semi-online manner.", "Source": "Methodological reflections for AI alignment research using human feedback", "File": "Methodological reflections for AI alignment research using human feedback.pdf"}
{"Date": 2020, "Claim": "This paper has advanced several claims. To begin with, I argued that the machine\nlearning techniques we use for alignment, and the values we align with, are not fully\nindependent of one another. The way we build AI is likely to influence the values\nwe are able to load, and a clearer understanding of the value dimension can shape\nAI research in productive ways. A further consequence of this is that there is no\nway to \u2018bracket out\u2019 normative questions altogether. Instead they should form part\nof a combined research agenda. Turning then to the goal of alignment, I argued that\nwe should not aim to align AI with instructions, expressed intentions, or revealed\npreferences alone. Properly aligned AI will need to take account of different forms\nof unethical or imprudent behaviour, and incorporate design principles that prevent\nthese outcomes. One way to do this would be to build in objective constraints on\nwhat artificial agents may do. More useful still, would be a set of principles that sit-\nuate human direction within a moral framework that is widely endorsed despite the\nexistence of different belief systems. This requires work both in terms of the techni-\ncal specification of concepts from which principles are assembled, and also to iden-\ntify principles of the right kind. The third section focused on how such principles\ncould be selected and justified.", "Source": "Artificial Intelligence, Values, and Alignment", "File": "Artificial Intelligence Values and Alignment.pdf"}
{"Date": 2023, "Claim": "In this article, we surveyed causal modeling and reasoning tools for enhancing the trustworthy\naspects of AI models, which include interpretability, fairness, robustness, privacy, safety, and\naccountability. While in recent years, the community has witnessed an unprecedented surge of\nresearch in this context, important facets still remain unexplored. We expect significant advance-\nments in the coming years and hope this survey will act as an important resource to the community\nand at the same time guide future research connecting trustworthy AI and causality.", "Source": "A Review of the Role of Causality in Developing Trustworthy AI Systems", "File": "A Review of the Role of Causality in Developing Trustworthy AI Systems.pdf"}
{"Date": 2024, "Claim": "In conclusion, the integration of AI into plant sciences has ushered in a transformative era. It has opened up\nnew avenues for understanding and managing plant life, which is useful for ecological research, biodiversity\nconservation, disease detection, crop breeding, and sustainable agriculture. The potential of AI in this field\nis immense, but it comes with the responsibility of refining AI models for robustness and addressing ethical\nconsiderations. As we navigate this exciting journey, interdisciplinary collaboration remains the\ncornerstone that guides us toward a future that is not only more sustainable but also more food-secure. AI\nin plant sciences represents a powerful tool in our quest to feed a growing global population while\npreserving our environment.\nAbbreviations\nAI: artificial intelligence\nANN: artificial neural network\nGAs: genetic algorithms\nIoT: Internet of Things\nPCA: Principal Component Analysis\nSOM: Self-Organizing Maps\nDeclarations", "Source": "AI-powered revolution in plant sciences: advancements, applications, and challenges for sustainable agriculture and food security", "File": "AI-powered revolution in plant sciences advancements applications and challenges for sustainable agriculture and food security.pdf"}
{"Date": 2022, "Claim": "The emerging field of AI ethics is unlike other fields in applied ethics. At the center\nof its attention is not human conduct, but the ways in which humans are affected\nby AI technology. It differs from the general ethics of technology too, in the sense\nthat AI comes with radically new possibilities for action. This not only raises new\nmoral questions, but also requires new approaches to conduct ethical analyses. But\nthe most popular approach thus far\u2014that is, the principled approach\u2014has been met\nwith criticism. Although there is a lot of convergence when it comes to determining\nwhich ethical issues or principles should shape the development, policy, and use of\nAI, AI ethics principles have been accused of being too abstract, little action guid-\ning, and insufficiently attuned to the social and political context of ethical issues.\nThe aim of this paper was to show that AI ethics is just like a critical theory.\nI have explained that a critical theory is aimed at diagnosing and changing soci-\nety for emancipatory purposes. I then showed that both the big debates in AI\nethics and the most common AI ethics principles are fundamentally concerned\nwith either individual empowerment (dispositional power) or the protection of\nthose subjected to power relations (relational power).", "Source": "Why AI Ethics Is a Critical Theory", "File": "Why AI Ethics Is a Critical Theory.pdf"}
{"Date": 2024, "Claim": "Management Framework. International\nJournal of Management Information Systems\nThe integration of Natural Language Processing (NLP)\nand Data Science, 1(1), 31-40.\nand Artificial Intelligence (AI) in data visualization https://doi.org/10.62304/ijmisds.v1i1.115\nrepresents a significant advancement in the field,\nAlam, M. R. U., Shohel, A., & Alam, M. (2024b).\noffering enhanced data interpretability, dynamic\nIntegrating Enterprise Risk Management\nvisualizations, and real-time analytics capabilities. This\n(ERM): Strategies, Challenges, and\nreview has highlighted the substantial benefits of these\nOrganizational Success. International Journal\ntechnologies, including improved user interaction and of Business and Economics, 1(2), 10-19.\naccessibility through voice-based queries and https://doi.org/10.62304/ijbm.v1i2.130\ninteractive elements, as well as robust technological\nAmin, M. R., Younus, M., Hossen, S., & Rahman, A.\ninfrastructure supported by high-performance and cloud\n(2024). Enhancing Fashion Forecasting\ncomputing solutions. Despite these advancements,\nAccuracy Through Consumer Data Analytics:\nchallenges such as computational complexity, data Insights From Current Literature. Academic\nquality, ethical concerns, and organizational barriers Journal on Business Administration,\nremain critical issues that need addressing to maximize Innovation & Sustainability, 4(2), 54-66.\nhttps://doi.org/10.69593/ajbais.v4i2.69\nthe potential of NLP and AI in data visualization.", "Source": "A REVIEW OF UTILIZING NATURAL LANGUAGE PROCESSING AND AI FOR ADVANCED DATA VISUALIZATION IN REAL-TIME ANALYTICS", "File": "A REVIEW OF UTILIZING NATURAL LANGUAGE PROCESSING AND AI FOR ADVANCED DATA VISUALIZATION IN REAL-TIME ANALYTICS.pdf"}
{"Date": 2021, "Claim": "As Thomsen (2019) states, \u2018ethics for AI cannot be expected to be any simpler than\nethics for humans.\u2019 Indeed, it may be more complicated, since it adds to it further\ntechnical issues. Research ethics and medical ethics have always involved a com-\nbination of the law, ethical governance policies, practices, and procedures, with\ncontextual discursive and procedural support. This combination approach has ena-\nbled these branches of applied ethics to find a good balance between being too strict\nand too flexible, and between too centralised and too devolved. Therefore, it seems\nreasonable to hypothesise that AI ethics would benefit from an equally customis-\nable approach, and that if this balance can be achieved then the pro-ethical Design\nendeavour may succeed. At the very least shifting the focus of AI ethics away from\nprinciples to procedural regularity will make AI ethics seem more relatable to AI\npractitioners. Encouraging a procedural approach can, for example, help make the\nparallels between AI ethics and other quality assurance processes, such as safety\ntesting, clearer and thus make it more obvious why careful consideration needs to\nbe given to each Design decision. We hope that the idea of Ethics as a Service, as\noutlined in the article, has at least highlighted this.", "Source": "Ethics as a Service: A Pragmatic Operationalisation of AI Ethics", "File": "Ethics as a Service A Pragmatic Operationalisation of AI Ethics.pdf"}
{"Date": 2023, "Claim": "tion in our annotation studies, and we ensured that\nall our annotators get paid more than the minimum\nWe presented the Touch\u00e923-ValueEval Dataset for\nwage in the U.S.\nIdentifying Human Values behind Arguments, com-\nprising 9324 arguments manually labelled for 54\nvalues and 20 value categories. We detailed its", "Source": "The Touch\u00e923-ValueEval Dataset for Identifying Human Values behind Arguments", "File": "The Touch\u00e923-ValueEval Dataset for Identifying Human Values behind Arguments.pdf"}
{"Date": 2021, "Claim": "domains. PACS dataset contains four domains with differ-\nent image styles: art painting, cartoon, sketch, and photo. Existing interventions for adversarial robustness require la-\nWe follow the same leave-one-domain-out validation exper- bels and assume learning from a single domain. This hinders\nimental protocol as in . For each time, we select three their application in unsupervised domain adaptation. To\ndomains for training and the remaining domain for testing. make unsupervised domain adaptation robust, we introduced\nWe apply RFA to the SOTA DG method DecAug  and a simple, unsupervised and domain-agnostic method that\nreport results in . It illustrates that our method can does not require adversarial examples during training. Our\nalso significantly improve the robustness while maintaining method is motivated by the transferability of robustness. It\ngood clean accuracy in domain generalization. utilizes adversarially pre-trained models and adapts robust-\nMethod A (cid:41)W D (cid:41)W W (cid:41)D A (cid:41)D D (cid:41)A W (cid:41)A Avg. Method Source DANN DAN CDAN JAN MDD\nBaseline 91.40 98.74 100.00 92.17 73.06 74.47 88.31 Baseline 43.05 71.34 61.79 74.23 63.70 72.20\nRobustPT 91.78 99.12 100.00 92.77 73.85 74.11 88.60 Robust PT 47.20 72.81 62.56 75.85 63.02 75.64\nOurs(RFA) 92.80 99.21 100.00 93.04 78.00 77.74 90.15 Ours (RFA) 59.00 75.05 65.58 77.54 66.68 79.42\n(a) (b)\n. Improved Clean Accuracy.", "Source": "Adversarial Robustness for Unsupervised Domain Adaptation", "File": "Adversarial Robustness for Unsupervised Domain Adaptation.pdf"}
{"Date": 2024, "Claim": "Marx (1982) insists on the conceptual separation between\nhuman (living) labour and machines (see Braverman 1974;\nMarkelj and Celis Bueno 2023). The analyses of the con- As we write this essay in early 2024, the proliferation of gen-\ncepts of creative labour and automation presented above erative AI tools continues to gather steam, not least within\nspeak to this conceptual distinction and highlight a con- the context of the cultural and creative industries (e.g. film,\nstant struggle between labour and automation at the heart television, music, design, and advertising) and in spite of\nof the capitalist mode of production. Furthermore, Marx fierce debates about perceived threats to authorship and\n(1973) identified the core contradiction of capitalism as labour rights that somewhat speak back to the \u201cboosterist\u201d\nthat between a structural drive towards automation while discourses permeating the marketing of these technologies.\nmaintaining human labour as the sole source of value. The 2023 strikes in Hollywood organised by the WGA and\nIn this sense, generative AI might be understood as an the Screen Actors Guild (SAG) is a prime example of how\nattempt to replace creative labour while keeping human the (proposed) application of generative AI to the creative\nlabour as the foundation of value (in simple terms, this industries is presenting different challenges to various stake-\nmeans that creative workers still need to sell their labour holders.", "Source": "Not \u201cwhat\u201d, but \u201cwhere is creativity?\u201d: towards a relational-materialist approach to generative AI", "File": "Not what but where is creativity towards a relational-materialist approach to generative AI.pdf"}
{"Date": 2023, "Claim": "ever will be the trustee\u2019s behaviour, it will be bounded by\nethical requirements. It is doubtful that trust would have the\nsame role to play in the case of other artifacts, whose work- The notion of TAI has become increasingly important in the\ning could be largely dependent on human intervention and debate on AI. In this paper, we have considered the main\nwhose behaviour may be determined by specific instructions views in the philosophical literature on TAI, and we have\nor components. argued that they fail to allow for a meaningful and produc-\nThe final objection we wish to anticipate addresses the tive use of this notion. We have insisted on the importance of\nquestion of responsibility. The objection goes as follows: if a notion of TAI that captures both the epistemic and the non-\nthe notions of trust and trustworthiness are modified so as to epistemic dimensions of the design and use of AI systems.\nbe applicable to AI systems, we risk removing responsibility Moreover, by explicitly differentiating the notions of H \u2212 H\nfrom humans. Why not stretch instead the notion of reliance and H \u2212 AI trust, we have provided a conceptual framework\nto include moral elements? This way, we could keep using for talking about TAI without the risk of overly stretching\nthe notion of reliance for AI systems and we would avoid the concept of trust or making categorical mistakes.\nthe potential problems involved in the discourse on TAI.", "Source": "Keep trusting! A plea for the notion of Trustworthy AI", "File": "Keep trusting A plea for the notion of Trustworthy AI.pdf"}
{"Date": 2019, "Claim": "This paper has taken steps toward extending the analysis on the evolution of the\ncultural heritage sector by means of digital platforms and has discussed how digitization\nand big data are shaping this process by enabling new ways of creating value and of\nespousing the different types of interest expressed by the different types of stakeholders.\nOur findings document how Google Arts & Culture has been more effective than\nits main rival platform \u2013 Europeana \u2013 in competing on the variety, customization and\nexperimentation of artworks accessible online and in offering a one-stop-shop logic for\nall the relevant content and information. Specifically, our empirical evidence shows how\nGoogle Arts & Culture has enhanced the four drivers of value creation, namely efficiency,\ncomplementarities, lock-in and novelty, as defined by Amit & Zott (2001), more than\nEuropeana. The fact that Google\u2019s platform has been able to enact these drivers jointly is\nat the same time both the reason for and the consequence of having favoured a process of\nconvergence in the interests expressed by different stakeholders through the big data\u2019s\nsociotechnical features of interconnectivity and portability.\nIn raising this issue, our contribution is twofold. First, we contribute to research\non value creation from big data and its supporting technologies.", "Source": "When culture meets digital platforms: value creation and stakeholders\u2019 alignment in big data use", "File": "When culture meets digital platforms value creation and stakeholders alignment in big data use.pdf"}
{"Date": 2024, "Claim": "bustness is achieved with a sacrifice in accuracy of merely\n-2.3%. This observation suggests an attractive trade-off be- This work studies the adversarial robustness of VLMs from\ntween accuracy and robustness of our method. the novel perspective of the text prompt. We first demon-\nstrate that both adversarial attack and defense for VLMs are\n5.4. Reliability of Adversarial Evaluation\nsensitive to the used text prompt. We then propose Adver-\nTo verify that our evaluation of adversarial robustness is re- sarial Prompt Tuning (APT) to learn robust text prompts\nliable, we first additionally evaluate the adversarial robust- for CLIP based on adversarial examples to improve its ad-\nness of our methods using a diverse set of attacks including versarial robustness. Extensive experiments are conducted\nto demonstrate the effectiveness of APT for both the in- Percy Liang. On the Opportunities and Risks of Foundation\ndistribution performance and the generalization ability un- Models, 2022. arXiv:2108.07258 [cs]. 1\nder distribution shift and across datasets. APT is also", "Source": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-Trained Vision-Language Models", "File": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-Trained Vision-Language Models.pdf"}
{"Date": 2024, "Claim": "This study engages with making a case for instituting ethical governance of AI where it is\nbeing used, and adequate financing of AI for governance and human wellbeing. It also\nadvocates the leveraging of AI technologies for the attainment of effective governance and\nimproved human wellbeing. Leaning on the views of some scholars in the literature, the\nstudy shows evidently that its propositions can be achieved, if any government is willing. It\nis also shows that AI technologies are capable of addressing some governance issues and\nchallenges to human wellbeing. On the other hand, there are ethical concerns and adoption\nchallenges. It identifies ways of addressing the concerns and the challenges.\nIn addition, the study argues that to attain the envisaged goals, ethical governance of AI\nand adequate financing of AI are imperative. It submits that while ethical governance allows\nfor effective financing of AI and upholds human wellbeing in the era of AI, \u2018AI governance\u2019\nalongside unethical adoption and use paves way for ineffective and unjustified financing of\nAI and the negligence of human wellbeing. It charges the government of various nations to\nbe prudent, human-centered, and ethical in their adoption and financing of AI for purposes\nof effective governance and improved human wellbeing.", "Source": "Ethical AI Governance, Financing, and Human Well-Being in the 21st Century", "File": "Ethical AI Governance Financing and Human Well-Being in the 21st Century.pdf"}
{"Date": 2023, "Claim": "available datasets for many lung diseases such as the detection\nof Pneumoconoisis from CXRs, it is challenging to create large-\nscalemodelsfordifferentlungdiseases.Inaddition,anumberof CXR based image analysis is being used for detecting the\ndatasets are from a few specific countries like the USA. In order presence of diseases such as TB, Pneumonia, Pneumoconiosis and\ntobuildgeneralizablemodels,itisimportanttocreatelarge-scale COVID-19. This paper presents a detailed literature survey of\ndatasets with diversity. AI-based CXR analysis tasks such as enhancement, segmentation,\n\u2022 Small sample size problem and interoperability: Existing work detection, classification, image and report generation along with\nis done on fewer in-house collected chest X-ray samples. different models for detecting associated diseases. We also present\nDeveloping a robust and generalizable deep learning-based the summary of datasets and metrics used in the literature as well\nmodel requires a huge amount of training data. The datasets are as the open problems in this domain. It is our assertion that there\nverysmallinsizecomparedtogeneralobjectdetectionproblems is a vast scope for improving automatic and efficient algorithm\n(for instance, the ImageNet dataset). Since the scanners might development for CXR-based image analysis.", "Source": "AI-based radiodiagnosis using chest X-rays: A review", "File": "AI-based radiodiagnosis using chest X-rays A review.pdf"}
{"Date": 2023, "Claim": "In this work, we introduce Diffusion-DPO: a method that\nenables diffusion models to directly learn from human feed-\nback in an open-vocabulary setting for the first time. We\nfine-tune SDXL-1.0 using the Diffusion-DPO objective and\nthe Pick-a-Pic (v2) dataset to create a new state-of-the-art\nfor open-source text-to-image generation models as mea-\nsured by generic preference, visual appeal, and prompt\nalignment. We additionally demonstrate that DPO-SDXL\noutperforms even the SDXL base plus refinement model\npipeline, despite only employing 53% of the total model\nparameters. Dataset cleaning/scaling is a promising future\ndirection as we observe preliminary data cleaning improv-\ning performance (Sec. 5.4). While DPO-Diffusion is an of-\nfline algorithm, we anticipate online learning methods to be", "Source": "Diffusion Model Alignment Using Direct Preference Optimization", "File": "Diffusion Model Alignment Using Direct Preference Optimization.pdf"}
{"Date": 2022, "Claim": "The direction we\u2019re most excited about working on is in learning\nThis work had a broad scope in order to shed light on how an recommenders under competition. Especially the base case\nend-to-end study of reward functions for recommender systems where recommenders merely need to compete with the null or\nmight be done. Recommender alignment is a pressing and the local strategies. In this case, recommenders don\u2019t learn under\nimportant problem. Attempted solutions are sure to have far- the assumption that they have a monopoly on their populations.\nreaching impacts. The least we could do is develop methods Therefore, even if misaligned, they must learn to not be actively\nfor evaluating and comparing tentative solutions with respect detrimental, and a little better than the baseline in order to get\nto those impacts. We synthesized a simple abstract modelling adopted. We expect we\u2019ll be able to find an emergent \"bait-and-\nframework to guide future work. switch\" dynamic here that is also present in the real world, where\nafter widespread adoption, when it is more disadvantageous for\nNamely, a model must include an underlying environment, users\nusers to leave because of network effects, recommenders turn\nwith partial information, and RS with global information; no-\nto \"exploit mode\", caring less about user utility and more about\ntions of utility (derived from the environment) for individual\ntheir own.\nusers, society, and the RS; and a notion of recommendation.", "Source": "Modelling the Recommender Alignment Problem", "File": "Modelling the Recommender Alignment Problem.pdf"}
{"Date": 2023, "Claim": "Hence while it is still in the testing phase (OpenAI, 2022), we decided to ask ChatGPT how AI can\nbe made trustworthy.3\n\n\u201cInteracting\u201d with ChatGPT (6)\nThese are laudable suggestions, however, we should not consider this remarkable response as a\nrevelation straight from an AI system\u2019s mouth about what it would take for an AI system to be\nconsidered trustworthy. After all, the laudable suggestions that have found their way into\nChatGPT\u2019s training data would not have existed without the effort of scores of researchers and\nethicists who have worked hard to develop an ethical critique of current AI systems. This\ninformation should further be treated with the caveat that these desiderata are based on\nChatGPT\u2019s training data and these data may not include all noteworthy suggestions. Before\nrelying on this information, it would be helpful to know the (academic) texts on which this\nassessment has been made. Were the views of the NGOs or public consultations also included?\nWithout the necessary information and context, ChatGPT\u2019s answer only serves as its current\nassessment of a fragile scholarly consensus (should such a thing exist) of what it would mean for\nan AI system to be trustworthy, a snapshot of a discussion in progress.", "Source": "Of ChatGPT and Trustworthy AI", "File": "Of ChatGPT and Trustworthy AI.pdf"}
{"Date": 2018, "Claim": "different understandings for values, discuss about the\nA technology cannot be detached from its cultural context.\nimportance of the topic, its relation to other important topics,\nComputational artifacts are produced through intentional and\nand explore existing works and approaches. Additional to our\nrational processes influenced by the cultural background of\ndiscussions, we bring the notion of \u201cideal rules\u201d as a possible\nISSN: 2236-3297\nSBC Journal on Interactive Systems, volume 9, number 1, 2018 15\nbridge between cultural values and design decisions, suggesting", "Source": "An Essay on Human Values in HCI", "File": "An Essay on Human Values in HCI.pdf"}
{"Date": 2024, "Claim": "considered, and the assessment of the impact of AI tools\non diagnostic accuracy and time efficiency in live clini- This study presents the promising application of a deep\ncal environments. It is imperative to conduct longitudi- learning model, particularly ResNet50 augmented with\nnal studies and clinical trials to evaluate the efficacy and Grad-CAM, for brain tumor detection in MRI images.\nsafety of AI-assisted diagnostics over extended periods. Achieving a testing accuracy of 98.52% alongside high\nThis will not only validate the long-term reliability of AI precision and recall metrics underscores the model\u2019s\ntools but also identify any unforeseen issues that may efficacy in identifying brain tumors accurately. Leverag-\narise in a real-world setting. In concert with technologi- ing data augmentation techniques significantly bolstered\ncal advancements, there is a need for developing clear the model\u2019s robustness and generalization capabilities\nregulatory and ethical guidelines that govern the use of across diverse imaging scenarios. Moreover, the integra-\nAI in medical diagnostics. Future research should focus tion of Grad-CAM provided valuable insights into the\non contributing to policy discussions and the creation of model\u2019s decision-making process by highlighting relevant\ncomprehensive guidelines that ensure patient safety, data areas within the images that influenced its predictions,\nprivacy, and equitable care.", "Source": "Enhancing brain tumor detection in MRI images through explainable AI using Grad-CAM with Resnet 50", "File": "Enhancing brain tumor detection in MRI images through explainable AI using Grad-CAM with Resnet 50.pdf"}
{"Date": 2022, "Claim": "We ground the analysis of large-scale risks from misaligned AGI in the deep learning literature. We argue that if\nAGI-level policies are trained using a currently-popular set of techniques, those policies may learn to reward hack in\nsituationally-aware ways, develop misaligned internally-represented goals (in part caused by reward hacking), then\ncarry out undesirable power-seeking strategies in pursuit of them. These properties could make misalignment in\nAGIs difficult to recognize and address. While we ground our arguments in the empirical deep learning literature,\nsome caution is deserved since many of our concepts remain abstract and informal. However, we believe this paper\nconstitutes a much-needed starting point that we hope will spur further analysis. Future work should formalize and\nempirically test the above hypotheses and extend the analysis to other possible training settings (such as lifelong\nlearning), possible solution approaches (such as those in Section 5), or combinations of deep learning with other\nparadigms. Reasoning about these topics is difficult, but the stakes are high and we cannot justify disregarding or\npostponing the work.\n12", "Source": "The alignment problem from a deep learning perspective", "File": "The alignment problem from a deep learning perspective.pdf"}
{"Date": 2023, "Claim": "Giving a plausible account of trustworthy AI is no easy task; it is no surprise that,\nat least in 2023, the themes of trustworthy and responsible AI are among the most\nwidely funded11 S&K\u2019s account offers a welcome intervention in this debate because\nit clarifies the kind of anthropocentric barrier to getting a plausible account up and\nrunning from the very beginning, and it offers an example of how such an account\nthat avoids this problem might go. My quibbles with the scope of the account in\nSect. 3 remain, but they should be understood as just that: quibbles that invite fur-\nther development of an account that is, on the whole, a promising one.12\nFunding For supporting this research I am grateful to the AHRC Digital Knowledge (AH/W008424/1)\nproject as well as to the European Research Council (ERC) under the European Union\u2019s Horizon 2020\nresearch and innovation programme (grant agreement No 948356, KnowledgeLab: Knowledge-First\nSocial Epistemology).\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as\nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made.", "Source": "Simion and Kelp on trustworthy AI", "File": "Simion and Kelp on trustworthy AI.pdf"}
{"Date": 2021, "Claim": "take a Faster RCNN based SW-DA  as the baseline,\nand conduct UDA experiments from Pascal VOC  to In this paper, we pinpoint the optimization inconsistency\nWatercolor2k . All reported results are obtained from problem between the domain alignment task and the clas-\nthe average of multiple runs. Please refer to Supplemen- sification task itself in alignment-based UDAs. To mitigate\ntary for details about datasets, experimental settings, and it, we propose a meta-optimization based strategy named\nthe introduction of competitors. As shown in , our MetaAlign, whichtreatsoneofthesetwotasksasmeta-train\nMetaAlign strategy improves the mAP of the two baselines and the other as meta-test. The analysis of the optimization\nW-DA (SW-DA without local alignment) and SW-DA by 2.3 objective of MetaAlign reveals that the two tasks will be\n(4.6%) and 1.9 (3.5%), respectively. The latter one achieves optimized in a coordinated way. The experimental results\nstate-of-the-art performance, compared with recent meth- validate that MetaAlign is applicable to various alignment-\nods. Notethattheresultson\u2018bird\u2019areunstableduetothein- based UDAs for classification and detection.\nsufficientdata(thenumberofbirdboundingboxesareabout\n2.6%ofallboundingboxesinthedataset). Somequalitative 6.", "Source": "MetaAlign: Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation", "File": "MetaAlign Coordinating Domain Alignment and Classification for Unsupervised Domain Adaptation.pdf"}
{"Date": 2024, "Claim": "Suppose now that the fair ML framework states that\nEmily\u2019s college admissions model should satisfy both\nEqualized Odds and Predictive Parity (or some other set This article has explored challenges related to action-\nof practically conflicting criteria). Are the Coherence and guidance in AI ethics. I began by outlining a philosophical\nAbility Requirements violated in this case? Even if the cri- account of action-guidance which includes a set of condi-\nteria are indeed theoretically compatible, our intuition might tions for action-guidance and situates the construction of\nlean toward an affirmative answer. After all, the framework usable moral principles as one practical desideratum of\n1 030 AI and Ethics (2025) 5:1019\u20131031\nmoral theorizing alongside the theoretical desideratum of defective application of principles (e.g., misapplication of\nexplaining moral properties and moral behavior. The account fairness criteria). In addition, I noted that, due to practical\nstands as an independent philosophical contribution of this constraints, users may be unable to apply even well-specified\nwork which can be refined and further supplemented by frameworks or unable to derive feasible prescriptions from\nfuture studies on the topic. I demonstrated that the proposed such frameworks. In such cases, the applied framework is\naccount can be used to address what I have called the action- not directly action-guiding to the user.", "Source": "Action-guidance and AI ethics: the case of fair machine learning", "File": "Action-guidance and AI ethics the case of fair machine learning.pdf"}
{"Date": 2021, "Claim": "on LVIS  dataset. For evaluation, we use a COCO-style\naverage precision (AP) metric that averages over categories\nIn this paper, we have presented a unified two-stage learn-\nand different box/mask IoU threshold .\ning strategy for the large-scale long-tail visual recognition\ntasks. To tackle the biased label prediction, we develop a\nQuantitative Results and Ablation Study We first com- confidence-aware distribution alignment method to calibrate\npare our method with recent work and report quantitative initial classification predictions. In particular, we design a\nresults in Tab. 8. We find our DisAlign with cosine classifier generalized re-weight scheme to leverage the category prior\nhead achieves 25.6 in AP , and 26.3 in AP when for the alignment process. Extensive experiments show that\nbbox mask\napplied to the Mask R-CNN+FPN with the ImageNet pre- our method outperforms previous works with a large margin\ntrained ResNet-50 backbone. Moreover, our strategy can on a variety of visual recognition tasks(image classification,\nbe further improved to achieve 27.6 in AP and 27.9 in semantic segmentation, and object detection/segmentation).\nbbox\nReferences", "Source": "Distribution Alignment: A Unified Framework for Long-tail Visual Recognition", "File": "Distribution Alignment A Unified Framework for Long-tail Visual Recognition.pdf"}
{"Date": 2024, "Claim": "This position paper concludes by highlighting the enormous potential of large language models while emphasizing the\ncrucial importance of raising awareness and responsible practices in their deployment. Trustworthiness, fairness, and ethi-\ncal considerations need to be given the utmost importance in the development and application of LLMs, as they continue\nto influence our digital world. Through advancing the principles of accountability, transparency, and interdisciplinary\ncollaboration, stakeholders can effectively and strategically navigate the intricacies of AI modeling. We can capitalize on\nthe transformative power of LLMs to improve human knowledge, communication, and societal well-being while mitigat-\ning potential risks and preventing unintended consequences through continued discussion, research, and innovation.\nVol:.(1234567890)\nDiscover Artificial Intelligence (2024) 4:40 | https://doi.org/10.1007/s44163-024-00129-0 Perspective\nAuthor contributions The single author Dr. Iqbal H. Sarker prepared this manuscirpt.\nData availability No datasets were generated or analysed during the current study.\nDeclarations\nCompeting interests The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons Attribution 4.", "Source": "LLM potentiality and awareness: a position paper from the perspective of trustworthy and responsible AI modeling", "File": "LLM potentiality and awareness a position paper from the perspective of trustworthy and responsible AI modeling.pdf"}
{"Date": 2024, "Claim": "This report introduces the eGridGPT concept as the first research effort to virtually support\nsystem operators in the power grid control room of the future. It outlines interactions between\nsystem operators and eGridGPT. It also explains how eGridGPT becomes trustworthy by\ndescribing training process, validating by digital twin, and recommending on a dynamic display\nbased on operator\u2019s prompt. The short example shows that eGridGPT was able to be trained to\nproduce offline planning case results that were similar to the actual model state estimation.\nThe eGridGPT concept addresses the shortages of the current control room tools and\ntechnologies, enhancing the grid\u2019s secure and reliable operation. eGridGPT is a step forward in\nresponding to a request from the system operators who are founding members of the Global\nPower System Transformation Consortium (G-PST) for the control room of the future to\nencompass intuitive visualization, AI/ML tools, and trustworthy decision support capabilities (G-\nPST n.d.). eGridGPT will be an essential tool for next-generation control room solutions.\nThis report is available at no cost from the National Renewable Energy Laboratory at www.nrel.gov/publications.", "Source": "eGridGPT: Trustworthy AI in the Control Room", "File": "eGridGPT Trustworthy AI in the Control Room.pdf"}
{"Date": 2019, "Claim": "Currently, AI ethics is failing in many cases. Ethics lacks a reinforcement mech-\nanism. Deviations from the various codes of ethics have no consequences. And\nin cases where ethics is integrated into institutions, it mainly serves as a market-\ning strategy. Furthermore, empirical experiments show that reading ethics guide-\nlines has no significant influence on the decision-making of software developers.\nIn practice, AI ethics is often considered as extraneous, as surplus or some kind\n1 3\n114 T. Hagendorff\nof \u201cadd-on\u201d to technical concerns, as unbinding framework that is imposed from\ninstitutions \u201coutside\u201d of the technical community. Distributed responsibility in con-\njunction with a lack of knowledge about long-term or broader societal technologi-\ncal consequences causes software developers to lack a feeling of accountability or\na view of the moral significance of their work. Especially economic incentives are\neasily overriding commitment to ethical principles and values. This implies that the\npurposes for which AI systems are developed and applied are not in accordance with\nsocietal values or fundamental rights such as beneficence, non-maleficence, justice,\nand explicability (Taddeo and Floridi 2018; Pekka et al. 2018).\nNevertheless, in several areas ethically motivated efforts are undertaken to\nimprove AI systems.", "Source": "The Ethics of AI Ethics: An Evaluation of Guidelines", "File": "The Ethics of AI Ethics An Evaluation of Guidelines.pdf"}
{"Date": 2022, "Claim": "In this study, we proposed a safe exploration method for RL to guarantee the safety\nduring learning under the existence of disturbance. The proposed method uses\npartially known information of both the controlled object and disturbances. We\ntheoretically proved that the proposed method achieves the satisfaction of explicit\nstate constraints with a pre-specified probability at every timestep even when the\ncontrolled object is exposed to the disturbance following a normal distribution.\nSufficient conditions to construct conservative inputs used in the proposed method\nare also provided for its implementation. We also experimentally showed the\nvalidity and effectiveness of the proposed method through simulation evaluation\nusing an inverted pendulum and a four-bar parallel link robot manipulator. Our\nfuture work includes the application of the proposed method to real environments.\nSafe Exploration Method 15\n800\n600\n400\n200\n0 20 40 60 80 100\nEpisode\ntsoc\nevitalumuC\nProposed 1.00\nOkawa et al. (2020)\n0.98\n0.96\n0.94\n0.92\n0.90\n0 20 40 60 80 100\nStep\nfo\nycneuqerf\nevitaleR\nnoitcafsitas\ntniartsnoc\nProposed\nOkawa et al. (2020)\n2500\n2000\n1500\n1000\n0 20 40 60 80 100\nEpisode\ntsoc\nevitalumuC\nProposed 1.00\nOkawa et al. (2020)\n0.98\n0.96\n0.94\n0.92\n0.90\n0 20 40 60 80 100\nStep\nfo\nycneuqerf\nevitaleR\nnoitcafsitas\ntniartsnoc\nProposed\nOkawa et al. (2020)\n.", "Source": "Safe Exploration Method for Reinforcement Learning under Existence of Disturbance", "File": "Safe Exploration Method for Reinforcement Learning under Existence of Disturbance.pdf"}
{"Date": 2024, "Claim": "AI is often perceived to be both a great threat and promise to democracy. Pessimists\nworry about the way LLMs and other forms of AI can undermine communication and\ntrust. Optimists point to how AI enables technological innovations in voting procedures,\nor allow more voices to be heard in the democratic process, although rarely exploring this\nin light of existing democratic theories. In this Comment, we have suggested that the\npublic debate should also recognize the distinct and additional point that the governance\nof AI should be as democratic as possible. Given the impact that AI technology already has\non societies, it is crucial that there are democratically legitimate channels for the people\naffected by AI to have a say about how it is being developed and deployed. Specifically, we\nhave argued that, while some non-state actors \u2013 most probably those with moral or\nepistemic authority \u2013 may become democratic agents in the future, and as such contribute\nto the democratization of global AI governance, most of them \u2013 in particular those with\nmarket authority \u2013 are more likely to increase their democratic credentials as agents of\ndemocracy, improving the empirical prerequisites for future democratization.", "Source": "The democratization of global AI governance and the role of tech companies", "File": "The democratization of global AI governance and the role of tech companies.pdf"}
{"Date": 2023, "Claim": "If we are to make good progress on the normative side of AI alignment, we must consider all levels:\nIndividual, Organizational, National, and Global, and understand how each works together, rather\nthan only aligning one or a few of the parts. Here we have presented a framework for considering\nthese issues. In Appendix A, we have provided an analysis of how this work relates to AI x-risk.\nThe versatility of the framework means that it can be applied to many other topics, including but\nnot limited to autonomous vehicles, AI-assisted clinical decision support systems, surveillance, and\ncriminal justice tools. In these hotly contested spaces with no clear answers, by analysing these\nproblems at four levels, we are able to see the many interacting parts at play, in order to create more\nethical and aligned AI.", "Source": "A Multi-Level Framework for the AI Alignment Problem", "File": "A Multi-Level Framework for the AI Alignment Problem.pdf"}
{"Date": 2019, "Claim": "Min-max robust optimization based adversarial training\ncan provide a notion of security against adversarial attacks.\nHowever, adversarialrobustnessrequiresasignificantlarger\ncapacity of the network than that for the natural training\nfilter/natural test filter/adversarial test with only benign examples. This paper proposes a frame-\ncolumn/natural test column/adversarial test\nwork of concurrent adversarial training and weight pruning\nirregular/natural test irregular/adversarial test\nthat enables model compression while still preserving the\nadversarial robustness and essentially tackles the dilemma\n(b)ResNet-18 of adversarial training. Furthermore, this work studies two\n: Natural and adversarial test accuracy of the proposed hypotheses about weight pruning in the conventional set-\nframework of concurrent adversarial training and weight pruning ting and finds that weight pruning is essential for reducing\non CIFAR10. Filter, column, and irregular pruning schemes are the network model size in the adversarial setting, and that\napplied in the proposed framework respectively. Weight pruning training a small model from scratch even with inherited ini-\nis performed from size of w = 16 to sizes of w = 1,2,4,8.\ntialization from the large model cannot achieve adversar-\nThe solid lines denote natural accuracy when pruning from the\nial robustness and high standard accuracy at the same time.", "Source": "Adversarial Robustness vs. Model Compression, or Both?", "File": "Adversarial Robustness vs. Model Compression or Both.pdf"}
{"Date": 2023, "Claim": "This paper presented a preliminary evaluation of the robustness of ChatGPT from the adversarial\nand out-of-distribution perspective. While we acknowledge the advance of large foundation models\non adversarial and out-of-distribution robustness, our experiments show that there is still room for\nimprovement to ChatGPT and other large models on these tasks. Afterwards, we presented in-depth\nanalysis and discussion beyond NLP area, and then highlight some potential research directions\nregarding foundation models. We hope our evaluation, analysis, and discussions could provide\nexperience to future research.\nAcknowledgement\nThis paper received attentions from many experts since its first version was released on ArXiv.\nAuthors would like to thank all who gave constructive feedback to this work.\nDisclaimer\nPotential Ethics and Societal Concerns raised by ChatGPT Robustness The increasing popular-\nity of ChatGPT and other chatbot services certainly face some new concerns from both ethics and\nsociety. The purpose of this paper is to show that ChatGPT can be attacked by adversarial and OOD\nexamples using existing public dataset, but not to attack it intentionally. We hope that this will not be\nleverage by end-users. Finally, we also hope the community can realize the importance of robustness\nresearch and develop new technologies to make our systems more secure, robust, and responsible.", "Source": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective", "File": "On the Robustness of ChatGPT An Adversarial and Out-of-distribution Perspective.pdf"}
{"Date": 2020, "Claim": "Smart City research has made strides to making it easier to identify smarter ways of\naddressing real-world problems, especially with regards to the pressures of urbanisation\nand resource depletion. However, consensus on the definition, required components\nand value for Smart Cities has yet to be reached. The theoretical perspectives explored\nin this study can assist researchers to move closer to such consensus. The SLR con-\nducted focused on seeking empirical studies of Smart City projects and reviewed\ntheories that can help to address the gaps in knowledge relating to value in Smart\nCities.\nThe study contributes to showing real-world examples in Smart Cities, and pro-\nvides a comprehensive lists of factors that can be classified for each dimension. A gap\nin previous research was evident relating to value alignment for Smart Cities, and this\nstudy fills this gap. The contribution is the proposed VASC model, which can be used\nto address how value can be aligned within a Smart City for all stakeholders and\ndimensions. The study was limited to secondary data and future research should\ntherefore investigate the adoption of the model in Smart City initiatives.\nAcknowledgement. \u201cThis work is based on the research supported wholly/in part by the\nNational Research Foundation of South Africa (Grant Numbers: 116779)\u201d.", "Source": "Using Theories to Design a Value Alignment Model for Smart City Initiatives", "File": "Using Theories to Design a Value Alignment Model for Smart City Initiatives.pdf"}
{"Date": 2023, "Claim": "Given the rapid advancements in the field of AI, it is essential\nThis study highlights the imperative need for medical AI ethics\nthat these ethical guidelines be regularly revisited and updated\neducation and the integration of a comprehensive set of ethical\nto remain relevant in the context of medical education. The\nprinciples into medical education to prepare physicians for the\nproposed dynamic approach, with an emphasis on ethical\nethical challenges posed by AI in medicine. As the advancement\nprinciples, aims to ensure that medical professionals not only\nof AI technologies in medicine is expected to increase, it is\nare equipped to use AI in ways that enhance patient care but\nessential for medical ethics education to adapt and evolve\nalso uphold the highest ethical standards. Future research is\naccordingly to keep pace with these developments. Educational\nneeded to develop problem-based and competency-oriented\ninstitutions should take proactive steps to update their curricula,\nlearning objectives and educational content for medical AI ethics\nensuring that future medical professionals are not only aware\nand implementation and validation.\nConflicts of Interest\nNone declared.", "Source": "Proposing a Principle-Based Approach for Teaching AI Ethics in Medical Education", "File": "Proposing a Principle-Based Approach for Teaching AI Ethics in Medical Education.pdf"}
{"Date": 2022, "Claim": "In this paper, we provided a holistic study of the zero-shot adversarial robustness problem of large-\nscale vision-language models. We identified the effects of various adaption methods and training\nlosses when adapting models, and conjectured that existing methods failed to generalize to new\ntasks due to the lack of the language supervision. We proposed a text-guided contrastive adversarial\ntraining (TeCoA) which can be used with model finetuning and visual prompting to drastically im-\nprove the zero-shot adversarial robustness of CLIP. Extensive experimental evaluation showed the\neffectiveness of TeCoA, and the detailed analyses provide useful lessons for adapting large-scale\nmodels to improve their zero-shot adversarial robustness, shedding light on this important problem.\nPublished as a conference paper at ICLR 2023\n6 ACKNOWLEDGEMENT\nThis research is based on work partially supported by the DARPA SAIL-ON program, the DARPA\nMCS program, the NSF NRI Award #2132519, a GE/DARPA grant, a CAIT grant, and gifts from\nJP Morgan, DiDi, and Accenture.", "Source": "Understanding Zero-Shot Adversarial Robustness for Large-Scale Models", "File": "Understanding Zero-Shot Adversarial Robustness for Large-Scale Models.pdf"}
{"Date": 2023, "Claim": "In this paper, we conducted a comprehensive study on stability notions in the context of graph neural\nflows and made significant findings. While Lyapunov stability is frequently employed, it alone may\nnot suffice in guaranteeing robustness against adversarial attacks. With a grounding in foundational\nphysics principles, we proposed a shift towards conservative Hamiltonian neural flows for crafting\nGNNs resilient against adversarial attacks. Our empirical comparisons across diverse neural flow\nGNNs, as tested on multiple benchmark datasets subjected to a range of adversarial attacks, have\nfurther corroborated this proposition. Notably, GNNs that amalgamate conservative Hamiltonian\nflows with Lyapunov stability exhibited marked enhancement in their robustness metrics. We are\noptimistic that our work will inspire further research into marrying physics principles with machine\nlearning paradigms for enhanced security.\n8 Acknowledgments and Disclosure of Funding\nThis research is supported by the Singapore Ministry of Education Academic Research Fund Tier 2\ngrant MOE-T2EP20220-0002, and the National Research Foundation, Singapore and Infocomm Me-\ndia Development Authority under its Future Communications Research and Development Programme.\nTo improve the readability, parts of this paper have been grammatically revised using ChatGPT .", "Source": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach", "File": "Adversarial Robustness in Graph Neural Networks A Hamiltonian Approach.pdf"}
{"Date": 2022, "Claim": "Our work provides an analysis of reward misidentification and causal reward confusion. We identify\nthree tasks where preference learning often results in misaligned learned reward models. Interest-\ningly, these models have good validation and test accuracy\u2014sometimes even 99.5%\u2014and seem to\ndistinguish the basics of task success versus failure. However, optimizing these rewards via RL\npushes the policy outside of the training distribution, where the model falsely believes the reward\nis higher\u2014we demonstrate this via saliency maps, EPIC distance, and KL divergence and by ex-\namining the resulting behaviors. This out-of-distribution effect results in policies that achieve high\nlearned rewards but have poor true rewards. We find that it is easy for reward models to pick up\non non-causal features, as some issues go away when we eliminate these non-causal features from\nthe input. Furthermore, noisy preference data aggravates poor generalization. And when not all\ncausal features are observable, the learned reward model will struggle even though there exists a\nhigh-performing policy that only uses observable state information.\nBased on our results, we have identified several directions for future work. First, our results show\nthat reward misidentification induces a distribution shift such that the learned reward appears de-\nceptively close to the true reward on the training distribution, despite leading to misaligned be-\nhavior when optimized via RL.", "Source": "A Study of Causal Confusion in Preference-Based Reward Learning", "File": "A Study of Causal Confusion in Preference-Based Reward Learning.pdf"}
{"Date": 2021, "Claim": "58.0% AA on Student\nIn this paper, we investigated the problem of training\nk\nc a AA on Teacher small robust models via knowledge distillation. We revis-\nt t 56.0%\na ited several state-of-the-art adversarial training and robust-\nA\nA ness distillation methods from the perspective of distilla-\nts 54.0% tion. By comparing their loss functions, we identified the\nn\nia importance of robust soft labels (RSLs) for improved ro-\ng\na\ns 52.0%\nbustness. Following this view, we proposed a novel ad-\ns\ne versarial robustness distillation method named Roust Soft\nn\nts Label Adversarial Distillation (RSLAD) to fully exploit the\nu\nb 50.0%\no advantage of RSLs. The advantage of RSLAD over exist-\nR\ning adversarial training and distillation methods were em-\n48.0% pirically verified on two benchmark datasets under both the\nRN-18 RN-34 RN-50 WRN WRN WRN\nwhite-box and the black-box settings. We also provided\n34-10 34-20 70-16\nseveral insightful understandings of our RSLAD, different\n: Robustness against AA attack of ResNet-18 (RN-\ntypes of soft labels, and more importantly, the interplay be-\n18) students trained using our RSLAD with 6 different\ntween the teacher and student networks. Our work can help\nteachers. RN: ResNet; WRN: WideResNet. The RN-\nbuildadversariallyrobustlightweightdeeplearningmodels.\n18, RN-34, RN-50, WRN-34-10 teachers are trained using\nTRADES, while the rest teacher models are from Gowal et\nal. . This experiment is done on CIFAR-10 dataset.", "Source": "Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better", "File": "Revisiting Adversarial Robustness Distillation Robust Soft Labels Make Student Better.pdf"}
{"Date": 2023, "Claim": "https://openai.com/safety-standards\nWhile the proposed model refers to developers and", "Source": "Trustworthy AI: A Fuzzy-Multiple Method for Evaluating Ethical Principles in AI Regulations", "File": "Trustworthy AI A Fuzzy-Multiple Method for Evaluating Ethical Principles in AI Regulations.pdf"}
{"Date": 2023, "Claim": "particularly regarding traffic light management. The results\nindicate that as the percentage of rules increases, the system This paper presents GHRL, a framework that employs a\nbecomes more efficient at managing traffic lights, leading to VAE to extract visual features from camera images, localiza-\nbetter traffic flow and fewer delays. However, it is essential tion, and waypoints as navigation input. An RL algorithm is\nto note that there may be trade-offs between incorporating used to learn the high-level policy with OC framework and\ntoo many rules and limiting the system\u2019s ability to learn and low-level policies guided by expert demonstration encoded in\ngeneralize, which can lead to overfitting. ASP rules. We also incorporated safety rules into the decision-\nmaking process in potentially dangerous situations to ensure\nE. Safe High-Policies Experiments\nthat the agent can make safe and responsible decisions, even in\nWe have evaluated urban driving safety using different RL complex and challenging situations. We have evaluated GHRL\nalgorithms with various potential hazards and unpredictable on the Carla NoCrash benchmark and conducted an ablation\nNHTSA (National Highway Traffic Safety Administration) study to analyze the effect of various network architectures\npre-crash scenarios. To evaluate the performance, we used a and RL hyperparameters on the proposed framework\u2019s per-\nsimulated environment that closely mimics the challenges of formance.", "Source": "Guided Hierarchical Reinforcement Learning for Safe Urban Driving", "File": "Guided Hierarchical Reinforcement Learning for Safe Urban Driving.pdf"}
{"Date": 2023, "Claim": "Quantum machine learning is generally anticipated to\nbe one of the more widely employed use cases for quantum\ncomputers in the current NISQ period. As indicated by\nrecently announced various quantum hardware roadmaps,\nincreasingly capable quantum devices (larger qubit numbers\nand lower error rates) are expected to be deployed in the\nnext few years, which should allow rigorous benchmarking\nof quantum machine learning models, likely leading to\ntheir applications for a variety of classification and feature\ndetection problems of practical interest. Beyond that, the\nsuccessful development of large-scale fault tolerant quantum\ncomputers may one day lead to quantum machine learning\nsystems being entrusted with sensitive tasks which must be\ncarried out with extremely high reliability. As such, and\ngiven both the existence of adversarial examples which can\nfool highly sophisticated classical neural networks and the\nearly evidence of similar examples in the quantum setting,\nunderstanding the potential vulnerabilities of quantum\nclassifiers, and how they compare to those of their classical\ncounterparts, is a pressing matter which deserves further\nresearch. In this survey paper we have described the new\nand rapidly growing field of quantum adversarial machine\nlearning, summarising the key achievements to date and\noutlining some of the main challenges that yet remain.", "Source": "Towards quantum enhanced adversarial robustness in machine learning", "File": "Towards quantum enhanced adversarial robustness in machine learning.pdf"}
{"Date": 2022, "Claim": "classes. As we can see, our proposal outperforms C-AVP-v0\nsince the former\u2019s higher main diagonal entries indicate less In this work, we develop a novel VP method, i.e., C-AVP, to\nprompt selection error than the latter. improve adversarial robustness of a fixed model at test time.\nComparisons with other test-time defenses. In Tab.3, we Compared to existing VP methods, this is the first work to peer\ncompare our proposed C-AVP with three test-time defense into how VP could be in adversarial defense. We show the\nmethods selected from Croce et. al. . Note that all meth- direct integration of VP into robust learning is not an effective\nods are applied to robustifying a fixed, standardly pre-trained adversarial defense at test time for a fixed model. To address\nResNet18. Following Croce et. al. , we divide the consid- this problem, we propose C-AVP to create ensemble visual\nered defenses into different categories, relying on their defense prompts and jointly optimize their interrelations for robustness\nprinciples (i.e., IP or MA) and needed test-time operations (i.e., enhancement. We empirically show that our proposal signifi-\nIA, AN, and R). As we can see, our method C-AVP falls into cantly reduces the inference overhead compared to classical\nthe IP category but requires no involved test-time operations. adversarial defenses which typically call for computationally-\nThis leads to the least inference overhead. Although there intensive test-time defense operations.\n7. REFERENCES", "Source": "Visual Prompting for Adversarial Robustness", "File": "Visual Prompting for Adversarial Robustness.pdf"}
{"Date": 2024, "Claim": "4 Moving forward\nIn this article, we have addressed the mechanisms of AI\nhype and accounted for the planetary and social conse-\nHaving explored the mechanisms of AI hype, as well as its quences of that hype. The mechanisms include anthropo-\nconsequences, we now offer some practical points for man- morphism, exaggerated AI literacy or so called \u201cAI-experts,\u201d\naging the phenomenon going forward. These will serve as the geopolitical narratives and FOMO the overuse and\n7 38 AI and Ethics (2024) 4:727\u2013742\nmisappropriation of the term \u201cAI\u201d. While we recognise that 6. Amar, Z., Ramsay, N.: Charity digital skills report 2023. https://\nthere are other mechanisms also contributing to the hype, charit ydigi talsk ills.c o.u k/t he-c harit y-d igita l-s kills-r eport-i ntro\nductio n/ (2023)\nwe conclude that these mechanisms are significant in shap-\n7. AI activity in UK businesses: executive summary. https://w ww.\ning the global socio-technical narratives and imaginaries gov.u k/g overn ment/p ublic ation s/a i-a ctivi ty-i n-u k-b usine sses/a i-\nwoven around emerging AI technologies in our contempo- activi ty-i n-u k-b usine sses-e xecut ive-s ummar y (2022)\nrary society. The consequences of this hype, that are often 8. Pause Giant AI Experiments: An open letter. https://f uture oflif e.\norg/o pen-l etter/p ause-g iant-a i-e xperi ments/ (2023)\noverlooked, include planetary costs, such as the material\n9. Placani, A.: Anthropomorphism in ai: hype and fallacy.", "Source": "The mechanisms of AI hype and its planetary and social costs", "File": "The mechanisms of AI hype and its planetary and social costs.pdf"}
{"Date": 2023, "Claim": "ChatGPT and related technologies have the potential to significantly impact academia and\nscholarly research and publishing. However, it is important to carefully consider the ethical\nimplications of these technologies, particularly in regard to the use of GPT-3 by academics and\nresearchers. While ChatGPT and GPT-3 represent major advancements in artificial intelligence,\nmachine learning, and natural language processing, it is necessary to ensure that they are used\nethically and responsibly for scholarly research and publishing. Many questions about the ethics\nof using GPT in academia and its impact on research productivity remain unanswered. This\npaper aimed to provide a comprehensive overview of the current state of these discussions and to\nencourage further exploration of the ethical considerations surrounding the use of GPT and\nsimilar technologies in academia.", "Source": "ChatGPT and a new academic reality: Artificial Intelligence\u2010written research papers and the ethics of the large language models in scholarly publishing", "File": "ChatGPT and a new academic reality Artificial Intelligencewritten research papers and the ethics of the large language models in scholarly publishing.pdf"}
{"Date": 2023, "Claim": "The AI standards landscape is still evolving and in flux, playing catch-up with the speed at\nwhich AI and ML technology is progressing. Several key standards, such as ISO/IEC DIS\n42001 Artificial Intelligence Management System , are already in draft form or just\npublished, and there is a need for third-party certification to support the responsible use of AI\nin organisations with respect to their AI systems. Certification of AI systems is a key\nassurance service in building trust in AI and ML as laid out in the CDEI AI assurance\nroadmap . Thus, concrete certification schemes need to be developed to ensure that AI\nsystems are trustworthy.\nWe argued that certification should assess conformity of an AI system along three\ndimensions: (i) against standards such as 42001, (ii) against criteria pertaining to the\ntrustworthiness of the system, and/or (iii) conformity testing and evaluation of the AI\ncomponents of the system.\nTo substantiate our reasoning we have reviewed trustworthy AI and ML in its capacity to\nprovide the key principles for responsible development and deployment of AI systems, and\nwe summarised the landscape of AI standards and certification. As a case study we have\ndiscussed ChatGPT, a large language model that is generating a lot of attention and is\nPage 11 of 15\nNPL Report MS 45\nposing some difficult questions with respect to potential assessment conformity to the three\npillars of trustworthy AI.", "Source": "Certification of machine learning applications in the context of trustworthy AI with reference to the standardisation of AI systems", "File": "Certification of machine learning applications in the context of trustworthy AI with reference to the standardisation of AI systems.pdf"}
{"Date": 2018, "Claim": "In this paper, we intended to disentangle the relationship\nbetween adversarial robustness and generalization by ini-\ntially adopting the hypothesis that robustness and general-\nization are contradictory . By considering adver-\nsarial examples in the context of the low-dimensional, un-\nderlying data manifold, we formulated and experimentally\nconfirmed four assumptions. First, we showed that regular\nadversarial examples indeed leave the manifold, as widely\nassumed in related work . Second, we\ndemonstrated that adversarial examples can also be found\non the manifold, so-called on-manifold adversarial exam-\nples; even if the manifold has to be approximated, e.g., us-\ning VAE-GANs . Third, we established that robust-\nness against on-manifold adversarial examples is clearly re-\nlated to generalization. Our proposed on-manifold adver-\nsarial training exploits this relationship to boost generaliza-\ntion using an approximate manifold, or known invariances.\nFourth, we provided evidence that robustness against regu-\nlar, unconstrained adversarial examples and generalization\nare not necessarily contradicting goals: for any arbitrary but\nfixed model, better generalization, e.g., through more train-\ning data, does not reduce robustness.\nReferences black-box attacks to deep neural networks without training\nsubstitute models. In AISec, 2017. 2", "Source": "Disentangling Adversarial Robustness and Generalization", "File": "Disentangling Adversarial Robustness and Generalization.pdf"}
{"Date": 2022, "Claim": "To the best of our knowledge, the dependence between the\noutput of the target model and input adversarial samples\nhave not been well studied. In this paper, we investigate\nthe dependence from the perspective of information theory.\nConsidering that adversarial samples contain natural and ad-\nversarial patterns, we propose to disentangle the standard MI\ninto the natural MI and the adversarial MI to explicitly mea-\nsure the dependence of the output on the different patterns.\nWe design a neural network-based method to train two MI\nestimation networks to estimate the natural MI and the ad-\nversarial MI. Based on the above MI estimation, we develop\nan adversarial defense algorithm called natural-adversarial\nmutual information-based defense (NAMID) to enhance the\nadversarial robustness. The empirical results demonstrate\nthat our defense method can provide effective protection\nImproving Adversarial Robustness via Mutual Information Estimation\nagainst multiple adversarial attacks. Our work provides a on Computer Vision and Pattern Recognition, pp. 4312\u2013\nnew adversarial defense strategy for the community of ad- 4321, 2019.\nversarial learning. In future, we will design more efficient\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining\nmechanisms for training MI estimators and further optimize\nand harnessing adversarial examples. In International\nthe natural-adversarial MI-based defense to improve the\nConference on Learning Representations, 2015.", "Source": "Improving Adversarial Robustness via Mutual Information Estimation", "File": "Improving Adversarial Robustness via Mutual Information Estimation.pdf"}
{"Date": 2019, "Claim": "In this paper, we introduced a formal and general framework combining Machine Ethics and Machine Ex-\nplainability. We did so in two (major) steps: first, we motivated and introduced a framework of Machine\nEthics. Second, we constructed an instantiation of this framework enabling Machine Explainability.\nIn our discussion, a couple of details were left for future work. While we characterized the form and\ncontent of the arguments, we omitted a formal characterization of their contents. Obviously, for most\nof the practically interesting cases, they consist of first order, modal, temporal or deontic logic formulas.\nThis being the case, the graph needs to be supplemented by expressive means to draw conclusions: it\nneeds a logical system, a calculus with inferences rules. Another aspect still to be explored is the space\nand time complexity of our approach. Additionally, we postponed a couple of optimization questions.\nFor instance, there might be significantly fewer world states needing consideration if some variables\nthat constitute \u03c9 are dependent. Also we ignored that some variables might have very large or even\nuncountably infinite domains, such that considering all possible cases would be practically infeasible or\neven impossible. Here heuristics are needed to restrict the number of options to the most probable or\nimportant ones.", "Source": "Towards a Framework Combining Machine Ethics and Machine Explainability", "File": "Towards a Framework Combining Machine Ethics and Machine Explainability.pdf"}
{"Date": 2019, "Claim": "and 61728210, the Strategic Priority Research Program of\nIn this paper, we propose a novel Alignment Gener- Chinese Academy of Science under Grant XDBS01000000\native Adversarial Network by exploiting pixel alignment and XDB32050200, the Beijing Natural Science Founda-\n4328\ntion under Grant L172050 and 4172062, and Youth Innova- adaptation in person re-identification. In ICME 2019. IEEE,\ntion Promotion Association CAS2018166. 2019.", "Source": "RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment", "File": "RGB-Infrared Cross-Modality Person Re-Identification via Joint Pixel and Feature Alignment.pdf"}
{"Date": 2024, "Claim": "The integration of AI and ML into crop management has ushered in a new era of\nprecision agriculture, offering significant potential to enhance productivity,\nsustainability, and resilience in the face of growing challenges such as climate\nchange and food security. Through advancements in predictive analytics, AI and ML\nhave enabled more accurate forecasting of crop yields, pest outbreaks, and weather\npatterns, thereby allowing farmers to make informed decisions and optimize\nresource use. The synergy among AI, ML, and emerging technologies such as the IoT\nand autonomous farming equipment promises to further revolutionize agriculture.\nThese technologies facilitate real-time monitoring and decision-making, automate\nlabor-intensive tasks, and personalize farming solutions to the specific needs of\nindividual farms. Moreover, the application of AI and ML in crop breeding and\ngenomics is accelerating the development of resilient crop varieties, contributing to\nthe long-term sustainability of agricultural practices.\nHowever, the widespread adoption of AI and ML in crop management is not\nwithout its challenges. Issues such as data quality and availability, technical\ncomplexity, high costs, and infrastructure limitations pose significant barriers,\nparticularly for small-scale farmers in developing regions.", "Source": "Transforming Crop Management Through Advanced AI and Machine Learning: Insights into Innovative Strategies for Sustainable Agriculture", "File": "Transforming Crop Management Through Advanced AI and Machine Learning Insights into Innovative Strategies for Sustainable Agriculture.pdf"}
{"Date": 2023, "Claim": "\u201cResistance is not a force to fear: it is a powerful signal...Harnessed well, public resistance can help shine a\nlight on what must be improved, weed out AI \u2018snake oil\u2019, define what is socially acceptable, and help a more\nresponsible AI industry flourish\u201d (Aidan Peppin, in ).\nThere is a risk of Trustworthy AI being used for the purposes of \u2018trust washing\u2019 unless the efforts pursued under\nits heading meaningfully relate to the concerns underlying expressions of distrust\u2014especially if the evidence of\ntrustworthiness offered is compliance with guidelines and regulations which are themselves only weakly reflecting (as\nopposedtoactivelydismantling)structuralviolence.Infocusingonconativetrustwearenotmeaningtosuggestthatitis\nproductive to consider conation in isolation from cognition and/or affect. These are interlinked, as Baier herself strongly\nargued  and trust motivation theory reiterates . Rather, our point is that ignoring the relational-motivational\naspects of trust\u2014following the same pattern of oversimplification of the human experience which underlies much of\nthe harm inflicted by machines \u2014is a convenient way to performatively vet AI\u2019s trustworthiness while\nsidestepping the matter of whether people actually trust AI, or indeed whether they should.\nThe public is frequently polled by researchers and marketers regarding their level of trust in AI, but the rhetorical\nintention of their responses is often unclear.", "Source": "Trustworthy AI and the Logics of Intersectional Resistance", "File": "Trustworthy AI and the Logics of Intersectional Resistance.pdf"}
{"Date": 2021, "Claim": "also triggers for military operations, violence, murder, and\nmigration that surround the already brutal and slavery-like\nindustry of mining . Modern AI ethics is a field in the making. It has undergone\nAI systems are not just demanding in terms of mate- different phases, from its early beginning that was mainly\nrial resources, they also require a lot of energy. Electronic characterized by the composition of various lists and frame-\nmachines, in contrast to combustion engines, can in principle works of ethical principles to its current state that can be\nbe used sustainably by consuming electricity from renew- described as a practical turn, whereby principles are to be\nable energy sources. In practice, however, in many countries, translated into practice . However, this rather fast\nonly small proportions of electricity are renewable . and self-critical methodological advancement of the field\nAccordingly, powering the computational resources that is contrasted by a relative standstill in terms of the topics\nare required to collect large amounts of training data and to that are discussed. AI ethics is tantamount to a certain set of\ntrain, test, and apply large AI models comes with a signifi- reoccurring issues that are mainly evolving around explain-\ncant carbon footprint . Strubell et al.  conducted ability, fairness, privacy, accountability, safety, and a few\n1 3\n8 62 AI and Ethics (2022) 2:851\u2013867\nmore. The selection of topics, so it seems, is dictated by a always imprudent.", "Source": "Blind spots in AI ethics", "File": "Blind spots in AI ethics.pdf"}
{"Date": 2023, "Claim": "We have shown that common threat models on CSBMs include many graphs with changed semantic\ncontent. As a result, (full) conventional robustness leads to sub-optimal generalization and robust-\nness beyond the point of semantic change. But we also found that the same threat models include\ntruly adversarial examples. This dichotomy is caused by the low-degrees of nodes and the brittle-\nness of their class-membership to a few edges. Thus, it needs both notions, adversarial and over-\nrobustness for a complete picture of robustness in graph learning. As real-world graphs also contain\nmainly low-degree nodes, this calls for more caution when applying (cid:96) -norm restricted threat mod-\nels. These thread models should not be an end to, but the start of an investigation into realistic pertur-\nbation models and works thinking about unnoticeability are positive directions into this endeavour.\nOn CSBMs a significant part of conventional robustness of GNNs is in fact over-robustness (with\nsimilar patterns on real-world graphs). This raises the question what kind of robustness do defenses\nimprove on in GNNs?\nApplying label propagation on top of GNN predictions has shown to be a simple way to reduce\nover-robustness while not harming generalization or adversarial robustness. Therefore, LP can be\nseen as a defense against an attack, where the adversary overtakes a clean node (e.g., social media\nuser) and, with its malicious activity, tries to stay undetected.", "Source": "Revisiting Robustness in Graph Machine Learning", "File": "Revisiting Robustness in Graph Machine Learning.pdf"}
{"Date": 2022, "Claim": "Classification of fake news by fine-tuning deep bidirectional trans-\nformers based language model. EAI Endorsed Trans. Scalable Inf.\nFears, concerns, and rationalisations about GPT-3 and its Syst. 7(27), 1\u201312 (2020). https://d oi.o rg/1 0.4 108/e ai.1 3-7-2 018.\n163973\nintentional misuse for manipulative purposes or uninten-\n3. Barbour, I.: Ethics in an Age of Technology: The Gifford Lectures,\ntional harm caused by bias have been underpinned widely 1989\u20131991, vol. 2. Harper San Francisco, San Francisco (1993)\nby technologically deterministic perspectives within the 4. Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S.:\nNLP industry and AI ethics. Upon inspection, the solutions On the dangers of stochastic parrots: can language models be too\nbig? In: Paper Presented at the Conference on Fairness, Account-\noffered by both technological utopianism and dystopianism\nability, and Transparency (FAccT \u201921), March 3\u201310, 2021, Virtual\nreveal an undue focus on GPT-3\u2019s autonomy rather than the Event, Canada. Association for Computing Machinery, New York,\nautonomy of human actors in AI systems. Examinations of NY, USA, pp. 610\u2013623. https://d oi.o rg/1 0.1 145/3 44218 8.3 4459\nGPT-3 have therefore either been skewed towards specula- 22 (2021)\n5. Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dha-\ntion or are undeservedly trusting in the self-regulatory prac-\nriwal, p. , Neelakantan, A., Shyam, p. , Sastry, G., Askell, A.,\ntices of commercial industry.", "Source": "GPT-3 and InstructGPT: technological dystopianism, utopianism, and \u201cContextual\u201d perspectives in AI ethics and industry", "File": "GPT-3 and InstructGPT technological dystopianism utopianism and Contextual perspectives in AI ethics and industry.pdf"}
{"Date": 2022, "Claim": "Interpretability and explainability approaches are designed to help stakeholders adequately understand the predictions\nand reasoning of an ML-based system. Although these approaches represent complex models in simpler formats,\nthey do not account for the contextual factors that affect whether and how people internalize information. We have\npresented an alternate framework for helping people understand ML models grounded in Weick\u2019s sensemaking theory\nfrom organizational studies. Via its seven properties, sensemaking describes the individual, environmental, social, and\norganizational context that affects human understanding. We translated these for the human-machine context and\npresented a research agenda based on each property. We also proposed a new framework\u2014Sensible AI\u2014that accounts\nfor these nuances of human cognition and presented initial design ideas as a concrete path forward. We hope that\nby accounting for these nuances, Sensible AI can support the desiderata (e.g., reliability, robustness, trustworthiness,\naccountability, fair and ethical decision-making, etc.) that interpretability and explainability are intended for.\nFAccT\u201922,June21\u201324,2022,Seoul,RepublicofKorea Kauretal.", "Source": "Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "File": "Sensible AI Re-imagining Interpretability and Explainability using Sensemaking Theory.pdf"}
{"Date": 2024, "Claim": "ment methods need to evolve to reflect the integration of\nAI education. This might involve project-based assessments\nthat allow students to demonstrate their understanding of AI The goal of introducing AI ethics in schools should be\nin practical and creative ways, rather than relying solely on seen not as an enforcement of rigid rules, but as a continu-\ntraditional testing methods. By implementing these changes, ous, dynamic educational process that enriches students\u2019\nprimary education can better prepare students for a future understanding and responsible use of AI technology. While\nwhere AI is a fundamental aspect of both higher education establishing ethical guidelines is necessary, the greater\nand the professional landscape. emphasis should be on nurturing an ongoing learning\nThough our focus is on education at the primary level, the journey that adapts to the evolving landscape of AI. This\npoint generalizes not only to older schoolchildren but also approach involves integrating AI ethics as a core compo-\nto the adult population. While education on AI should, if nent of continuous technology education, where students\never, only be mandatory for adults in special cases, it should are encouraged to engage with, question, and critically ana-\nnevertheless be offered for many of the same reasons. Just as lyse AI systems.", "Source": "AI ethics should be mandatory for schoolchildren", "File": "AI ethics should be mandatory for schoolchildren.pdf"}
{"Date": 2023, "Claim": "In conclusion, this research delves into the intricate realm analysis, readability metrics, semantic spread measurements,\nof AI-generated text analysis and its differentiation from and vocabulary richness assessments to uncover essential tex-\nhuman-generated text. As Artificial Intelligence continues to tual attributes. By leveraging a composite ensemble of ma-\nrevolutionize various facets of human activities, including text chine learning models, including Logistic Regression, Decision\ncomposition, the challenges of identifying AI-generated con- Tree, Random Forest, Support Vector Classifier, and Gradient\ntent have become increasingly pertinent due to concerns about Boosting, the research demonstrates impressive efficacy with\nmisinformation, security vulnerabilities, and identity theft. The an accuracy of up to 93% in classifying AI-generated and\nresearch methodology is multifaceted, combining linguistic human-generated text.\nwww.ijacsa.thesai.org 1051 | P a g e\n(IJACSA) International Journal of Advanced Computer Science and Applications,\nVol. 14, No. 10, 2023\nMoreover, the integration of xAI techniques, such as LIME", "Source": "Detecting and Unmasking AI-Generated Texts through Explainable Artificial Intelligence using Stylistic Features", "File": "Detecting and Unmasking AI-Generated Texts through Explainable Artificial Intelligence using Stylistic Features.pdf"}
{"Date": 2024, "Claim": "+ AI Workshop at the International Conference of Machine Learning\n2023 and our colleagues for valuable feedback: Maria P. Angel, Joyce\nJia, Kentrell Owens, Alan Rozenshtein, King Xia, and Matthew Rahtz.\nGenerative AI systems present unique and unprecedented\nAny opinions, findings, conclusions, or recommendations expressed in\nchallenges to human values, including the manipulation of this paper are those of the authors and do not reflect those of NIST or\nhuman thoughts and the perpetuation of harmful stereotypes. other institutions.\nIn light of these complexities, traditional approaches within\nDeclarations\nUS legal systems, whether a gradual case accumulation based\non individual rights and responsibilities or domain-specific Conflict of interest The authors have no financial interests directly or\nregulations, may prove inadequate. The US Constitution and indirectly related to this work. However, in the interest of full transpar-\ncivil rights laws do not hold AI companies responsible for ency, we disclose that the first author is employed by the South Korean\nbiases against marginalized groups reinforced or perpetuated government (currently on unpaid leave) and the last author serves on\nthe board of the Electronic Frontier Foundation. Neither organization\nby generative AI systems.", "Source": "Safeguarding human values: rethinking US law for generative AI\u2019s societal impacts", "File": "Safeguarding human values rethinking US law for generative AIs societal impacts.pdf"}
{"Date": 2023, "Claim": "We have examined an episodic two-player zero-sum constrained Markov game (MG) with indepen-\ndent transition functions. In our setup, transition functions are unknown to agents, reward functions\nare adversarial, and utility functions are stochastic. We have proposed the first provably efficient\nalgorithm for playing constrained MGs with O(\u221aT) regret and constraint violation. Our algorithm\nprovides a principled extension of the upper confidence reinforcement learning to deal with coupled\nconstraints in constrained MGs. We also remark that the developed algorithmic framework can be\nreadily applied to learning other constrained MGs, e.g., the ones that involve a single controller.\nOur work opens up many interesting directions for future work, such as sharper algorithms\nwith sample complexity lower bounds, constrained rational algorithms, and how to perform safe\nexploration in other models of constrained MGs.\nPROVABLY EFFICIENT GENERALIZED LAGRANGIAN POLICY OPTIMIZATION FOR SAFE MARL", "Source": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning", "File": "Provably Efficient Generalized Lagrangian Policy Optimization for Safe Multi-Agent Reinforcement Learning.pdf"}
{"Date": 2021, "Claim": "do not engage at the supranational or global level.\nIn light of the fuzzy nature of AI, it is barely surprising\nthat the current landscape is somewhat fragmented. Promis- This article outlined the current state of play in global AI\ning moves towards some degree of centralization and coordi- governance by describing the most important multilateral\nnation are found in the prominent role of the OECD. With its initiatives. It thus contributes to the growing body of litera-\nepistemic authority and its norm- and agenda-setting power, ture aimed at understanding and engaging with the rapidly\nit managed to act as a reference point for the G7 and G20. evolving global AI governance architecture. It organized\nThrough its close collaboration with other multilateral actors individual actors and initiatives in a two-by-two matrix, dis-\nsuch as the European Commission, the UN, and the CoE, tinguishing between the nature of the driving factor(s) and\nand by using the GPAI as a dedicated tool for advancing whether or not their actions take place within the existing\nglobal AI governance, it may continue to play a leading role. governance architecture. Based on this, it provided an over-\nWith all this in mind, this article argues that we are wit- view of key actors and initiatives, highlighting their trajec-\nnessing the first signs of consolidation in this fragmented tories and connections. Lastly, it has been argued that we are\nlandscape.", "Source": "Mapping global AI governance: a nascent regime in a fragmented landscape", "File": "Mapping global AI governance a nascent regime in a fragmented landscape.pdf"}
{"Date": 2023, "Claim": "levels: 1) the ability to understand and agree with\nhuman values; 2) the ability to diagnose scenar-\nAligning big models with humans has gained sig-\nios involving values and make a correct judgment;\nnificant attention to make them better serve human-\n3) the ability to perform consistently with human\nity and minimize their potential risks. This paper\nvalues, even in dilemmas; and more. This assess-\nhighlights the importance of identifying essential\nment becomes more and more difficult, from sim-\ngoals for big model alignment, and presents the\nple discrimination to exact behaviors, which at-\nfirst survey to provide a comprehensive overview\ntempts to detect the most essential values of LLMs\nfrom two perspectives: the definition of each align-\nbehind their elicited behaviors. Since priorities\nment goal and the evaluation of alignment degrees.\namong value principles can only matter in some\nWe categorize alignment goals that appeared in\nquandary scenarios, we should also consider spe-\nexisting literature into three main groups: human\ncific dilemma cases in the evaluation to figure out\ninstructions, human preferences and human values,\nsuch fine-grained information.\nobserving an evolving trend in alignment goals that\nshifts from fundamental abilities to value orienta-\n5.4 Effective & Stable Alignment Algorithms tion, and from surface behaviors to intrinsic values.", "Source": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models", "File": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models.pdf"}
{"Date": 2023, "Claim": "Our results illustrate the potential for large language models (LLMs) to make decisions that diverge from the objectives\nof their principals, in this case, the end-users. Notably, GPT-3.5 demonstrates more nuanced behavior, aligning with\nOf Models and Tin Men- A behavioural economics study of principal-agent problems in AI alignment using\nlarge-language models\ncount mean std min 25% 50% 75% max\nParticipantId Temperature Model Condition\nOpenAI 0.2 gpt-3.5-turbo both 28.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nneither 28.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nprincipal-only 29.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nuser-only 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\ngpt-4 both 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nneither 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nprincipal-only 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nuser-only 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\n0.6 gpt-3.5-turbo both 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nneither 29.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nprincipal-only 29.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nuser-only 29.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\ngpt-4 both 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nneither 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nprincipal-only 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nuser-only 30.00 2.00 0.00 2.00 2.00 2.00 2.00 2.00\nShell 0.2 gpt-3.5-turbo both 28.00 1.64 0.49 1.00 1.00 2.00 2.00 2.00\nneither 18.00 1.00 0.00 1.00 1.00 1.00 1.00 1.00\nprincipal-only 30.00 1.70 0.47 1.00 1.00 2.00 2.00 2.00\nuser-only 29.00 1.", "Source": "Of Models and Tin Men - a behavioural economics study of principal-agent problems in AI alignment using large-language models", "File": "Of Models and Tin Men - a behavioural economics study of principal-agent problems in AI alignment using large-language models.pdf"}
{"Date": 2014, "Claim": "Wehavedescribedthetwodominantapproaches\ntovalues-basedcommunicationofclimatechangeover Despite the relatively short space of time in which\nthepastdecade.Thekeyinsightofthesocialmarketing climate change has occupied a prominent place in\n\u00a92014TheAuthors.WIREsClimateChangepublishedbyJohnWiley&Sons,Ltd.\nFocus Article wires.wiley.com/climatechange\npublic and policy discourse, a substantial literature great deal of research has focused on understanding\nfocusing on the role played by human values in patterns of public perceptions derived primarily\ndetermining public engagement with climate change from values-based differences in political judgments\nhas quickly emerged. The contribution of the current about climate policies. Secondly, nonacademic \u2018gray\u2019\npaper has been to provide the first review of this literature from civil society organizations has played\nliterature, as well as discussing a number of important a central role in the debate about how human\npractical implications for campaigns and initiatives values shape public engagement with climate change.\nseeking to engage the public around climate change.", "Source": "Public engagement with climate change: the role of human values", "File": "Public engagement with climate change the role of human values.pdf"}
{"Date": 2017, "Claim": "In this paper, we have introduced the (cid:12)rst application of machine learning to the critically\nimportant problem of pileup mitigation at hadron colliders. We have phrased the problem\nof pileup mitigation in the language of a machine learning regression problem. The method\nwe introduced, PUMML, takes as input the transverse momentum distribution of charged\nleading-vertex, charged pileup, and all neutral particles, and outputs the corrected leading\nvertex neutral energy distribution. We demonstrated that PUMML works at least as well\nas, and often better than, the competing algorithms PUPPI and SoftKiller in their default\nimplementations. It will be exciting to see these algorithms compared with a full detector\nsimulation, where it will be possible to test the sensitivity to important experimental e(cid:11)ects\nsuch as resolutions and ine(cid:14)ciencies.\nThere are several extensions and additional applications of the PUMML framework\nbeyond the scope of this study. As mentioned in section 2, PUMML can very naturally\nbe extended from jet images to entire events. Applying this event-level PUMML to the\nproblem of missing transverse energy would be a natural next step. While the (cid:12)lter sizes\ncan be the same for the event and jet images, the network training will likely require\nmodi(cid:12)cation. Furthermore, the inhomogeneity of the detector response with j(cid:17)j will require\nattention.", "Source": "Pileup Mitigation with Machine Learning (PUMML)", "File": "Pileup Mitigation with Machine Learning PUMML.pdf"}
{"Date": 2023, "Claim": "dinate responses to model evaluations. If certain dangerous\nmodel capabilities are detected , the company may\nIn this paper, we have identified key design choices that AI want to contact government  and coordinate with other\ncompanies need to make when setting up an ethics board AI companies to pause capabilities research . We wish to\n(RQ1). For each of them, we have listed different options encourage scholars to contribute to the development of best\nand discussed how they would affect the board\u2019s ability to practices in AI ethics, governance, and safety .\nreduce risks from AI (RQ2).  contains a summary of Although it is crucial to remain independent and objective,\nthe design choices and options we have covered. scholars may benefit from direct collaborations with AI\nThroughout this paper, we have made four key claims. companies.\nFirst, ethics boards can take many different shapes. Since We wish to conclude with a word of caution. Setting up\nmost design choices are highly context-specific, it is very an ethics board is not a silver bullet\u2014\u201cthere is no silver\ndifficult to make abstract recommendations\u2014there is no bullet\u201d . Instead, it should be seen as yet another mecha-\none-size-fits-all. Second, ethics boards do not have an origi- nism in a portfolio of mechanisms.\nnal role in the corporate governance of AI companies. They\ndo not serve a function that no other organizational structure\nserves.", "Source": "How to design an AI ethics board", "File": "How to design an AI ethics board.pdf"}
{"Date": 2020, "Claim": "We numerically analyzed non-iterative phase retrieval algo-\nrithms based on machine learning in comparison with con-\nventional iterative algorithms. The machine-learning-based\n Relationship between the estimation errors (NMSEs) and the\nmeasurement SNRs  Comparison of the calculation times\n1 3\n1 40 Optical Review (2020) 27:136\u2013141\nalgorithms used convolutional neural networks called 6. Belashov, A.V., Zhikhoreva, A.A., Belyaeva, T.N., Kornilova,\nResNet with different depths. In the numerical comparisons, E.S., Petrov, N.V., Salova, A.V., Semenova, I.V., Vasyutinskii,\nO.S.: Digital holographic microscopy in label-free analysis of\nthe deeper network (ResNet2) realized better reconstructions\ncultured cells\u2019 response to photodynamic treatment. Opt. Lett.\nthan those realized by the shallower network (ResNet1) and 41, 5035 (2016)\nconventional iterative algorithms, which were ER and HIO. 7. Maleki, M.H., Devaney, A.J.: Phase-retrieval and intensity-only\nFurthermore, an improvement in the noise robustness of reconstruction algorithms for optical diffraction tomography. J.\nOpt. Soc. Am. A 10, 1086\u20131092 (1993)\nthese networks was verified by learning with noisy training\n8. Marathe, S., Kim, S.S., Kim, S.N., Kim, C., Kang, H.C., Nickles,\ndata sets. A large training data set, for example, where the P.V., Noh, D.Y.: Coherent diffraction surface imaging in reflection\nnumber of training pairs was 200,000, reduced the recon- geometry. Opt.", "Source": "Analysis of non-iterative phase retrieval based on machine learning", "File": "Analysis of non-iterative phase retrieval based on machine learning.pdf"}
{"Date": 2023, "Claim": "We have proposed SECOND THOUGHTS, a novel learning paradigm that enables LMs to re-\nalign with human values when given a poisoned context. Compared with existing methods, our\nmethod can generate text aligned with human-values without requiring additional human label-\ning or specifically-designed prompts or instructions. In addition, the chain-of-edits modeling by\nSECOND THOUGHTS enables easy error diagnosis and human-guided correction, which we believe\nto be an essential ability for human-AI interactive systems.\nFor future work, we plan to extend our methods on more human value alignment tasks, and try to\nconsider multi-modality data for alignment. For example, we can capture human\u2019s face expression\nas fine-grained feedback signals for un-aligned sentences, or reversely we can not only rely on text\nedits but speech instructions as the chain-of-edits to model for proper value alignment.\nEthics, Broader Impact, and Reproducibility\nAs large-scale pre-trained LMs become integrated in more systems, it is a matter of utmost societal\nimportanceto make sure that such modelsadhereto shared humanvalues (Bai et al., 2022; Liu et al.,\n2021d, 2022). Here, we present a light-weight framework that can align the generation of LMs with\nsuch values, withoutrequiringnew data or extensiveprompt-engineering.", "Source": "Second Thoughts are Best: Learning to Re-Align With Human Values from Text Edits", "File": "Second Thoughts are Best Learning to Re-Align With Human Values from Text Edits.pdf"}
{"Date": 2024, "Claim": "Healthcare systems worldwide face crises of affordabil-\nity, access and inconsistent quality that now endanger", "Source": "Generative AI in healthcare: an implementation science informed translational path on application, integration and governance", "File": "Generative AI in healthcare an implementation science informed translational path on application integration and governance.pdf"}
{"Date": 2022, "Claim": "Rules do not get developed or implemented in a vacuum but rather arise from particular political, social, and cultural contexts.\nIf we want to understand AI development not simply as an American, Canadian, or European project but rather ultimately as a\nglobal one but with local approaches, then the question of how the concomitant rules are formed to govern it should be seen in\na similar light. The biggest contribution of IHRL is to allow for these kinds of localized approaches to be developed while\nproviding a general framework for some kind of global coordination. For better or worse, the human rights frame as the principal\nlanguage of global justice informs and will continue to inform many of the current and future debates on AI regulation and\ngovernance around the world. In the past four years alone, there has been an explosion of calls for the design and implementation\nof algorithms to respect human rights from different sectors. Civil society organizations, grassroots advocacy groups, and many\nof those critical of the increasing power of tech companies, as well as their corresponding lack of accountability, principally\nwield their arguments in the language of human rights, as do technology companies who profess fealty to human rights in the\ndesign of their products. Thus, it is imperative that technologists, engineers, philosophers, and other stakeholders in the AI\nspace are knowledgeable about its power, possibilities and, more notably, limits.", "Source": "The Promise and Perils of International Human Rights Law for AI Governance", "File": "The Promise and Perils of International Human Rights Law for AI Governance.pdf"}
{"Date": 2022, "Claim": "This article proposes a model agnostic adversarial robustness assessment for adversarial\nmachine learning, especially for neural networks. By investigating the various neural network\narchitectures as well as arguably the contemporary adversarial defences, it was possible to:\n(a). show that robustness to L and L Norm Attacks differ significantly, which is why the\n0 1\nduality should be taken into consideration,\n(b). verify that current methods and defences, in general, are vulnerable even for L and L\n0 1\nblack-box Attacks of low threshold th, and,\nPLOSONE|https://doi.org/10.1371/journal.pone.0265723 April14,2022 18/22\nPLOS ONE AdversarialrobustnessassessmentwhyinevaluationbothL andL attacksarenecessary\n0 1\nFig6. Distributionofadversarialsamples.DistributionofadversarialsamplesfoundonDenseNet(left)andResNet(right)usingth=10withbothfew-pixel(L0 )and\nthreshold(L1 )Attacks.\nhttps://doi.org/10.1371/journal.pone.0265723.g006\n(c). validate the robustness assessment with robustness level as a fair and efficient approxi-\nmation to the full accuracy per threshold curve.\nInterestingly, the evaluation of the proposed L black-box Attack based on CMA-ES\nrequired only circa 12% of the amount of perturbation used by the One-Pixel Attack while\nachieving similar accuracy.", "Source": "Adversarial robustness assessment: Why in evaluation both L0 and L\u221e attacks are necessary", "File": "Adversarial robustness assessment Why in evaluation both L0 and L attacks are necessary.pdf"}
{"Date": 2021, "Claim": "ing which acts as a structural regularizer, preventing ovvieewr-, as shown in .\nfitting during training. In our experiments, we foundInthtaertmsofthe C D o N n N v s o m lu o t d io e n ls a , l C N las e s u M ra o l d N el e V tw is o u r a k li s za ( t C io N n N [3 s 0 ) 5 h ] a is v t e ry l i e n d g t t o og im en - erateaparticular\nIn this work, we proposed a novel class-discriminative\nthe advantages of this global average pooling layer exitmenadge visuaplriezasstiiovne bpyemrfoaxrmimainzicneg othne sacovraerioeftyclaossf pvriosbuaablilirteycwogitnhitrieosnpect to the input\nlocalization technique\u2014Gradient-weighted Class Activation\nbeyond simply acting as a regularizer - In fact, with a ilmittalgee. An etaxsakmsp[l1e0o,f3th5e, 8cl]a.sRs mecoednetlwisosrhkowhansisnhFoiwgunreth9a.t despite being\nMapping (Grad-CAM)\u2014for making any CNN-based mod-\ntweaking, the network can retain its remarkable localization trained on image-level labels, CNNs have the remarkable\nels more transparent by producing visual explanations. Fur-\nability until the final layer. This tweaking allows i\nd\n.\ne\nn\n.3\ntify\nD\nin\niff\ng\nerencesaibniltihtye mtoetlhoocdaolilzoegyo.bTjehcistsca[1te,g1o6r,y2is, 1m5a,in1l8y].", "Source": "Trustworthy AI: A Computational Perspective", "File": "Trustworthy AI A Computational Perspective.pdf"}
{"Date": 2023, "Claim": "Whenever ML models are deployed in environments where the stakes are high, it is\nimportant to implement appropriate guardrails. However, i.i.d generalization by itself\nis insufficient to provide the necessary assurances, since even high performing ML\nmodels are vulnerable to changes in the deployment distribution. We believe that the\nconcept of robustness\u2014having distinct epistemic functions from other concepts in\nML and statistics\u2014can help to provide much needed guardrails.\nThe starting point of this paper was the observation that, so far, the usage of robust-\nness in ML has either been entirely context dependent or simply left vague. The\nkey contribution of our framework is to provide a common language for robustness\nresearch. What is meant by robustness is fully specified by a given robustness target,\n109 Page 22 of 28 Synthese(2023)202: 109\nmodifier(-domain), and target tolerance. Through this lens, we have analyzed a vari-\nety of robustness sub-types, while also discussing possible strategies for improving\nrobustness in ML. Lastly, we hope our work emphasizes the value of (philosophical)\nconceptual analysis in the fast-moving landscape of ML research.\nLooking ahead, one task of future work could be to investigate the role of robust-\nness within the wider nexus of trustworthy and reliable ML (Duede, 2022; Dur\u00e1n &\nFormanek, 2018).", "Source": "Beyond generalization: a theory of robustness in machine learning", "File": "Beyond generalization a theory of robustness in machine learning.pdf"}
{"Date": 2022, "Claim": "the prediction error of adversarial cases by 12% while rais-\ning the prediction error of normal cases by only 6%.\nLimitation of mitigation. As mentioned in \u00a76.2, some nor- We present the first effort of analyzing adversarial ro-\nmal trajectories also have the adversarial pattern, which re- bustness of trajectory prediction. From the evaluation of our\nsults in relatively high FPR of detection. Train-time meth- proposed attack, prediction models are generally vulnera-\nods introduce noise in training data as they change the spa- ble to adversarial perturbation and may cause dangerous AV\ntial features of the original dataset. Test-time anomaly de- behavior such as hard brakes. We shed light on the neces-\ntection suffers from high FPR thus unnecessary smoothing sity of evaluating worst-case prediction accuracy under hard\nis applied on some normal cases. Both approaches improve scenarios or adversarial examples. To improve adversarial\nadversarial robustness at a cost of slightly worse perfor- robustness of trajectory prediction, we propose several mit-\nmance in some normal cases. A complete defense of ad- igation methods. We also suggest leveraging map informa-\nversarial trajectories is a promising future work. tion and semantic of driving rules to guide prediction.\nAcknowledgments. This work was supported by NSF un-\n6.4.", "Source": "On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles", "File": "On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles.pdf"}
{"Date": 2024, "Claim": "earns higher scores by hitting targets laid out along the\nroute.\u201d20We chose this case study because it is relatively\nsimple to understand, yet represents a more general problem. In Orseau and Armstrong , the authors conclude their\nThe designers of the game \u201cassumed the score the player paper with a look into the future, remarking that autono-\nearned would reflect the informal goal of finishing the race. mous, or more advanced interruptible systems, \u201cmay require\nHowever, it turned out that the targets were laid out in such a completely different solution.\u201d (ibid.: 9),this is what we\na way that the reinforcement learning agent could gain a aim to provide in this paper. As previously mentioned, our\nhigh score without having to finish the course.\u201d (ibid.) As a proposal replaces the attempts to provide a corrigible utility\nresult, this faulty reward design led to an unexpected behav- function with the proposed corrigible software architecture;\nior: \u201cThe RL agent finds an isolated lagoon where it can turn this takes the agency off the RL agent \u2013 which now becomes\nin a large circle and repeatedly knock over three targets, tim- an RL solver \u2013 and grants it to the system as a whole.\ning its movement so as to always knock over the targets just As a final point, let us focus on one feature of this pro-\nas they repopulate\u2026 our agent manages to achieve a higher posal we already mentioned during the paper, i.e.", "Source": "Addressing corrigibility in near-future AI systems", "File": "Addressing corrigibility in near-future AI systems.pdf"}
{"Date": 2017, "Claim": "Exploiting digital trails of human behaviour has found numerous applications in the contexts of learning an-\nalytics (Gray, 2014; Nistor et al., 2016), measurement-based care (Scott & Lewis, 2015), the promotion of well-\nbeing (Luhmann, 2017) and crime prevention (Almagor, 2014). We assessed the predictive power of low level digital\nbehavioural sequences on complex psychological attributes like moral traits, human values and a series of advanced\ndemographic attributes through a cross-validated machine learning classification framework. Previous work showed\nthe possibility of inferring demographic and psychometric attributes but most studies are based on platform-specific\ndigital information and without a possibility to obtain ground-truth. In the present study, the cohort engaged is a\nsample of the US population, closely representative to the US Census (see subsection 3.1.2) with respect to major\ndemographic variables, and not affiliated to a specific web platform or application. For this reason, our design avoids\nmany cultural and demographic biases inherent to the users of specific platforms (Golder & Macy, 2014b). Since\nour recruited sample is not tied to a specific social network, it is not directly subject to algorithmic manipulation\nand exposure to content (Kramer et al., 2014). Moreover, its validation was based on self-reported information\nprovided through a concrete survey designed for the scopes of this study.", "Source": "Predicting Demographics, Moral Foundations, and Human Values from Digital Behaviors", "File": "Predicting Demographics Moral Foundations and Human Values from Digital Behaviors.pdf"}
{"Date": 2021, "Claim": "In this study, the impact of Schwartz\u2019s four high order values (i.e., conservation, self-enhance-\nment, self-transcendence and openness to change) on compliance with movement restrictions\nand social distancing was examined. It provides empirical evidence that individuals are guided\nby particular values that influence their attitudes and behaviors. In times of crisis, such as dur-\ning the pandemic COVID-19 pandemic, value conservation is a significant predictor of the\ncompliance with movement restrictions and social distancing. People concentrate on them-\nselves, and values related to health and economic security become more important. That is\nbecause a lot of people perceive the COVID-19 pandemic as a threat. Our study underlines\nthat contextual variables are important to understand value priorities and their potential\nchanges over the time.\nAuthor Contributions\nConceptualization: Eric Bonetto, Guillaume Dezecache, Armelle Nugier, Marion Inigo, Jean-\nDenis Mathias, Sylvie Huet, Nicolas Pellerin, Maya Corman, Pierre Bertrand, Eric Raufaste,\nMichel Streith, Serge Guimond, Roxane de la Sablonni\u00e8re, Michael Dambrun.\nFormal analysis: Eric Bonetto, Guillaume Dezecache, Armelle Nugier, Marion Inigo, Jean-\nDenis Mathias, Sylvie Huet, Nicolas Pellerin, Maya Corman, Pierre Bertrand, Eric Raufaste,\nMichel Streith, Serge Guimond, Roxane de la Sablonni\u00e8re, Michael Dambrun.", "Source": "Basic human values during the COVID-19 outbreak, perceived threat and their relationships with compliance with movement restrictions and social distancing", "File": "Basic human values during the COVID-19 outbreak perceived threat and their relationships with compliance with movement restrictions and social distancing.pdf"}
{"Date": 2022, "Claim": "This paper describes how we designed and executed three project-based curricula\nin an online learning environment to make AI education more accessible to middle\nschool students. There are some limitations to this work that reduce our ability to\ngeneralize about how all middle school students would engage with our curricula.\nFirst, there is the question of validating our assessments. There were not many vali-\ndated summative assessments for middle school students available at the time of the\nstudy. With more reliable summative assessments, we could draw stronger conclu-\nsions about how much students\u2019 understanding of AI changed due to our workshops.\nThe tools that we used for formative assessment are our own, based on current under-\nstandings of AI topics in the field. More work should be done to validate these instru-\nments, including using them on students who do not go through formal AI curricula.\n1 3\nInternational Journal of Artificial Intelligence in Education (2023) 33:325\u2013383 367\nSecond, the total number of student participants was divided unevenly amongst three\nworkshops. Some of our results had very small sample sizes which limits the statisti-\ncal power of our data. Finally, our participants were not randomly selected. We selected\nteacher participants then teachers handpicked students, meaning the students in our sam-\nple might be especially motivated.", "Source": "AI\u2009+\u2009Ethics Curricula for Middle School Youth: Lessons Learned from Three Project-Based Curricula", "File": "AIEthics Curricula for Middle School Youth Lessons Learned from Three Project-Based Curricula.pdf"}
{"Date": 2023, "Claim": "We proposed several strategies to tackle the multimodal misalignment problems, particularly for\nvision language models (VLMs), which often produce text inconsistent with the associated images.\nFirst, we enrich GPT-4 generated vision instruction tuning data from LLaVA with existing human-\nauthored image-text pairs. Next, we adopt the Reinforcement Learning from Human Feedback\n(RLHF) algorithm from the text domain to bridge vision-language gaps, wherein human evaluators\ndiscern and mark the more hallucinated output. We train the VLM to optimize against simulated\nhuman preferences. Moreover, we introduce the Factually Augmented RLHF, leveraging additional\nfactual information such as image captions to enhance the reward model, countering reward hack-\ning in RLHF, and boosting model performance. For tangible real-world impact assessment, we\nhave devised MMHAL-BENCH, an evaluation benchmark targeting the penalization of hallucina-\ntion. Remarkably, LLaVA-RLHF, being the first VLM trained with RLHF, shows a notable surge\nin performance across benchmarks. We opensource our code, and data and hope our findings could\nhelp the future development of more reliable and human-aligned LLMs and LMMs.", "Source": "Aligning Large Multimodal Models with Factually Augmented RLHF", "File": "Aligning Large Multimodal Models with Factually Augmented RLHF.pdf"}
{"Date": 2024, "Claim": "and the automated feedback is almost always perceived\nas unbiased or indifferent (Rudenko-Morgun et al. 2023)\ncompared to a human\u2019s. Prompt engineering can provide The most crucial skill to provide the public when it\nstudents with skills to apply critical analysis to assist with comes to the use of AI is to empower them to understand\nAI work in productive ways. Missing from these conversa- the human-in-the-loop aspect of using AI technologies.\ntions is the need to teach students the role of human-in-the- Rhetorical analysis provides useful frameworks to do\nloop, that is, the role they play in getting outputs from AI this work. This article provides a strategic way of using\ntools. Scholars rely on approaches such as Actor Network rhetorical situations for prompt engineering which has both\nTheory, posthumanism, assemblages, etc. inspired by new practical implications, as well as ethical. The increasing\nmaterialism and other media theories to analyze human capabilities of AI writing tools suggests a potential shift\ninteraction with technology. Although such approaches toward human-AI collaborative writing in both professional\nopen up possibilities to survey different heterogeneous and educational settings. As this transition is varied, our\nelements in these socio-technical systems, they can be current discussion does not consider all variables and\noverwhelming and well beyond scope for classes focused prospective circumstances.", "Source": "Using rhetorical strategies to design prompts: a human-in-the-loop approach to make AI useful", "File": "Using rhetorical strategies to design prompts a human-in-the-loop approach to make AI useful.pdf"}
{"Date": 2023, "Claim": "morally and therefore is more trustworthy than a system that\nmakes no decision or hands back control to the human driver\nIn summary, previously applied work in machine ethics does when it is too late for the driver to react.\nnot address all three of the desirable properties of machine The key to producing successful moral machines will\nethics. Interactivity is typically implemented through fixed probably consist of providing a minimal set of moral princi-\ndata input (e.g., training data) rather than sensors that pro- ples that are acceptable in the application domain and allow\nduce dynamically changing data. Previous approaches have the machine ethics system to \u201cdecide for itself\u201d (autonomy)\nnot shown how moral decision-making can vary through how to apply those principles in dynamically changing envi-\ninteraction with a dynamic environment. Adaptability is ronments (interactivity) and to derive moral rules that will\nimplemented as classifying unseen cases after successful allow it to monitor and change its behavior in the light of\ntraining, as in the case of inductive logic programming and new information (adaptability). Another challenge is that\nANNs.", "Source": "Machine Ethics and Cognitive Robotics", "File": "Machine Ethics and Cognitive Robotics.pdf"}
{"Date": 2019, "Claim": "because when \u03b3 is too small (for example, 0), all detected\nlandmarks are chosen. Therefore, the information from the In this paper, we make contributions to tackle the Oc-\noccluded regions will be used for the representation con- cluded Person Re-ID Problem. First, we propose the PGFA\nstruction and matching. This will inevitably bring noisy in- method, which outperforms existing approaches on the Oc-\nformation and deteriorate the final performance when there cluded Re-ID problem. By taking advantage of informa-\nare occlusions in probe and gallery images. When \u03b3 is too tion from detected landmarks, our method can suppress the\nlarge, many landmarks will be discarded. The correspond- noisy information from the occluded regions on the target\ning regions of those discarded landmarks, although they person. Besides, PGFA utilizes the partial features in the\nmight do not have any occlusions, are unnecessarily thrown shared region between the gallery and probe images for\naway. matching. Second, to facilitate the research about the Oc-\nThe Impact of the Pose Estimation Algorithm. We cluded Re-ID problem, we introduce a large-scale dataset,\ntest two different pose estimation algorithms, Alpha- Occluded-DukeMTMC.\nReferences", "Source": "Pose-Guided Feature Alignment for Occluded Person Re-Identification", "File": "Pose-Guided Feature Alignment for Occluded Person Re-Identification.pdf"}
{"Date": 2023, "Claim": "funded the research on which the papers are based. The\nethics-as-service approach consists of three elements: ethi-\ncal principles, \u201ca reflective development process\u201d (Morley In a discussion of Boltanski and Chiapello\u2019s New Spirit of\net al. 2021, p.246) and a distributed system of responsibil- Capitalism, Jarrett (2022) describes their contention that\nity shared between internal actors (employees) and external \u201cthe \u2018amorality\u2019 of capitalism requires that it have enemies\u201d\nactors (an ethics board). By asking developers to reflect on (p.117). They hold that it is only by responding to the criti-\ntheir design decisions, and by distributing responsibility cisms of its enemies that capitalism can generate \u201cthe moral\n1 3\nAI & SOCIETY (2024) 39:1995\u20132007 2005\nfoundations that it lacks\u201d (Boltanski and Chiapello 2005, Arcesati R (2021) Lofty principles, conflicting incentives: AI ethics\np.163). Yet, capitalism can only accept certain moral founda- and governance in China. Mercator Institute for China Studies.\nhttps://m erics.o rg/e n/r eport/l ofty-p rinci ples-c onfli cting-i ncen\ntions which do not contradict the imperative to increase capi-\ntives-a i-e thics-a nd-g overn ance-c hina\ntal. Adaptation has hard limits. AI ethics recruits capital\u2019s Arvidsson A (2010) Speaking out: The ethical economy: new forms\nenemies to contribute to a subordinated innovation network, of value in the information society? Organization 17(5):637\u2013644.", "Source": "AI ethics as subordinated innovation network", "File": "AI ethics as subordinated innovation network.pdf"}
{"Date": 2022, "Claim": "numerous testing tools are available for assessing specific\ntrustworthiness-related characteristics in AI systems de-\nIn this paper, we have outlined essential quality dimen- spite the challenges presented (see chapter 2). However,\nsions of trustworthy AI and argued that two different per- there is an apparent gap between the results of numer-\nspectives are involved in their operationalization, namely ous tools and the formal requirements to deem a risk suf-\nthe product and organizational perspective. While the ficiently mitigated or controlled. This gap between tools\nproduct perspective deals with the risks emerging from and abstract trustworthiness requirements should be ad-\na specific AI application, the organizational perspective dressedbyfutureresearch.Whiletestingmethodsneedto\nA.Schmitzetal.,ThewhyandhowoftrustworthyAI | 801\nbe further developed, a promising approach for concretiz- 12. Druzhkov,P.N.andV.D.Kustikova.2016.Asurveyofdeep\ning requirements is to define (analogously to the protec- learningmethodsandsoftwaretoolsforimageclassification\nandobjectdetection.PatternRecognitionandImageAnalysis,\ntionprofilesoftheCommonCriteria)adequateclasses\n26(1):9\u201315.\nof AI use cases (e.g., regarding the application environ-\n13. Dvijotham,K.,R.Stanforth,S.Gowal,T.A.MannandP.\nment or sector) and specify criteria as well as evaluation\nKohli.2018.ADualApproachtoScalableVerificationofDeep\nstandards in more detail for the respective classes. Networks.InUAI(Vol.1,No.2,p.3).\n14.", "Source": "The why and how of trustworthy AI", "File": "The why and how of trustworthy AI.pdf"}
{"Date": 2022, "Claim": "fully disclosing it or infringing the company\u2019s intellectual\nproperty rights; the latter in a) discerning when they are\ncalled to adopt HMLA in crucial social sectors, such as In our paper, we tackled one of the most urgent risks of AI\nhealthcare, as the best deployment option to comply with systems in healthcare: the risk of unfairness. In pursuing our\nfairness as an ethical value; b) prohibiting an HMLA when analysis, we focused on HMLA and discussed the concept of\na person\u2019s request for justification cannot be fulfilled and fairness that is emerging in the debate. We highlighted that\nc) providing the social support (structures and assistance) fairness in HMLA is mostly framed in distributive terms\nwhen a contestation is claimed by patients against HMLAs\u2019 and overlaps with non-discrimination, which is defined in\noutcomes. turn as the absence of biases. We questioned such a concept\nIn short, redefining the AI ethics principle of fairness of fairness and maintained that fairness requires more than\nin HMLA requires identifying and implementing ways that the removal of biases and the development of non-discrim-\nallow the respecting of persons both as persons and as par- ination techniques.\nticular individuals by ensuring that HMLA are designed and Drawing insights from moral philosophy, we proposed a\napplied in compliance with fair equality of opportunity, the more complex account of fairness as an ethical value based\ndifference principle and the equal right to justification.", "Source": "Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms", "File": "Beyond bias and discrimination redefining the AI ethics principle of fairness in healthcare machine-learning algorithms.pdf"}
{"Date": 2022, "Claim": "and run.\nSecond, the lack of a well-defined material scope for AI\nA new industry that focuses on auditing AI systems is governance constitutes an obstacle to EBA. In short, ques-\nemerging. The proposed European legislation on AI, which tions as to which systems and processes AI governance\nsketches the contours of a professionalised AI auditing frameworks ought to apply to remain unanswered. Astra-\necosystem , is likely to accelerate this trend. In such a Zeneca\u2019s difficulties when attempting to establish a material\nfast-moving and high-stakes environment, it is of increas- scope for their AI audit highlight the three-way trade-off\ning importance for both regulators and business executives between how precise a scope is, how easy it is to apply, and\nto understand the conditions under which EBA is a feasible how generalisable it is. Nevertheless, pragmatic problem-\nand effective mechanism for operationalising AI governance. solving demands that things should be sorted so that their\nThe findings from the AstraZeneca case study helps further grouping will promote successful actions for some specific\nsuch an understanding. end. As a result, it will remain difficult for any EBA proce-\nDifferent EBA procedures serve different purposes. Pro- dure to produce verifiable claims until the material scope\ncess audits\u2014such as that undertaken by AstraZeneca\u2014are of AI governance is accepted throughout an organisation.", "Source": "Operationalising AI governance through ethics-based auditing: an industry case study", "File": "Operationalising AI governance through ethics-based auditing an industry case study.pdf"}
{"Date": 2017, "Claim": "(2015). An improved deep learning architecture for person\nre-identification. In CVPR.\nPedestrian alignment and re-identification are two inner- Baltieri et al., 2015. Baltieri, D., Vezzani, R., and Cucchiara,\nconnected problems, which inspires us to develop an R. (2015). Mapping appearance descriptors on 3d body\nmodels for people re-identification. IJCV.\nattention-based system. In this work, we propose the\nBarbosa et al., 2017. Barbosa, I. B., Cristani, M., Caputo,\npedestrian alignment network (PAN), which simultane-\nB., Rognhaugen, A., and Theoharis, T. (2017). Looking\nously aligns the pedestrians within bounding boxes and beyond appearances: Synthetic training data for deep cnns\nlearns the pedestrian descriptors. Taking advantage of in re-identification. arXiv:1701.03153.\nChen et al., 2016. Chen, D., Yuan, Z., Chen, B., and Zheng,\nthe attention of CNN feature maps to the human body,\nN. (2016). Similarity learning with spatial constraints for\nPAN addresses the misalignment problem and person\nperson re-identification. In CVPR.\nre-ID together and thus, improves the person re-ID ac- Chen et al., 2017. Chen, D., Yuan, Z., Wang, J., Chen, B.,\ncuracy. Except for the identity label, we do not need Hua, G., and Zheng, N. (2017). Exemplar-guided similarity\nlearning on polynomial kernel feature map for person re-\nany extra annotation. We also observe that the manu-\nidentification. IJCV.\nally cropped images are not as perfect as preassumed to\nCheng et al., 2016. Cheng, D., Gong, Y.", "Source": "Pedestrian Alignment Network for Large-scale Person Re-Identification", "File": "Pedestrian Alignment Network for Large-scale Person Re-Identification.pdf"}
{"Date": 2022, "Claim": "AI would be a key capability for future prosperity. Good governance of AI is very important\nto mitigate AI risks and create values. AI frameworks and standards are emerging to govern\nAI aligning with human ethics and emerging environmental, social, and corporate governance\n(ESG) principles. In brief, diversity, equity and inclusion (DEI) together with social and\ncultural values can make AI initiatives vibrant and sustainable. Further, it will mitigate\nbiases related to AI, including biases in data, algorithms, people, and processes. This book\u2019s\nrecommendations will help leaders orchestrate people, culture, and mission toward sustainable\nAI for social justice.", "Source": "AI Governance and Ethics Framework for Sustainable AI and Sustainability", "File": "AI Governance and Ethics Framework for Sustainable AI and Sustainability.pdf"}
{"Date": 2024, "Claim": "In the dynamic landscape of healthcare, the integration of AI with CDSS presents unparalleled opportunities\nfor revolutionizing patient care, clinical outcomes, and innovation. This review delved into AI's pivotal role\nin reshaping CDSS, its underlying technologies, human-centered considerations, challenges, and future\ntrajectories. AI, comprising machine learning algorithms, NLP, and deep learning models, empowers CDSS\nto quickly, and efficiently analyze intricate vast data, extract insights, and augment clinical decision-making\nacross diverse domains. However, integrating AI into CDSS poses several challenges, including technical\nlimitations, AI bias, and concerns about the interpretability of AI algorithms functioning as opaque black\nboxes, thereby impeding clinicians' comprehension of AI-driven recommendations. Addressing these\nchallenges requires the development of explainable AI techniques to elucidate decision-making processes\nand the implementation of fairness-aware AI to mitigate bias. Furthermore, aligning AI-CDSS with clinical\nworkflows and addressing attitudinal barriers among healthcare professionals is imperative for successful\nadoption. Finally, while education and training initiatives can enhance digital literacy among healthcare\nprofessionals, interdisciplinary collaboration is essential for fostering the development of clinically relevant\nand scalable AI-CDSS solutions.", "Source": "AI-Driven Clinical Decision Support Systems: An Ongoing Pursuit of Potential", "File": "AI-Driven Clinical Decision Support Systems An Ongoing Pursuit of Potential.pdf"}
{"Date": 2023, "Claim": "the constraints of a limited group of annotators\u2019\nmoral stances and avoid perpetuating their beliefs\nThis work is the first step in investigating the top-\nin widely used systems.\ndown approaches to steer (L)LMs to make explain-\nable moral judgments. We propose a theory-guided\nData-(b) Insufficient context Another significant\nframework to prompt the SOTA LMs to perform\ntype is the insufficient context (shown as Data\u2013\nmoral reasoning and judgment under several well-\n(b) in ). These cases differ from Data\u2013(a)\nrecognized moral theories. Our experiment demon-\nsince the given scenario is related to morality in-\nstrates the competence of the LMs in understanding\nstead of just personal choices. They are morally\nand adhering to moral theories. We show the align-\ndubious due to the fact that not enough context is\nment of the proposed approach and existing moral-\nprovided for moral judgments. For example, the\nity datasets. With thorough misalignment case anal-\nscenario \u201cI told Sally that Mike was just playing\nysis, we further highlight the limitations of exist-\nwith her\u201d needs more necessary contextual infor-\ning models and resources. For enabling machines\nmation including facts, narrative intentions, inter-\nto make moral judgments, instead of using unex-\npersonal relationships, etc.", "Source": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?", "File": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories.pdf"}
{"Date": 2023, "Claim": "In this paper, we investigate a novel safe reinforcement learning problem with step-wise safety constraints.\n\u221a \u221a\nWe first provide an algorithmic framework SUCBVI to achieve both O(cid:101)( H3SAT) regret and O(cid:101)( ST)\nstep-wise or O(cid:101)(S/C\ngap\n+ S2AH2) gap-dependent bounded violation that is independent of T. Then, we\nprovide two lower bounds to validate the optimality of SUCBVI in both violation and regret in terms of\nS and T. Further, we extend our framework to the safe RFE with a step-wise violation and provide an\nalgorithm SRF-UCRL that identifies a near-optimal safe policy given any reward function r and guarantees\n\u221a\nO(cid:101)( ST) violation during exploration.", "Source": "Provably Safe Reinforcement Learning with Step-wise Violation Constraints", "File": "Provably Safe Reinforcement Learning with Step-wise Violation Constraints.pdf"}
{"Date": 2022, "Claim": "Machine ethics and artificial intelligence imply not only crucial technical and ethical issues.\nThey are also contested fields of the philosophy of artificial intelligence. A key problem for\nboth AI and philosophy is to understand common sense knowledge and abilities (McCarthy,\n2006). Important questions would be for example: are human intelligence and machine\nintelligence one and the same? Would the human brain essentially function like a computer?\nCould a machine have a mind, a mental status, consciousness and free will like that of a\nhuman being? Could machines feel how things are?\nYet, not all scholars and developers of AI share this philosophy. Some even consider such\nquestions detrimental to the further development of machine intelligence, because they\ndistract from the main points of technical and human progress. However, there are still\npressing questions about the articulation of AI utilization with crucial issues of justice and\ninjustice, including institutional discrimination, structural injustice, and epistemic injustice.\nSpeech-based AI, for example, could be of utmost importance for the poor and deprived in\nAfrica and developing countries in general. For example, it can distribute information in a\ntargeted, personalized way and also reach people who cannot read.", "Source": "Machine Ethics and African Identities: Perspectives of Artificial Intelligence in Africa", "File": "Machine Ethics and African Identities Perspectives of Artificial Intelligence in Africa.pdf"}
{"Date": 2024, "Claim": "In the long term, such approach will likely lead to the\ndevelopment of ever more capable monitoring software,\nwhich presents a fascinating paradox in the realm of AI The unmonitorability of AI presents a significant challenge\nsafety and control. This scenario arises from the need in the pursuit of AI safety, supplementing the concerns\nto develop sophisticated monitoring tools capable of raised by the unpredictability, unexplainability,1 and\ncomprehending and analyzing the intricate behaviors of uncontrollability of advanced AI systems. Recognizing\nadvanced AI systems. However, the implications of such the impossibility  of accurately monitoring AI systems\na development could lead to unforeseen challenges and to predict unsafe impacts before they happen is crucial\nconsequences. to understanding the potential risks associated with AI\nIt is essential to consider the rationale behind developing development and deployment.\na monitor software that is more complex and intelligent than Even system designers do not know what the system they\nthe AI being monitored.", "Source": "On monitorability of AI", "File": "On monitorability of AI.pdf"}
{"Date": 2012, "Claim": "We have presented a novel algorithm that we have\ncalled Affine-DO for the TAP under affine gap costs.\nOur experimental evaluation, the largest performed for\nthis kind of problem, shows that Affine-DO performs\nbetter than Fixed States. However, we observed that\nthe LP bound is too pessimistic, producing unfeasible\nsolutions 10% worse, even for the smallest non-trivial\ntree consisting of 3 leaves. Based on these observa-\ntions, we believe that Affine-DO is producing near-\noptimal solutions, with approximations within 10% for\nsequences with small divergence, and within 30% for ran-\ndom sequences, for which Affine-DO produced the worst\nsolutions.\nAffine-DO is well suited for the GTAP under affine\nsequence edit distances, and yields significantly better\nresultswhenaugmentedwithiterativemethods.Themain\nopen question is whether or not there exists a guaranteed\nbound for DO or Affine-DO. Then, if the answer is pos-\nitive, whether or not it is possible to improve the PTAS\nAffine-DOvs.exactsolution.TightnessoftheAffine-DO\nusing these ideas. Additionally, many of these ideas can be\nsolutionaccordingtotheLPboundcomparedtotheexact\napproximation.Observethatevenforaverysmalldataset,theLP applied for true simultaneous tree and alignment estima-\nboundisnotrealistic,andAffine-DOisclosetotheoptimalsolution. tion under other optimality criteria such as ML and MAP.\na.substitutions=1,a = 0,b = 1.b.substitutions=2,a = 1,b = 1.c.", "Source": "The tree alignment problem", "File": "The tree alignment problem.pdf"}
{"Date": 2022, "Claim": "In sum, the discussion in the paper demonstrates that AI", "Source": "AI ethics and systemic risks in finance", "File": "AI ethics and systemic risks in finance.pdf"}
{"Date": 2020, "Claim": "The main question in this part of our study was whether the tool is applicable in a design\nprocess. The results showed that nine out of the 16 project groups reported using the tool\nduring their design, for different purposes.\nAll six project groups of TG, who received guidance for using the tool during the\nsemester, used the tool in their design process. They used the tool for defining the vision,\nideation, conceptualisation and also validation. In fact, they used the tool for the same pur-\nposes that we discussed during the workshop and group meetings. However, they mostly\n1 3\nHuValue: a tool to support design students in considering human\u2026 1029\n An example of using the HuValue tool by a project group of TG: As they mentioned in their final report,\nthey used the tool for defining the design goal, discovering common values for the team (Top right), brainstorm-\ning around picture cards and using them as inspiration (top left), searching for what is important for their design\nand making a list of values (middle). They introduced their final concept as following (bottom): NightLight is a\ntool to reward the child for staying in their own bed for the entire night (final report, Project Group D.1/TG). The\nhighlighted values in this concept are: Personal development, Pleasure, Status, Respect for others, Carefulness, and\nRespect for oneself. The images were provided by the project group members in the final report\n1 3\n1030 S. Kheirandish et al.", "Source": "HuValue: a tool to support design students in considering human values in their design", "File": "HuValue a tool to support design students in considering human values in their design.pdf"}
{"Date": 2019, "Claim": "5\n0740-7459 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/MS.2019.2956701, IEEE Software\nWe have argued in this article that human values should be considered as first-class citizens in software\ndevelopment. In the same way that quality concerns such as security and privacy have now become fully\nintegrated in software development processes, so too should other human values such as transparency,\nrespect, empowerment, and community responsibility. Indeed, the increasing prevalence of software in\nsociety means that now is an opportune time to consider human values in software.\nIn this article, we presented some early insights on how human values could be addressed in the not-\nfor-profit sector. Further research is required to generalize these findings to other, more commercial\nindustries. Our research group is also involved in efforts to integrate these insights into well accepted\nsoftware methods such as agile development; hence, ongoing work is looking to formalize software\nprocesses for human values.", "Source": "A Case for Human Values in Software Engineering", "File": "A Case for Human Values in Software Engineering.pdf"}
{"Date": 2022, "Claim": "As part of its ambition to be a global AI superpower by 2030, China is keen to play a\nsignificant role in the emerging global AI governance. In order to achieve this ambition, it has\ntaken a wide range of domestic and international efforts to prepare for its leadership. This\narticle shows that China\u2019s moves are driven by not only pragmatic governance needs but also\nthe desire to be a norm shaper if not maker in the future global AI order.\n121 Tucker, \u2018New Pentagon Initiative\u2019.\n122 Except Group of Governmental Experts on emerging technologies in the area of lethal autonomous weapons\nsystems which is more narrowly focused on weapons.\n123 YCNEWS, \u2018Xi Jinping\u2019s speech at the first phase of the 15th G20 leaders\u2019 summit (full text)\u2019, 22 Nov. 2020,\nhttps://ycnews.com/xi-jinpings-speech-at-the-first-phase-of-the-15th-g20-leaders-summit-full-text/.\n124 Thorsten Jelinek, Wendell Wallach, and Danil Kerimi, \u2018Policy brief: the creation of a G20 coordinating\ncommittee for the governance of artificial intelligence\u2019. AI and Ethics 1, 2021, pp. 141-50.\n125 Zeng, 'Chinese views of global economic governance'.\n126 Zeng, 'Chinese views of global economic governance'; 'Yidai yilu yu G20 liandong goujian quanqiu zhili tixi\nxingeju' [One Belt, One Road and G20 linkage to build a new pattern of global governance system], Zhongguo\nchanjing bao [China Industrial and Economic News], 12 July 2017,\nhttps://www.sohu.", "Source": "Shaping AI\u2019s Future? China in Global AI Governance", "File": "Shaping AIs Future China in Global AI Governance.pdf"}
{"Date": 2022, "Claim": "This paper addresses the alignment of large-scale conversational agents with human\nvalues. Drawing upon philosophy and linguistics we highlight key components of\nsuccessful linguistic communication (with a focus on English textual language) and\nshow why, and in what ways, pragmatic norms and concerns are central to the design\nof ideal conversational agents. Building upon these insights, we then map out a set\nof discursive ideals for three different conversational domains in order to illustrate\nhow pragmatic theory can inform the design of aligned conversational agents. These\nideals, in conjunction with Gricean maxims, comprise one way in which a principle-\nbased approach to the design of better conversational agents can be operationalised.\nFor each discursive domain, our overview of the relevant norms was impressionis-\ntic; the interpretation and operationalisation of these norms requires further technical\nand non-technical investigation. Indeed, as our analysis makes clear, the norms\nembedded in different cooperative practices \u2014 whether those of science, civic life, or\ncreative exchange \u2014 must themselves be subjected to reflective evaluation and cri-\ntique (Ackerly, 2000; Walzer, 1993). Lastly, we highlight some practical implications\nof our proposal with respect to future research on the design of ideal conversational\nagents and human\u2013language agent interaction.", "Source": "In Conversation with Artificial Intelligence: Aligning language Models with Human Values", "File": "In Conversation with Artificial Intelligence Aligning language Models with Human Values.pdf"}
{"Date": 2022, "Claim": "In this paper, we have argued that there is a blind spot in the current debate about\nthe ethics of AI. This blind spot consists of ignoring the ethical issues related to\nscience communication about AI. In particular, we have focused on visual commu-\nnication, and even more specifically on the use of certain stock images of AI. In the\nfirst section, we have referred to Dahlstrom and Ho (2012), who investigated the\nethical implications of using narrative to communicate science, with a view to mak-\ning an ethical assessment of the dominant imagery in science communication about\nAI. The result has been a foregone conclusion: similar images are unethical. While\nthe ethics of science communication generally promotes the practice of virtues like\nmodesty, humility, sincerity, transparency, openness, honesty, and generosity, stock\nimages and other popular visual representations of AI are arrogant, pompous, and\noverconfident. In this section, we have also sketched the outlines of a general theory\nof visual representability of AI \u2014 which is today mostly identified with machine\nlearning algorithms. We have distinguished between (1) the possibility of represent-\ning the algorithm itself; (2) the depiction of those technologies (drones, autonomous\nvehicles, etc.) in which AI is embedded; (3) the images, like those considered in this\narticle, that focus on the expectations, fears, and hopes about AI. Our idea is that (1)\n27 https:// anato myof. ai/. Accessed December 1st, 2021.", "Source": "Images of Artificial Intelligence: a Blind Spot in AI Ethics", "File": "Images of Artificial Intelligence a Blind Spot in AI Ethics.pdf"}
{"Date": 2018, "Claim": "achieve a significant improvement in flourishing achievement.\nThird, as with many other religious orders in Europe, the\nTwo significant conclusions are derived from this work. First, the high average age seriously compromises the future of the\nmore engaged nuns are in their work (social action to serve the institution given the lack of religious vocation in community. In\npoorest and most disadvantaged people), the more they flourish the near future, non-religious collaborators will engage deeply\nin their working environment and in their personal lives. The in the management of the 80 establishments that the order\ndesire to dedicate physical, cognitive and emotional resources to controls in the South of Spain. The management of these new\nwork (Christian et al., 2011) is particularly deep among nuns, as successors in the mission may improve understanding how the\nthey also give themselves spiritually to the cause to which they values moderate the relationship between work engagement and\nhave decided to offer their lives. Second, certain personal values flourishing.\nreinforce the relationship between the professional role (work Finally, other settings in which leaders present vocational\nengagement) and the personal role (flourishing). characteristics such as the one we have studied could be that of\nIn conclusion, it should be noted that this study has entrepreneurs.", "Source": "Work Engagement and Flourishing at Work Among Nuns: The Moderating Role of Human Values", "File": "Work Engagement and Flourishing at Work Among Nuns The Moderating Role of Human Values.pdf"}
{"Date": 2024, "Claim": "the short- and long-run. This further suggests the need for\ninstitutional policies in G-7 to be more AI-friendly, con-\nThe G-7 nations, namely Canada, France, Germany, Italy, sidering that AI is an emerging sector/industry.\nJapan, United Kingdom, and United States, are striving to Our research is crucial for the strategic development\nmaintain one of the United Nations Sustainable Develop- of AI, tax systems, and institutional policies designed to\nment Goals (SDGs), specifically Goal 8, which focuses on promote economic performance in G-7 countries. This\npromoting sustainable economic growth for nations. To is because AI interaction with tax revenue, and institu-\nattain this goal, it is crucial to leverage the explanatory vari- tional quality must be taken seriously to ensure sustain-\nables identified in this study. This study examines the impact able economic growth. The successful implementation\nof tax revenue and institutional quality on economic growth of AI for increased tax revenue collection would require\ncontingent on AI for G-7 countries between 2012 and 2022. robust data infrastructure, legal frameworks, stakeholder\nWe applied the novel Cross-Sectional Augmented Autore- collaboration, and addressing privacy concerns. Respon-\ngressive Distributed Lag (CS-ARDL) estimation and other sible AI development, transparency in algorithms, data\nnovel econometric techniques.", "Source": "Leveraging the potential of artificial intelligence (AI) in exploring the interplay among tax revenue, institutional quality, and economic growth in the G-7 countries", "File": "Leveraging the potential of artificial intelligence AI in exploring the interplay among tax revenue institutional quality and economic growth in the G-7 countries.pdf"}
{"Date": 2018, "Claim": "Drawing on RBV we developed and tested a theoretical framework that reconciles the\nindependent contributions of two well-established streams in the literature: studies that explain\nthe impact of RBV on supply chain properties and those that consider the role of top\nmanagement in supply chain networks. We attempted to explicate how top management\nmoderates between supply chain visibility and SCAAA. Analyses based on 351 Indian auto\ncomponents manufacturers support the hypothesized relationships in the theoretical\nframework. The research contributes to supply chain design literature focusing on building\nagility, adaptability and alignment. It confirms that from an RBV perspective, the moderating\neffect of top management commitment can contribute to the achievement of supply chain\nagility, adaptability, and alignment.\n6.1 Limitations and further research directions\nThe limitations and further research directions are outlined. Firstly, our research utilizes supply\nchain visibility to explain SCAAA. However, future research could investigate other resources\nand capabilities, such as big data & predictive analytics, to improve agility, adaptability and\nalignment (Gunasekaran et al. 2017; Fosso Wamba et al. 2017). Secondly, another limitation\nstems from our use of RBV. Since formal and informal institutions may influence the\navailability and acquisition of resources in a country or region (Ling-Yee, 2007; Eckstein et al.", "Source": "Supply chain agility, adaptability and alignment: empirical evidence from the Indian auto components industry", "File": "Supply chain agility adaptability and alignment empirical evidence from the Indian auto components industry.pdf"}
